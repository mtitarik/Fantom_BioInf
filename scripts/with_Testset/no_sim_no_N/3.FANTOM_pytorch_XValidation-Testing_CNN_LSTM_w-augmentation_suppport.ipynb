{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "#import pandas as pd\n",
    "#import matplotlib.pyplot as plt\n",
    "import requests\n",
    "\n",
    "import os\n",
    "\n",
    "from astropy.units import one\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import classification_report, confusion_matrix, precision_score, recall_score, f1_score, cohen_kappa_score, \\\n",
    "                            precision_recall_curve, average_precision_score, roc_auc_score, roc_curve, auc, accuracy_score\n",
    "\n",
    "import itertools\n",
    "import random\n",
    "import math\n",
    "\n",
    "from IPython.display import clear_output\n",
    "\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from pathlib import Path\n",
    "import seaborn as sns\n",
    "import matplotlib.ticker as ticker\n",
    "#import matplotlib.pyplot as plt\n",
    "import copy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch import Tensor\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as data_utils\n",
    "import torch.optim.lr_scheduler as lr_scheduler \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0);\n",
    "torch.manual_seed(0);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyper-parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_features = True\n",
    "\n",
    "use_comp_features = True\n",
    "use_bio_features = True\n",
    "\n",
    "use_testset = True\n",
    "\n",
    "do_horz_flip = False\n",
    "\n",
    "do_cross_validation = False\n",
    "do_testing_using_best_model = False\n",
    "do_retraining = True\n",
    "do_testing_only = False\n",
    "\n",
    "use_padding_for_bucketing = False\n",
    "\n",
    "class_weights = {0: 1., 1: 1.} \n",
    "class_weights = {0: 2.36/(2.36+1.), 1: 1./(2.36+1.)} \n",
    "\n",
    "num_classes=2\n",
    "\n",
    "val_pct = 0.20\n",
    "\n",
    "\n",
    "use_full_DS = True;\n",
    "\n",
    "CLASS_BALANCE_RATIO = 1;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_dir = '../../../'\n",
    "\n",
    "#ds_dir = f'{project_dir}data/seq/'\n",
    "ds_dir = f'{project_dir}data/processed/'\n",
    "\n",
    "snapshot_dir = f'{project_dir}snapshots/'\n",
    "\n",
    "classes = ['lnc_e','lnc_p']\n",
    "class_names = ['e-lncRNA','p-lncRNA']\n",
    "class_suffix = '_noN'\n",
    "\n",
    "train_val_test_dir = 'train_val_test__80_20'\n",
    "\n",
    "# sequence files (train + val)\n",
    "neg_filename = f'{ds_dir}{train_val_test_dir}/{classes[0]}{class_suffix}_tr_val.csv'\n",
    "pos_filename = f'{ds_dir}{train_val_test_dir}/{classes[1]}{class_suffix}_tr_val.csv'\n",
    "\n",
    "# sequence files (test)\n",
    "test_neg_filename = f'{ds_dir}{train_val_test_dir}/{classes[0]}{class_suffix}_test.csv'\n",
    "test_pos_filename = f'{ds_dir}{train_val_test_dir}/{classes[1]}{class_suffix}_test.csv'\n",
    "\n",
    "# kmer feature files (train + val)\n",
    "feature_filenames_class_0 = [f'{ds_dir}{train_val_test_dir}/{classes[0]}{class_suffix}_1mer_features_tr_val.csv',\n",
    "                             f'{ds_dir}{train_val_test_dir}/{classes[0]}{class_suffix}_2mer_features_tr_val.csv',\n",
    "                             f'{ds_dir}{train_val_test_dir}/{classes[0]}{class_suffix}_3mer_features_tr_val.csv']\n",
    "feature_filenames_class_1 = [f'{ds_dir}{train_val_test_dir}/{classes[1]}{class_suffix}_1mer_features_tr_val.csv',\n",
    "                             f'{ds_dir}{train_val_test_dir}/{classes[1]}{class_suffix}_2mer_features_tr_val.csv',\n",
    "                             f'{ds_dir}{train_val_test_dir}/{classes[1]}{class_suffix}_3mer_features_tr_val.csv']\n",
    "\n",
    "# kmer feature files (test)\n",
    "test_feature_filenames_class_0 = [f'{ds_dir}{train_val_test_dir}/{classes[0]}{class_suffix}_1mer_features_test.csv',\n",
    "                                  f'{ds_dir}{train_val_test_dir}/{classes[0]}{class_suffix}_2mer_features_test.csv',\n",
    "                                  f'{ds_dir}{train_val_test_dir}/{classes[0]}{class_suffix}_3mer_features_test.csv']\n",
    "test_feature_filenames_class_1 = [f'{ds_dir}{train_val_test_dir}/{classes[1]}{class_suffix}_1mer_features_test.csv',\n",
    "                                  f'{ds_dir}{train_val_test_dir}/{classes[1]}{class_suffix}_2mer_features_test.csv',\n",
    "                                  f'{ds_dir}{train_val_test_dir}/{classes[1]}{class_suffix}_3mer_features_test.csv']\n",
    "\n",
    "\n",
    "# biological featurefiles (train + val)\n",
    "bio_feature_filenames_class_0 = [f'{ds_dir}{train_val_test_dir}/{classes[0]}_nosim.fa.matrix_tr_val.csv']\n",
    "bio_feature_filenames_class_1 = [f'{ds_dir}{train_val_test_dir}/{classes[1]}_nosim.fa.matrix_tr_val.csv']\n",
    "\n",
    "# biological featurefiles (test)\n",
    "test_bio_feature_filenames_class_0 = [f'{ds_dir}{train_val_test_dir}/{classes[0]}_nosim.fa.matrix_test.csv']\n",
    "test_bio_feature_filenames_class_1 = [f'{ds_dir}{train_val_test_dir}/{classes[1]}_nosim.fa.matrix_test.csv']\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load sequence data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(filename):\n",
    "    with open(filename,'r') as nf:\n",
    "        examples = [seq.strip() for seq in nf.readlines()]\n",
    "        num_examples = len(examples) # number of negative examples\n",
    "    return num_examples, examples\n",
    "\n",
    "# read sequence data\n",
    "num_neg, neg_examples = get_data(neg_filename)\n",
    "print(\"num_neg:\", num_neg)\n",
    "num_pos, pos_examples = get_data(pos_filename)\n",
    "print(\"num_pos:\", num_pos)\n",
    "\n",
    "if do_horz_flip:\n",
    "    num_neg *=2\n",
    "    neg_examples.extend(neg_examples)    \n",
    "\n",
    "    num_pos *=2\n",
    "    pos_examples.extend(pos_examples)    \n",
    "    print(\"After augmentation\")\n",
    "    print(\"num_neg:\", num_neg)\n",
    "    print(\"num_pos:\", num_pos)\n",
    "    \n",
    "# combine negative and positive examples (sequences)\n",
    "sequences_DS = neg_examples + pos_examples \n",
    "labels_DS = np.array([0]*num_neg + [1]*num_pos)   \n",
    "\n",
    "print(\"# of sequences:\", len(sequences_DS), \"\\n# of labels:\", len(labels_DS), \"\\nlength of sequences:\", len(sequences_DS[0]))\n",
    "\n",
    "idx_neg =  np.arange(num_neg)\n",
    "idx_pos =  num_neg + np.arange(num_pos) # needed shift as negative examples' idx in concatenated array: [neg, pos]\n",
    "\n",
    "print(\"index sizes:\",len(idx_neg), len(idx_pos))\n",
    "\n",
    "\n",
    "if (use_testset):\n",
    "    print()\n",
    "\n",
    "    # read test sequence data\n",
    "    test_num_neg, test_neg_examples = get_data(test_neg_filename)\n",
    "    print(\"test_num_neg:\", test_num_neg)\n",
    "    test_num_pos, test_pos_examples = get_data(test_pos_filename)\n",
    "    print(\"test_num_pos:\", test_num_pos)\n",
    "\n",
    "    if do_horz_flip:\n",
    "        test_num_neg *=2\n",
    "        test_neg_examples.extend(test_neg_examples)    \n",
    "\n",
    "        test_num_pos *=2\n",
    "        test_pos_examples.extend(test_pos_examples)    \n",
    "        print(\"After augmentation\")\n",
    "        print(\"num_neg:\", num_neg)\n",
    "        print(\"num_pos:\", num_pos)\n",
    "    \n",
    "    # combine negative and positive test examples (sequences)\n",
    "    test_sequences_DS = test_neg_examples + test_pos_examples \n",
    "    test_labels_DS = np.array([0]*test_num_neg + [1]*test_num_pos)   \n",
    "\n",
    "    print(\"# of test sequences:\", len(test_sequences_DS), \"\\n# of test labels:\", len(test_labels_DS), \"\\nlength of test sequences:\", len(test_sequences_DS[0]) )\n",
    "\n",
    "    test_idx_neg =  np.arange(test_num_neg)\n",
    "    test_idx_pos =  test_num_neg + np.arange(test_num_pos) # needed shift as negative examples' idx in concatenated array: [neg, pos]\n",
    "    print(\"test index size\",len(test_idx_neg), len(test_idx_pos))\n",
    "\n",
    "    \n",
    "# with open(neg_filename,'r') as nf:\n",
    "#     neg_examples = [seq.strip() for seq in nf.readlines()]\n",
    "#     num_neg = len(neg_examples) # number of negative examples\n",
    "#     print(num_neg)\n",
    "\n",
    "# with open(pos_filename,'r') as pf:\n",
    "#     pos_examples = [seq.strip() for seq in pf.readlines()]\n",
    "#     num_pos = len(pos_examples) # number of positive examples\n",
    "#     print(num_pos)    \n",
    "\n",
    "# sequences_DS = neg_examples + pos_examples \n",
    "# labels_DS = np.array([0]*num_neg + [1]*num_pos)   \n",
    "# len(sequences_DS), len(labels_DS)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load features (if used)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(use_features):\n",
    "    \n",
    "    if use_comp_features:\n",
    "        # kmers (train + val)\n",
    "        features_class0 = np.concatenate([pd.read_csv(feature_filename, header=None, sep=\"\\t\", index_col=False).to_numpy() for feature_filename in feature_filenames_class_0], axis = 1)\n",
    "        features_class1 = np.concatenate([pd.read_csv(feature_filename, header=None, sep=\"\\t\", index_col=False).to_numpy() for feature_filename in feature_filenames_class_1], axis = 1)\n",
    "        \n",
    "        if do_horz_flip:\n",
    "            features_class0 = np.concatenate([features_class0, features_class0], axis = 0)    \n",
    "            features_class1 = np.concatenate([features_class1, features_class1], axis = 0)    \n",
    "            \n",
    "        neg_features = features_class0\n",
    "        pos_features = features_class1\n",
    "\n",
    "        # combine negative and positive examples into a single array\n",
    "        features_DS = np.concatenate([features_class0, features_class1], axis = 0)\n",
    "    \n",
    "        print(\"neg features shape:\",neg_features.shape, \"\\npos features shape:\",pos_features.shape, \"\\ncombined features shape:\",features_DS.shape)\n",
    "        print()\n",
    "\n",
    "    if use_bio_features:  \n",
    "        # bio featurers (train + val)\n",
    "        bio_features_class0 = np.concatenate([pd.read_csv(bio_feature_filename, header=None, index_col=False).to_numpy() for bio_feature_filename in bio_feature_filenames_class_0], axis = 1)\n",
    "        bio_features_class1 = np.concatenate([pd.read_csv(bio_feature_filename, header=None, index_col=False).to_numpy() for bio_feature_filename in bio_feature_filenames_class_1], axis = 1)\n",
    "\n",
    "        if do_horz_flip:\n",
    "            bio_features_class0 = np.concatenate([bio_features_class0, bio_features_class0], axis = 0)    \n",
    "            bio_features_class1 = np.concatenate([bio_features_class1, bio_features_class1], axis = 0)   \n",
    "        \n",
    "        neg_bio_features = bio_features_class0\n",
    "        pos_bio_features = bio_features_class1\n",
    "\n",
    "        # combine negative and positive examples into a single array\n",
    "        bio_features_DS = np.concatenate([bio_features_class0, bio_features_class1], axis = 0)  \n",
    "\n",
    "        print(\"neg bio features shape:\",neg_bio_features.shape, \"\\npos bio features shape:\",pos_bio_features.shape, \"\\ncombined bio features shape:\",bio_features_DS.shape)\n",
    "        print()\n",
    "\n",
    "        \n",
    "    if (use_testset):\n",
    "\n",
    "        if use_comp_features:\n",
    "            # kmers (test)\n",
    "            test_features_class0 = np.concatenate([pd.read_csv(test_feature_filename, header=None, sep=\"\\t\", index_col=False).to_numpy() for test_feature_filename in test_feature_filenames_class_0], axis = 1)\n",
    "            test_features_class1 = np.concatenate([pd.read_csv(test_feature_filename, header=None, sep=\"\\t\", index_col=False).to_numpy() for test_feature_filename in test_feature_filenames_class_1], axis = 1)\n",
    "\n",
    "            if do_horz_flip:\n",
    "                test_features_class0 = np.concatenate([test_features_class0, test_features_class0], axis = 0)    \n",
    "                test_features_class1 = np.concatenate([test_features_class1, test_features_class1], axis = 0)                 \n",
    "\n",
    "            test_neg_features = test_features_class0\n",
    "            test_pos_features = test_features_class1    \n",
    "\n",
    "            # combine negative and positive examples into a single array    \n",
    "            test_features_DS = np.concatenate([test_features_class0, test_features_class1], axis = 0)    \n",
    "            print(\"test neg features shape:\",test_neg_features.shape, \"\\ntest pos features shape:\",test_pos_features.shape, \"\\ncombined test features shape:\",test_features_DS.shape)\n",
    "            print()\n",
    "            \n",
    "        if use_bio_features:  \n",
    "            # bio featurers (test)\n",
    "            test_bio_features_class0 = np.concatenate([pd.read_csv(bio_feature_filename, header=None, index_col=False).to_numpy() for bio_feature_filename in test_bio_feature_filenames_class_0], axis = 1)\n",
    "            test_bio_features_class1 = np.concatenate([pd.read_csv(bio_feature_filename, header=None, index_col=False).to_numpy() for bio_feature_filename in test_bio_feature_filenames_class_1], axis = 1)\n",
    "\n",
    "            if do_horz_flip:\n",
    "                test_bio_features_class0 = np.concatenate([test_bio_features_class0, test_bio_features_class0], axis = 0)    \n",
    "                test_bio_features_class1 = np.concatenate([test_bio_features_class1, test_bio_features_class1], axis = 0)                 \n",
    "   \n",
    "\n",
    "            test_neg_bio_features = test_bio_features_class0\n",
    "            test_pos_bio_features = test_bio_features_class1\n",
    "\n",
    "            # combine negative and positive examples into a single array    \n",
    "            test_bio_features_DS = np.concatenate([test_bio_features_class0, test_bio_features_class1], axis = 0)    \n",
    "    \n",
    "            print(\"test neg bio features shape:\",test_neg_bio_features.shape, \"\\ntest pos bio features shape:\",test_pos_bio_features.shape, \"\\ntest combined bio features shape:\",test_bio_features_DS.shape)\n",
    "            print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# labels_DS[:10],labels_DS[18487-10:18487+10],labels_DS[-10:],len(labels_DS) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def compute_kmer_counts(seq, k, do_sliding = True):\n",
    "#     alphabet = list(set(list(seq)))\n",
    "#     alphabet.sort()\n",
    "\n",
    "#     kmers_list = []\n",
    "#     kmers_list.append(alphabet)\n",
    "    \n",
    "#     for k_idx in range(k-1):\n",
    "#         kmers_list.append([a+b for a in kmers_list[k_idx] for b in alphabet ])\n",
    "\n",
    "#     seq_lst = list(seq)\n",
    "#     subs = [\"\".join(seq_lst[idx:idx+k]) for idx in range(len(seq)-k+1)]\n",
    "#     kmers = kmers_list[k-1]\n",
    "#     kmers_counts = [subs.count(kmer) for kmer in kmers]\n",
    "    \n",
    "#     return kmers_counts\n",
    "\n",
    "# results = []\n",
    "# for idx in range(len(sequences_DS)):#np.random.randint(0, len(sequences_DS), size= (20,1) ):\n",
    "#     #idx = idx.item()\n",
    "#     seq = sequences_DS[idx]\n",
    "#     kmer_counts1 = compute_kmer_counts(seq[:1000],3)\n",
    "#     kmer_counts2 = compute_kmer_counts(seq[1000:],3)\n",
    "#     #print(kmer_counts1, \"||\", kmer_counts2, \"||\",features_DS[idx,:][40:])\n",
    "#     result = np.all(np.array(kmer_counts1+kmer_counts2) == np.array(features_DS[idx,40:].squeeze())) \n",
    "#     if(not result):\n",
    "#         print(idx)\n",
    "#         print(kmer_counts1)\n",
    "#         print(kmer_counts2)\n",
    "#         print(seq)\n",
    "#         print(features_DS[idx,40:72].squeeze())\n",
    "#         print(features_DS[idx,72:].squeeze())\n",
    "#         print()\n",
    "#     results.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.all(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## subsample dataset (if needed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffle the positive and negative indices\n",
    "idx_neg_shuf = np.random.permutation(idx_neg)\n",
    "idx_pos_shuf = np.random.permutation(idx_pos)\n",
    "\n",
    "# cap class size ratio to be used to the class size ratio of the dataset\n",
    "DS_class_balance_ratio = len(idx_neg_shuf)/len(idx_pos_shuf) \n",
    "DS_class_balance_ratio = 1/DS_class_balance_ratio if DS_class_balance_ratio < 1 else DS_class_balance_ratio\n",
    "\n",
    "\n",
    "if DS_class_balance_ratio < CLASS_BALANCE_RATIO:\n",
    "    effective_class_balance_ratio = DS_class_balance_ratio \n",
    "else:\n",
    "    effective_class_balance_ratio = CLASS_BALANCE_RATIO\n",
    "\n",
    "print(\"class balance ratio in dataset:\", DS_class_balance_ratio)\n",
    "print(\"specified class balance ratio:\", CLASS_BALANCE_RATIO)\n",
    "print(\"effective class balance ratio being used :\", effective_class_balance_ratio)    \n",
    "print()\n",
    "\n",
    "if use_full_DS:\n",
    "    merged_index = np.concatenate([idx_neg_shuf, idx_pos_shuf], axis = 0) ;\n",
    "    \n",
    "    print(\"no subsampling, size of neg :\", len(idx_neg_shuf))\n",
    "    print(\"no subsampling, size of pos :\", len(idx_pos_shuf))\n",
    "    print( 'no subsampling, length of merged: ' , len(merged_index) )\n",
    "    print()\n",
    "else:\n",
    "    if(num_neg <= num_pos):\n",
    "        idx_neg_selected =  idx_neg_shuf\n",
    "        idx_pos_selected =  idx_pos_shuf[:int(effective_class_balance_ratio*num_neg)]\n",
    "    else:\n",
    "        idx_neg_selected =  idx_neg_shuf[:int(effective_class_balance_ratio*num_pos)]\n",
    "        idx_pos_selected =  idx_pos_shuf\n",
    "    \n",
    "    # shift the pos's indices as pos examples come after negative example in the combines data structure\n",
    "    #idx_pos_selected += len(neg_examples)  \n",
    "    #idx_pos_selected += len(idx_neg_selected)  \n",
    "        \n",
    "    merged_index = np.concatenate([idx_neg_selected, idx_pos_selected], axis = 0)\n",
    "    \n",
    "    print(\"after subsampling, size of neg :\", len(idx_pos_selected))\n",
    "    print(\"after subsampling, size of pos :\", len(idx_neg_selected))\n",
    "    print( 'len of merged : ' , len(merged_index) )\n",
    "    print()\n",
    "\n",
    "\n",
    "# perform subsampling\n",
    "sequences, labels = zip(*[ (sequences_DS[i], labels_DS[i]) for i in merged_index ])\n",
    "labels = np.array(labels)\n",
    "\n",
    "print('#seq before subsampling:' , len(sequences_DS) )\n",
    "print('#labels before subsampling:' , len(labels_DS) )\n",
    "print()\n",
    "print('#seq after subsampling:' , len(sequences) )\n",
    "print('#labels after subsampling:' , len(labels) )\n",
    "print()\n",
    "\n",
    "\n",
    "if (use_testset):\n",
    "    # no subsampling for test set\n",
    "    test_sequences, test_labels = test_sequences_DS, test_labels_DS\n",
    "    test_labels = np.array(test_labels)\n",
    "\n",
    "if(use_features):\n",
    "\n",
    "    if use_comp_features:\n",
    "        features = features_DS[merged_index,:]\n",
    "        print('#features before subsampling:' , len(features_DS), '#features after subsampling:' , len(features))    \n",
    "    else:\n",
    "        features = None\n",
    "        \n",
    "    if use_bio_features:    \n",
    "        bio_features = bio_features_DS[merged_index,:]\n",
    "        print('#bio features before subsampling:' , len(bio_features_DS) , '#bio features after subsampling:' , len(bio_features) )    \n",
    "    else:\n",
    "        bio_features = None\n",
    "        \n",
    "    if (use_testset):\n",
    "        # no subsampling for test set\n",
    "        if use_comp_features: \n",
    "            test_features = test_features_DS \n",
    "        else: \n",
    "            test_features = None \n",
    "        \n",
    "        if use_bio_features:  \n",
    "            test_bio_features = test_bio_features_DS\n",
    "        else:\n",
    "            test_bio_features = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.all(np.sort(sequences_DS, axis = 0)==np.sort(sequences, axis = 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NOT ACCURATE ANYMORE: sanity check: checking whether the selected positive examples are truly from the original positive examples loaded from the disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tmp_neg = [sequences[i] for i in range(len(labels)) if labels[i] == 0]\n",
    "# tmp_pos = [sequences[i] for i in range(len(labels)) if labels[i] == 1]\n",
    "# print(len(tmp_pos), len(tmp_neg))\n",
    "\n",
    "# print(np.all([s in neg_examples for s in tmp_neg]))\n",
    "# print(np.all([s in pos_examples for s in tmp_pos]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# identify the alphabet of the whole DS\n",
    "unique_DNAs= set()\n",
    "for seq in sequences_DS:\n",
    "    s = set (seq)\n",
    "    unique_DNAs = unique_DNAs | s\n",
    "\n",
    "unique_DNAs = list(unique_DNAs)\n",
    "unique_DNAs.sort()\n",
    "num_DNAs = len(unique_DNAs)\n",
    "print(\"All nucleotides: \")\n",
    "print(unique_DNAs)\n",
    "print(\"# unique nucleaotides:\",num_DNAs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lengths of the sequences\n",
    "sequence_lens = np.array([len(x) for x in sequences])\n",
    "\n",
    "print(\"# of different sample lengths:\",len(np.unique(sequence_lens)))\n",
    "print(\"number of samples:\",len(sequence_lens))\n",
    "print(\"length of shortest sample: \",min(sequence_lens))\n",
    "print(\"length of longest sample: \",max(sequence_lens))\n",
    "print()\n",
    "\n",
    "if (use_testset):\n",
    "    # lengths of the sequences\n",
    "    test_sequence_lens = np.array([len(x) for x in test_sequences])\n",
    "\n",
    "    print(\"# of different sample lengths:\",len(np.unique(sequence_lens)))\n",
    "    print(\"number of samples:\",len(sequence_lens))\n",
    "    print(\"length of shortest sample: \",min(sequence_lens))\n",
    "    print(\"length of longest sample: \",max(sequence_lens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the lengths of the sequences\n",
    "plt.plot(list(sorted(sequence_lens)))\n",
    "plt.show()\n",
    "\n",
    "if (use_testset):\n",
    "    # plot the lengths of the test sequences\n",
    "    plt.plot(list(sorted(test_sequence_lens)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code borrowed from lncNet repository for integer coding and bucketing\n",
    "\n",
    "class CharacterTable(object): #make encoding table\n",
    "    '''\n",
    "    Given a set of characters:\n",
    "    + Encode them to integer coding representation\n",
    "    + Decode the one hot integer representation to their character output\n",
    "    + Decode a vector of probabilities to their character output\n",
    "    #chars : 0 (padding ) + other characters\n",
    "    '''\n",
    "    def __init__(self, chars):\n",
    "        self.chars = sorted(set(chars))\n",
    "        self.char_indices = dict((c, i) for i, c in enumerate(self.chars))\n",
    "        self.indices_char = dict((i, c) for i, c in enumerate(self.chars))\n",
    "    \n",
    "    def encode(self, l):\n",
    "        X = np.zeros((len(l)),dtype=np.int32)\n",
    "        for i, c in enumerate(l):\n",
    "            X[i]= self.char_indices[c]\n",
    "    \n",
    "        return X\n",
    "\n",
    "    def decode(self, X, calc_argmax=True):\n",
    "        if calc_argmax:\n",
    "            X = X.argmax(axis=-1)\n",
    "        return ''.join(self.indices_char[x] for x in X)\n",
    "\n",
    "def create_index_buckets(sequence_lens, bucket_high_lims):\n",
    "    \n",
    "    # bucket of data indices\n",
    "    buckets = [ [] for _ in bucket_high_lims]\n",
    "\n",
    "    for i,len_x in enumerate(sequence_lens):\n",
    "        for b_id, bucket_len in enumerate(bucket_high_lims):\n",
    "            if len_x <= bucket_len:\n",
    "                # append the index of the data instead of the actual data\n",
    "                buckets[b_id].append(i)  \n",
    "                break\n",
    "\n",
    "    return buckets\n",
    "\n",
    "def seq2intcoding(seqs, ctable):\n",
    "    \n",
    "    X=[]\n",
    "    for seq in seqs:\n",
    "        enc = ctable.encode(seq)\n",
    "        X.append(enc)\n",
    "    \n",
    "    X = np.array(X)\n",
    "    \n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Do index bucketing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket_high_lims = [500*i for i in range(1,201)]\n",
    "\n",
    "if (use_testset):\n",
    "    test_bucket_high_lims = [500*i for i in range(1,201)]\n",
    "\n",
    "# do not pad sequences if option not enabled (for datasets with fixed-length sequences where the length does not match any bucket's high limit)\n",
    "if not use_padding_for_bucketing:\n",
    "    assert max(sequence_lens) == min(sequence_lens), \"Sequence length varies; please enable padding for bucketing\"\n",
    "    assert max(test_sequence_lens) == min(test_sequence_lens), \"test set's sequence length varies; please enable padding for bucketing\"\n",
    "    bucket_high_lims = [max(sequence_lens)]   # fixed length sequences, so there will be only one bucket\n",
    "    test_bucket_high_lims = [max(test_sequence_lens)]\n",
    "    \n",
    "chars= \"0\" + \"\".join(unique_DNAs)\n",
    "ctable = CharacterTable(chars)\n",
    "\n",
    "\n",
    "# integer coded sequences\n",
    "seq_int_coded = seq2intcoding(sequences, ctable)\n",
    "# create buckets containing data indices\n",
    "idxs_in_buckets = create_index_buckets(sequence_lens, bucket_high_lims)   # returning array of arrays containing indices of data to put in each bucket\n",
    "# remove buckets with no elements\n",
    "used_bucket_idxs, idxs_in_buckets  = zip(*[(i,idxs_in_buckets[i]) for i,b in enumerate(idxs_in_buckets) if b])\n",
    "# only keep the high limits for the non-empty buckets\n",
    "bucket_high_lims = np.array(bucket_high_lims).reshape(1,-1)[used_bucket_idxs] \n",
    "\n",
    "\n",
    "# for test set\n",
    "if (use_testset):\n",
    "    # integer coded sequences\n",
    "    test_seq_int_coded = seq2intcoding(test_sequences, ctable)\n",
    "    # create buckets containing data indices\n",
    "    test_idxs_in_buckets = create_index_buckets(test_sequence_lens, test_bucket_high_lims)   # returning array of arrays containing indices of data to put in each bucket\n",
    "    # remove buckets with no elements\n",
    "    test_used_bucket_idxs, test_idxs_in_buckets  = zip(*[(i,test_idxs_in_buckets[i]) for i,b in enumerate(test_idxs_in_buckets) if b])\n",
    "    # only keep the high limits for the non-empty buckets\n",
    "    test_bucket_high_lims = np.array(test_bucket_high_lims).reshape(1,-1)[test_used_bucket_idxs] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Do sequence and feature (if being used) bucketing along with one-hot encoding of the sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data_buckets(X, Y, idxs_in_buckets, bucket_high_lims, sequence_lens, features = None, bio_features = None):    \n",
    "    \n",
    "    \"\"\"\n",
    "    params: \n",
    "    X: integer coded sequences\n",
    "    Y: labels\n",
    "    idx_buckets: array-like containing list of indices in each bucket\n",
    "    bucket_high_lims: maximum length of sequences in each bucket\n",
    "    sequence_lens: lengths of sequences in X\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    X_buckets = []\n",
    "    X_buckets_padded = []\n",
    "    Y_buckets = []\n",
    "    X_buckets_packed = []\n",
    "    \n",
    "    \n",
    "    if features is None and bio_features is None:  \n",
    "        use_features = False\n",
    "    else:\n",
    "        use_features = True\n",
    "        \n",
    "    sequence_lens = np.array(sequence_lens)\n",
    "    \n",
    "    for b_id, idxs_in_bucket in enumerate(idxs_in_buckets):\n",
    "        \n",
    "        seq_lens_bucket = sequence_lens[idxs_in_bucket]\n",
    "        \n",
    "        # ignore empty buckets\n",
    "        if len(idxs_in_bucket)==0:\n",
    "            continue\n",
    "\n",
    "        num_elements_in_bucket = len(idxs_in_bucket)\n",
    "        \n",
    "        # indices of the data points in this bucket\n",
    "        x_idxs = np.array(idxs_in_bucket)\n",
    "        \n",
    "        #all_idxs = np.concatenate([all_idxs, x_idxs], axis = 0)\n",
    "        \n",
    "        # indices to the bucket contents (which is a set of indices starting from 0 ending in num_elements_in_bucket-1 )\n",
    "        # for future compatibility - for example, selecting a fraction of the elements from each bucket, in which case, select a subset of this list of indices\n",
    "        idx_in_bucket = np.arange(num_elements_in_bucket)\n",
    "        \n",
    "        X_bucket = X[ x_idxs[ idx_in_bucket ] ]   \n",
    "        Y_bucket = Y[ x_idxs[ idx_in_bucket ] ]\n",
    "        \n",
    "         \n",
    "        X_bucket_T = [torch.Tensor(x) for x in X_bucket]\n",
    "        #print(len(X_bucket_T))\n",
    "        \n",
    "        # in pytorch, in order to pad a list of sequences to a specific length, we need a (possibly dummy) sequence of that length\n",
    "        X_bucket_T.append(torch.zeros((bucket_high_lims[b_id],)))\n",
    "        X_bucket_padded = pad_sequence(X_bucket_T, batch_first = True, padding_value = 0.0 )\n",
    "        X_bucket_padded = X_bucket_padded.numpy()\n",
    "\n",
    "        #print(X_bucket_padded.shape)\n",
    "        \n",
    "        # delete the last dummy row that served as element with padding length \n",
    "        X_bucket_padded = np.delete(X_bucket_padded, -1, axis = 0)        \n",
    "\n",
    "        #print(X_bucket_padded.shape)\n",
    "        \n",
    "        # one-hot encoding\n",
    "        X_bucket_padded = (np.arange(X_bucket_padded.max()+1) == X_bucket_padded[:,:,None]).astype(dtype='float32') \n",
    "        X_bucket_padded = np.delete(X_bucket_padded,0, axis=-1)\n",
    "        \n",
    "        #X_bucket = [X_bucket_padded[idx][:seq_lens_bucket[idx],:] for idx in range(X_bucket_padded.shape[0]) ]\n",
    "        \n",
    "        #X_buckets.append(X_bucket)\n",
    "        if(use_features):\n",
    "            \n",
    "            if not features is None:\n",
    "                features_expanded = np.expand_dims(features[x_idxs[idx_in_bucket],:],-1)  # add a last dimension - feature dimension (length =1)\n",
    "            else:\n",
    "                features_expanded = np.empty((0,0))\n",
    "              \n",
    "            if not bio_features is None:\n",
    "                bio_features_expanded = np.expand_dims(bio_features[x_idxs[idx_in_bucket],:],-1)\n",
    "            else:\n",
    "                bio_features_expanded = np.empty((0,0))\n",
    "            \n",
    "            X_bucket_padded = ( X_bucket_padded, features_expanded, bio_features_expanded )  \n",
    "            \n",
    "            print(X_bucket_padded[0].shape, X_bucket_padded[1].shape, X_bucket_padded[2].shape)\n",
    "        \n",
    "        else:\n",
    "            X_bucket_padded = ( X_bucket_padded, )\n",
    "            \n",
    "        X_buckets_padded.append(X_bucket_padded)        \n",
    "        Y_buckets.append(Y_bucket)\n",
    "    \n",
    "        #print(torch.Tensor(X_bucket_padded).shape)\n",
    "        #sorted_lens = list(sorted(sequence_lens[idx_buckets[b_id]]))\n",
    "        #print(sorted_lens[:10])\n",
    "\n",
    "        # returns a packed sequences with #batches equal to the size of the maximum seq length, where a batch size\n",
    "        # corresponds to how many sequences have a valid input at that time step\n",
    "        #packed_input = pack_padded_sequence(torch.Tensor(X_bucket_padded), seq_lens_buckets, batch_first=True, enforce_sorted=False)\n",
    "        #X_buckets_packed.append(packed_input)\n",
    "        \n",
    "    return X_buckets_padded,Y_buckets#, X_buckets_packed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(use_features):\n",
    "    \n",
    "    X_buckets, Y_buckets = create_data_buckets(seq_int_coded, labels, idxs_in_buckets, bucket_high_lims, sequence_lens, features, bio_features)\n",
    "    \n",
    "    if (use_testset):\n",
    "        test_X_buckets, test_Y_buckets = create_data_buckets(test_seq_int_coded, test_labels, test_idxs_in_buckets, test_bucket_high_lims, test_sequence_lens, test_features, test_bio_features)\n",
    "        \n",
    "else:\n",
    "\n",
    "    X_buckets, Y_buckets = create_data_buckets(seq_int_coded, labels, idxs_in_buckets, bucket_high_lims, sequence_lens)\n",
    "\n",
    "    if (use_testset):\n",
    "        test_X_buckets, test_Y_buckets = create_data_buckets(test_seq_int_coded, test_labels, test_idxs_in_buckets, test_bucket_high_lims, test_sequence_lens)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for b_id in range(len(X_buckets)):\n",
    "#     idxs_bucket = idx_buckets[b_id]\n",
    "#     for rand_idx in np.random.randint(len(X_buckets[b_id]), size = 10):\n",
    "#         print(X_buckets[b_id][rand_idx].shape, sequence_lens[idxs_bucket[rand_idx]])\n",
    "        \n",
    "#     print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(X_buckets), len(idxs_in_buckets), torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bucket_content = X_buckets[0]\n",
    "# seq_elements = bucket_content[0]\n",
    "# seq_element = seq_elements[0,:]\n",
    "\n",
    "# len(bucket_content), seq_element.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## sanity check \n",
    "[converting one-hot encoded sequences to DNA seq and checking against the corresponding original from the dataset]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # sanity check [converting one-hot encoded sequences to DNA seq and checking against the corresponding original from the dataset]\n",
    "\n",
    "# def compute_kmer_counts(seq, k, alphabet, do_sliding = True):\n",
    "\n",
    "#     kmers_list = []\n",
    "#     kmers_list.append(alphabet)\n",
    "    \n",
    "#     for k_idx in range(k-1):\n",
    "#         kmers_list.append([a+b for a in kmers_list[k_idx] for b in alphabet ])\n",
    "\n",
    "#     seq_lst = list(seq)\n",
    "#     subs = [\"\".join(seq_lst[idx:idx+k]) for idx in range(len(seq)-k+1)]\n",
    "#     kmers = kmers_list[k-1]\n",
    "#     kmers_counts = [subs.count(kmer) for kmer in kmers]\n",
    "    \n",
    "#     return kmers_counts\n",
    "\n",
    "# data_check_arr = []\n",
    "# feature_check_arr = []\n",
    "# incorrect_feature_match_indices = []\n",
    "# incorrect_feature_match_seqs = []\n",
    "\n",
    "# for b_idx in range(len(X_buckets)):\n",
    "#     x_bucket_padded = X_buckets[b_idx]\n",
    "\n",
    "#     if(use_features):\n",
    "#         features_bucket = x_bucket_padded[1]\n",
    "#         x_bucket_padded = x_bucket_padded[0]\n",
    "        \n",
    "#     y_bucket = Y_buckets[b_idx]\n",
    "#     idx_bucket = idxs_in_buckets[b_idx]\n",
    "    \n",
    "#     for seq_idx in range(x_bucket_padded.shape[0]):\n",
    "#         seq_oh = x_bucket_padded[seq_idx,:,:]\n",
    "        \n",
    "#         seq_str = \"\".join([unique_DNAs[dna_idx] for dna_idx in np.argmax(seq_oh, axis = -1)]) # convert from one-hot to DNA\n",
    "#         seq_str = seq_str[:sequence_lens[idx_bucket[seq_idx]]]  # get rid of the padding\n",
    "        \n",
    "#         seq = sequences[idx_bucket[seq_idx]]  # original seq retrieved according to saved indices of the one-hot data\n",
    "        \n",
    "#         if(use_features):\n",
    "#             idx_range = [[0,11],[12,59],[60,251]]\n",
    "#             feature_check_acc = []\n",
    "#             #seq = seq[-500:] + seq[:-500]\n",
    "#             for k in range(1,4):\n",
    "#                 kmer_counts1 = compute_kmer_counts(seq_str[:200], k, unique_DNAs)    \n",
    "#                 kmer_counts2 = compute_kmer_counts(seq_str[200:400], k, unique_DNAs)    \n",
    "#                 kmer_counts3 = compute_kmer_counts(seq_str[400:], k, unique_DNAs)    \n",
    "            \n",
    "#                 features_computed= np.array(kmer_counts1+kmer_counts2+kmer_counts3)\n",
    "#                 features_from_file = features_bucket[seq_idx,idx_range[k-1][0]:idx_range[k-1][1]+1].squeeze()\n",
    "#                 feature_check = np.all( features_computed == features_from_file)\n",
    "#                 #print(feature_check)\n",
    "#                 #print(features_computed)\n",
    "#                 #print(features_from_file)\n",
    "#                 #print()\n",
    "#                 #print(features_bucket[seq_idx,idx_range[k-1][0]:idx_range[k-1][1]+1].squeeze())\n",
    "#                 #print()\n",
    "#                 feature_check_acc.append(feature_check)\n",
    "#             #print()    \n",
    "#             feature_check_arr.append(np.all(feature_check_acc))\n",
    "            \n",
    "#             if not np.all(feature_check_acc):\n",
    "#                 #print(seq_idx)\n",
    "#                 incorrect_feature_match_indices.append(seq_idx)\n",
    "#                 incorrect_feature_match_seqs.append(seq)\n",
    "#                 for k in range(1,4):\n",
    "#                     kmer_counts1 = compute_kmer_counts(seq_str[:200], k)    \n",
    "#                     kmer_counts2 = compute_kmer_counts(seq_str[200:400], k)    \n",
    "#                     kmer_counts3 = compute_kmer_counts(seq_str[400:], k)    \n",
    "\n",
    "#                     features_computed= np.array(kmer_counts1+kmer_counts2+kmer_counts3)\n",
    "#                     features_from_file = features_bucket[seq_idx,idx_range[k-1][0]:idx_range[k-1][1]+1].squeeze()\n",
    "#                     feature_check = np.all( features_computed == features_from_file)\n",
    "#                     feature_check_acc.append(feature_check)\n",
    "#                     print(len(kmer_counts1), len(kmer_counts2), len(kmer_counts3),features_computed.shape, features_from_file.shape)\n",
    "#                 print(seq)\n",
    "#                 print(seq_oh.shape, len(seq))\n",
    "#                 print()\n",
    "#         data_check_arr.append(seq == seq_str)\n",
    "        \n",
    "        \n",
    "# print(\"One-hot encoding and sequence bucketing is correct:\", np.all(np.array(data_check_arr)))\n",
    "\n",
    "# if(use_features):\n",
    "#     print(\"feature computation and bucketing is correct:\", np.all(np.array(feature_check_arr)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# s = \"CATCACTATCATCATCATCATCACACCGCCACCATCACCACCACCACCATCACACCACCACCACCACCACCACCGTCACCATCACTATCATCATCATCACACCACCACCACCACCACCATCATCACTATCATCATCATCATCACCACACCACCACCATCACCACCACCACCATCACACCACCACCACCACTGTCACCACCACCACCACCACCATCACACCACCACCACCGCCACCGTCGTCATCACTATCATCATCATCATCACACCACCACCATCACCACCACCATCACACCACCACCACCACTACTGTCATCACTATCATCATCATCACACCACCACCATCACCATCATCACACCACCACCACCATCACCATCACCATCATCACACCACCACCACCATCACACCACCACCACCATCATTATCATCACCATCATCACACCACCACCACCACCATCACCATCACCATCATCACACCACCACCACCATTACCATCACCATCATCACACCACCACCACCACCATTACCATCACCATCATCACACCACCACCATCACCATCATCACACCACCACCACCATCACCATCATCACACCACCACCAT\"\n",
    "# kcount = compute_kmer_counts(s[400:], 1)\n",
    "\n",
    "# print(s[400:])\n",
    "# kcount, s[400:].count('G'), len(s[400:]), len(s),[s[400:].count(c) for c in \"ACGT\"]\n",
    "# incorrects = [sequences[idxs_buckets[0][i]] for i in incorrect_feature_match_indices]\n",
    "# [print(incorrect,\"\\n\\n\") for incorrect in incorrects]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(incorrect_feature_match_indices), incorrect_feature_match_seqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Verify that in a packed seq, batch_size equals the number of sequences having a valid input at (index+1) of the index batch_size in batch_sizes\n",
    "\n",
    "# sequence_lens_np = np.array(sequence_lens)\n",
    "# batch_size_idx = random.choice(range(len(X_buckets_packed[0].batch_sizes)))\n",
    "# batch_size = X_buckets_packed[0].batch_sizes[batch_size_idx]\n",
    "# batch_size.numpy() == np.sum(sequence_lens_np[idx_buckets[0]]>=(batch_size_idx+1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Metrics:\n",
    "    \n",
    "    def __init__(self, benchmark_acc = None, benchmark_f1 = None, benchmark_spec = None, benchmark_sen = None, benchmark_auc = None, benchmark_mcc = None):\n",
    "        self.b_accuracy = benchmark_acc\n",
    "        self.b_f1 = benchmark_f1\n",
    "        self.b_sensitivity = benchmark_sen\n",
    "        self.b_specificity = benchmark_spec\n",
    "        self.b_auc_roc = benchmark_auc\n",
    "        self.b_mcc = benchmark_mcc\n",
    "        \n",
    "    def reset_history(self):\n",
    "        self.accuracies = []\n",
    "        self.f1s = []\n",
    "        self.recalls = []\n",
    "        self.precisions = []\n",
    "        self.sensitivity = []\n",
    "        self.specificity = []\n",
    "        self.auc_roc = []        \n",
    "        self.tp_tn_fp_fn = []\n",
    "        self.mcc = []\n",
    "\n",
    "    @classmethod\n",
    "    def compute_mcc(cls, y_true, y_pred):\n",
    "        cm = confusion_matrix(y_true, y_pred)\n",
    "        tn, fp, fn, tp = cm.ravel()\n",
    "        mcc = (tp*tn - fp*fn) / (np.sqrt(  (tp+fp)*(tp+fn)*(tn+fp)*(tn+fn)  ) + 1e-8) \n",
    "        return mcc\n",
    "    \n",
    "    @classmethod\n",
    "    def compute_sensitivity(cls, y_true, y_pred):\n",
    "        cm = confusion_matrix(y_true, y_pred)\n",
    "        tn, fp, fn, tp = cm.ravel()\n",
    "        sensitivity = tp/(tp+fn)\n",
    "        return sensitivity\n",
    "    \n",
    "    @classmethod\n",
    "    def compute_specificity(cls, y_true, y_pred):\n",
    "        cm = confusion_matrix(y_true, y_pred)\n",
    "        tn, fp, fn, tp = cm.ravel()\n",
    "        specificity = tn/(fp+tn)\n",
    "        return specificity\n",
    "    \n",
    "    @classmethod\n",
    "    def compute_accuracy(cls, y_true, y_pred):\n",
    "        accuracy = (y_true==y_pred).sum()/len(y_true)\n",
    "        return accuracy\n",
    "    \n",
    "    def compute_metrics(self, y_true, y_pred, epoch, do_print = False, store_vals = False):\n",
    "        accuracy = (y_true==y_pred).sum()/len(y_true)\n",
    "    \n",
    "        cm = confusion_matrix(y_true, y_pred)\n",
    "        #cm = cm.astype('float') / cm.sum(axis = 1)[:, np.newaxis]\n",
    "        tn, fp, fn, tp = cm.ravel()\n",
    "\n",
    "        specificity = tn/(fp+tn)\n",
    "        sensitivity = tp/(tp+fn)\n",
    "        f1 = f1_score(y_true, y_pred)\n",
    "        recall = recall_score(y_true, y_pred)\n",
    "        precision = precision_score(y_true, y_pred)\n",
    "        auc_roc = roc_auc_score( y_true, y_pred )  \n",
    "        mcc = (tp*tn - fp*fn) / (np.sqrt(  (tp+fp)*(tp+fn)*(tn+fp)*(tn+fn)  ) + 1e-8) \n",
    "        \n",
    "        results = {'accuracy': accuracy, 'specificity':specificity, 'sensitivity':sensitivity, 'f1_score':f1, 'auc_roc':auc_roc, 'recall':recall, \n",
    "                   'precision': precision, 'tp_fp_tn_fn':{'tp':tp, 'fp':fp, 'tn':tn, 'fn':fn}, 'mcc': mcc }\n",
    "        \n",
    "        if(store_vals):\n",
    "            self.accuracies.append(accuracy)\n",
    "            self.specificity.append(specificity)\n",
    "            self.sensitivity.append(sensitivity)\n",
    "            self.f1s.append(f1)\n",
    "            self.auc_roc.append(auc_roc)        \n",
    "            self.recalls.append(recall)\n",
    "            self.precisions.append(precision)\n",
    "            self.tp_tn_fp_fn.append((tp, tn, fp, fn)) \n",
    "            self.mcc.append(mcc) \n",
    "            \n",
    "        if(do_print):\n",
    "            is_beaten_acc = False\n",
    "            is_beaten_f1 = False\n",
    "            is_beaten_sens = False\n",
    "            is_beaten_spec = False\n",
    "            is_beaten_auc = False\n",
    "            is_beaten_mcc = False\n",
    "            \n",
    "            if( not self.b_accuracy is None and accuracy > self.b_accuracy ):\n",
    "                is_beaten_acc = True\n",
    "            \n",
    "            if( not self.b_f1 is None and f1 > self.b_f1 ):\n",
    "                is_beaten_f1 = True\n",
    "            \n",
    "            if( not self.b_sensitivity is None and sensitivity > self.b_sensitivity ):\n",
    "                is_beaten_sens = True\n",
    "\n",
    "            if( not self.b_specificity is None and specificity > self.b_specificity ):\n",
    "                is_beaten_spec = True\n",
    "\n",
    "            if( not self.b_auc_roc is None and auc_roc > self.b_auc_roc ):\n",
    "                is_beaten_auc = True\n",
    "\n",
    "            if( not self.b_mcc is None and mcc > self.b_mcc ):\n",
    "                is_beaten_mcc = True\n",
    "                \n",
    "            print(f'_________________________________________ METRICS for epoch {epoch} _______________________________________________________')\n",
    "            print('accuracy '.ljust(16),f':{accuracy:.5f} ',f' bmark:({self.b_accuracy:.4f})',f' accuracy beaten:{is_beaten_acc}')\n",
    "            #print('f1_score '.ljust(16),f':{f1:.5f} ',f' bmark:({self.b_f1:.4f})',f' f1 beaten:{is_beaten_f1}')\n",
    "            print('sensitivity '.ljust(16),f':{sensitivity:.5f} ',f' bmark:({self.b_sensitivity:.4f})',f' sen.beaten:{is_beaten_sens}')\n",
    "            print('specificity '.ljust(16),f':{specificity:.5f} ',f' bmark:({self.b_specificity:.4f})',f' spe.beaten:{is_beaten_spec}')\n",
    "            #print('auc_roc '.ljust(16),f':{auc_roc:.5f} ',f' bmark:({self.b_auc_roc:.4f})',f' spe.beaten:{is_beaten_auc}')            \n",
    "            print('mcc '.ljust(16),f':{mcc:.5f} ',f' bmark:({self.b_mcc:.4f})',f' mcc.beaten:{is_beaten_mcc}')\n",
    "            \n",
    "            #print('precision '.ljust(16),f':{precision} ')\n",
    "            #print('recall '.ljust(16),f':{recall}')\n",
    "            print( '---------------------') \n",
    "            print(f'| tp:{tp} '.ljust(9),f'| fp:{fp}'.ljust(9),'|')\n",
    "            print(f'| fn:{fn} '.ljust(9),f'| tn:{tn}'.ljust(9),'|')\n",
    "            print('---------------------') \n",
    "\n",
    "            #if(is_beaten_acc and is_beaten_f1 and is_beaten_sens and is_beaten_spec and is_beaten_auc and is_beaten_mcc):\n",
    "            if(is_beaten_acc and is_beaten_sens and is_beaten_spec and is_beaten_mcc):\n",
    "                print('+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++')\n",
    "                print(f'##############################  BEATEN ALL at epoch:{epoch} ##############################################################')\n",
    "                print('+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++')\n",
    "        \n",
    "        \n",
    "        return results\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning rate schedular"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://discuss.pytorch.org/t/how-to-implement-torch-optim-lr-scheduler-cosineannealinglr/28797\n",
    "\n",
    "class OneCycleCosineAnnealing:\n",
    "    \n",
    "    def __init__(self, optim, steps_per_cycle, cycles_per_round, max_lr = 1e-3, start_lr = None, end_lr = None, half_cycle_len_pct = .3, decay_rate = 1.0):\n",
    "        init_div_factor, last_div_factor = 25.0, 1e4\n",
    "        self.optim = optim\n",
    "        self.max_lr = max_lr\n",
    "        self.decay_rate = decay_rate\n",
    "        # one round is during a span for which the max lr is constant. It is decayed at the start of every round\n",
    "        self.cycles_per_round = int(cycles_per_round) \n",
    "        self.start_lr = self.max_lr/init_div_factor if start_lr is None else start_lr\n",
    "        self.end_lr = self.max_lr/(init_div_factor*last_div_factor) if end_lr is None else end_lr\n",
    "        for param_group in self.optim.param_groups:\n",
    "            param_group['lr'] = self.start_lr\n",
    "        self.steps_per_cycle = int(steps_per_cycle)\n",
    "        self.half_cycle_len_pct = half_cycle_len_pct\n",
    "        self.peak_step_idx = int(self.steps_per_cycle*self.half_cycle_len_pct)\n",
    "        print(\"self.start_lr\",self.start_lr)\n",
    "        print(\"self.max_lr\",self.max_lr)\n",
    "        print(\"self.end_lr\",self.end_lr)\n",
    "        \n",
    "        self.coslr1 = lr_scheduler.CosineAnnealingLR(self.optim, T_max = self.peak_step_idx, eta_min = self.max_lr)        \n",
    "        self.coslr2 = lr_scheduler.CosineAnnealingLR(self.optim, T_max = self.steps_per_cycle - self.peak_step_idx, eta_min = self.end_lr)\n",
    "        self.step_idx = 0\n",
    "        self.cycle_idx = 0\n",
    "        self.lrs = []\n",
    "        self.lrs.append(self.optim.param_groups[0]['lr'])\n",
    "        \n",
    "    def step(self):\n",
    "        self.step_idx+=1\n",
    "        if(self.step_idx < self.peak_step_idx):\n",
    "            self.coslr1.step()  \n",
    "        elif(self.step_idx <= self.steps_per_cycle):\n",
    "            self.coslr2.step()\n",
    "        else:\n",
    "            self.cycle_idx +=1\n",
    "            if(self.cycle_idx%self.cycles_per_round==0):\n",
    "                self.max_lr*=self.decay_rate\n",
    "            self.coslr1 = lr_scheduler.CosineAnnealingLR(self.optim, T_max = self.peak_step_idx, eta_min = self.max_lr)  \n",
    "            self.coslr2 = lr_scheduler.CosineAnnealingLR(self.optim, T_max = self.steps_per_cycle - self.peak_step_idx, eta_min = self.end_lr)\n",
    "            self.coslr1.step()           \n",
    "            self.step_idx = 1\n",
    "            \n",
    "        self.lrs.append(self.optim.param_groups[0]['lr'])\n",
    "        \n",
    "            \n",
    "model = torch.nn.Linear(1, 1)\n",
    "\n",
    "max_lr = 1e-3\n",
    "init_lr = max_lr/25.0\n",
    "last_lr = max_lr/25e4\n",
    "\n",
    "\n",
    "num_epochs = 50\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = init_lr)\n",
    "\n",
    "for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = init_lr\n",
    "\n",
    "scheduler = OneCycleCosineAnnealing(optimizer, num_epochs, 5, max_lr = 1e-3, decay_rate = .95)\n",
    "\n",
    "lrs = []\n",
    "\n",
    "lrs.append(optimizer.param_groups[0]['lr'])    \n",
    "\n",
    "for _ in range(10*num_epochs):\n",
    "    #print(\"-\", l.last_epoch, optim.param_groups[0]['lr'])\n",
    "    scheduler.step()    \n",
    "    lrs.append(optimizer.param_groups[0]['lr'])\n",
    "print(\"min:\",min(lrs))        \n",
    "plt.plot(lrs)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class ListDataset(torch.utils.data.Dataset):\n",
    "    \n",
    "#     def __init__(self, data, label) -> None:\n",
    "#         super().__init__()\n",
    "#         self.data = data\n",
    "#         self.label = label\n",
    "        \n",
    "#         assert len(self.data) == len(self.label)\n",
    "    \n",
    "#     def __getitem__(self, index):\n",
    "#         return (self.data[index], self.label[index])\n",
    "    \n",
    "#     def __len__(self):\n",
    "#         return len(self.data)\n",
    "    \n",
    "class LSTM_CNN_Net(nn.Module):\n",
    "    \n",
    "    def __init__(self, num_features, seq_info):\n",
    "        super(LSTM_CNN_Net, self).__init__()\n",
    "        \n",
    "        hidden_sz, seq_len = seq_info\n",
    "        \n",
    "        self.num_features = num_features\n",
    "        self.hidden_sz = hidden_sz\n",
    "        self.seq_len = seq_len\n",
    "        self.num_layers = 2\n",
    "        self.is_bidirectional = True\n",
    "        \n",
    "        #Apply LSTM on the input first, then apply 2-D convolution the final layer's output.        \n",
    "        self.lstm = nn.LSTM(input_size = self.num_features, \n",
    "                            hidden_size = self.hidden_sz, \n",
    "                            batch_first = True, bidirectional = self.is_bidirectional, num_layers = self.num_layers)\n",
    "        \n",
    "        self.conv2D_1 = nn.Conv2d(in_channels = 1, out_channels= 32, kernel_size= 3)\n",
    "        self.batchnorm2D_1 = nn.BatchNorm2d(num_features = self.conv2D_1.out_channels)\n",
    "        \n",
    "        self.maxpool2D_1 = torch.nn.MaxPool2d(kernel_size=2)\n",
    "        self.conv2D_2 = nn.Conv2d(in_channels = 32, out_channels= 64, kernel_size= 5)\n",
    "        self.batchnorm2D_2 = nn.BatchNorm2d(num_features = self.conv2D_2.out_channels)\n",
    "        \n",
    "        #self.linear1_allstatesout = nn.Linear(in_features = self.seq_len*self.num_lstm_directions*self.hidden_sz, out_features= self.seq_len*self.hidden_sz//4)        \n",
    "        self.linear1 = nn.Linear(in_features = 5*147*64, out_features= 48)\n",
    "        self.linear2 = nn.Linear(in_features = 48, out_features= 2)\n",
    "        \n",
    "        self.dropout1 = nn.Dropout(0.8)\n",
    "        self.dropout2 = nn.Dropout(0.5)\n",
    "        \n",
    "#         for name, param in self.lstm.named_parameters():\n",
    "#             if 'bias' in name:\n",
    "#                 nn.init.constant_(param, 0.0)\n",
    "#             elif 'weight' in name:\n",
    "#                 nn.init.xavier_normal_(param)\n",
    "        \n",
    "    def forward(self, x, seq_lengths):\n",
    "#         x_packed = pack_padded_sequence(x, seq_lengths, batch_first=True, enforce_sorted=False)\n",
    "#         packed_out, (hn, cn) = self.lstm(x_packed)        \n",
    "#         out, input_sizes = pad_packed_sequence(packed_out, batch_first=True)\n",
    "\n",
    "\n",
    "        #output: output of shape (batch, seq_len, num_directions * hidden_size) are output features (h_t) from the last layer of the LSTM, for each time step \n",
    "        #                        [if batch_first was True]\n",
    "        #hn = output of last time step sz: (num_direction*num_layers, batch_sz, hidden_sz)\n",
    "        #cn = cell state of last time step sz: (num_direction*num_layers, batch_sz, hidden_sz)\n",
    "        #input is: (batch_sz, sequence_length, num_feature_in_each_time_step) since batch_first was True in LSTM initialization\n",
    "        out, (hn, cn) = self.lstm(x)\n",
    "\n",
    "        out = torch.unsqueeze(out, dim = 1)\n",
    "        out = self.conv2D_1(out)\n",
    "        out = self.batchnorm2D_1(out)\n",
    "        out = F.relu(out)\n",
    "        \n",
    "        out = self.dropout2(out)\n",
    "        out = self.maxpool2D_1(out)\n",
    "        \n",
    "        out = self.conv2D_2(out)\n",
    "        out = self.batchnorm2D_2(out)\n",
    "        out = F.relu(out)\n",
    "        out = self.dropout2(out)\n",
    "        out = self.maxpool2D_1(out)\n",
    "        \n",
    "        #print(out.shape)\n",
    "        out = self.linear1(out.contiguous().view(-1, 5*147*64))\n",
    "        out = F.relu(out)\n",
    "        out = self.dropout1(out)\n",
    "        \n",
    "        out = self.linear2(out)\n",
    "        \n",
    "        #l2allstates = torch.sqrt(torch.sum(out**2, dim = 1))\n",
    "        #last_state_cmbnd = torch.cat([hn, cn], dim = -1)\n",
    "        #l2allstates = self.batchnrm(l2allstates)\n",
    "        #net_out = self.linear2(l2allstates.view(-1, 2*self.hidden_sz))\n",
    "        #net_out = self.linear2(last_state_cmbnd.view(-1, 2*self.hidden_sz))\n",
    "        \n",
    "        return out\n",
    "\n",
    "    def getname(self):\n",
    "        return \"LSTM_CNN_Net\"\n",
    "    \n",
    "    \n",
    "# The network\n",
    "class RNA_Net(nn.Module):\n",
    "    \n",
    "    def __init__(self, num_features, hidden_sz):\n",
    "        super(RNA_Net, self).__init__()\n",
    "        \n",
    "        self.num_features = num_features\n",
    "        self.hidden_sz = hidden_sz\n",
    "        self.lstm = nn.LSTM(input_size = num_features, hidden_size = hidden_sz, batch_first = True, bidirectional = True)\n",
    "        self.batchnrm = nn.BatchNorm1d(hidden_sz)\n",
    "        #self.linear = nn.Linear(in_features = hidden_sz, out_features= 2)\n",
    "        self.linear2 = nn.Linear(in_features = 2*hidden_sz, out_features= 2)\n",
    "        \n",
    "#         for name, param in self.lstm.named_parameters():\n",
    "#             if 'bias' in name:\n",
    "#                 nn.init.constant_(param, 0.0)\n",
    "#             elif 'weight' in name:\n",
    "#                 nn.init.xavier_normal_(param)\n",
    "        \n",
    "    def forward(self, x, seq_lengths):\n",
    "#         x_packed = pack_padded_sequence(x, seq_lengths, batch_first=True, enforce_sorted=False)\n",
    "#         packed_out, (hn, cn) = self.lstm(x_packed)        \n",
    "#         out, input_sizes = pad_packed_sequence(packed_out, batch_first=True)\n",
    "\n",
    "        out, (hn, cn) = self.lstm(x)        \n",
    "        l2allstates = torch.sqrt(torch.sum(out**2, dim = 1))\n",
    "        last_state_cmbnd = torch.cat([hn, cn], dim = -1)\n",
    "        #l2allstates = self.batchnrm(l2allstates)\n",
    "        net_out = self.linear2(l2allstates.view(-1, 2*self.hidden_sz))\n",
    "        #net_out = self.linear2(last_state_cmbnd.view(-1, 2*self.hidden_sz))\n",
    "        \n",
    "        return net_out\n",
    "\n",
    "    \n",
    "    \n",
    "class CNPPNet(nn.Module):\n",
    "    \n",
    "    def __init__(self, num_features, hidden_sz):\n",
    "        super(CNPPNet, self).__init__()\n",
    "        #super().__init__()\n",
    "        \n",
    "        self.hidden_sz = hidden_sz\n",
    "        # inception layer 1\n",
    "        self.conv11 = nn.Conv1d(num_features,64, kernel_size= 3, padding=1)\n",
    "        self.conv12 = nn.Conv1d(num_features,64, kernel_size= 5, padding=2)\n",
    "        self.conv13 = nn.Conv1d(num_features,32, kernel_size= 7, padding=3)\n",
    "        \n",
    "        # inception layer 2\n",
    "        self.conv21 = nn.Conv1d(160,64, kernel_size= 3, padding=1)\n",
    "        self.conv22 = nn.Conv1d(160,64, kernel_size= 5, padding=2)\n",
    "        self.conv23 = nn.Conv1d(160,32, kernel_size= 7, padding=3)\n",
    "\n",
    "        \n",
    "        # inception layer 3\n",
    "        self.conv31 = nn.Conv1d(160,64, kernel_size= 3, padding=1)\n",
    "        self.conv32 = nn.Conv1d(160,64, kernel_size= 5, padding=2)\n",
    "        self.conv33 = nn.Conv1d(160,32, kernel_size= 7, padding=3)\n",
    "        \n",
    "        # inception layer 3\n",
    "        self.conv41 = nn.Conv1d(160,64, kernel_size= 3, padding=1)\n",
    "        self.conv42 = nn.Conv1d(160,64, kernel_size= 5, padding=2)\n",
    "        self.conv43 = nn.Conv1d(160,32, kernel_size= 7, padding=3)\n",
    "        \n",
    "        self.maxpool1 = torch.nn.MaxPool1d(kernel_size=2)\n",
    "        self.maxpool2 = torch.nn.MaxPool1d(kernel_size=4)\n",
    "\n",
    "        self.conv3 = nn.Conv1d(160,32, kernel_size= 7)\n",
    "        #self.lstm = nn.LSTM(input_size = 64, hidden_size = hidden_sz, batch_first = True, bidirectional = True)\n",
    "        self.linear1 = nn.Linear(in_features = 32*15, out_features= 16)\n",
    "        self.linear2 = nn.Linear(in_features = 16, out_features= 2)\n",
    " \n",
    "        #self.dropout1 = nn.Dropout(0.4)\n",
    "        self.dropout1 = nn.Dropout(0.5)\n",
    "        self.dropout2 = nn.Dropout(0.6)\n",
    "        self.dropout3 = nn.Dropout(0.7)\n",
    "\n",
    "        \n",
    "    def forward(self,x, features):\n",
    "        x = x.permute(0,2,1)\n",
    "        \n",
    "        #print(x.shape)\n",
    "        out1 = self.conv11(x)\n",
    "        out1 = F.relu(out1)\n",
    "        out2 = self.conv12(x)\n",
    "        out2 = F.relu(out2)\n",
    "        out3 = self.conv13(x)\n",
    "        out3 = F.relu(out3)\n",
    "        out = torch.cat((out1,out2,out3),1)\n",
    "        #out = F.relu(out)\n",
    "        out = self.dropout1(out)\n",
    "        out = self.maxpool1(out)\n",
    "        \n",
    "        out1 = self.conv21(out)\n",
    "        out1 = F.relu(out1)\n",
    "        out2 = self.conv22(out)\n",
    "        out2 = F.relu(out2)\n",
    "        out3 = self.conv23(out)\n",
    "        out3 = F.relu(out3)\n",
    "        out = torch.cat((out1,out2,out3),1)\n",
    "        #out = F.relu(out)\n",
    "        out = self.dropout1(out)\n",
    "        out = self.maxpool1(out)\n",
    "\n",
    "        out1 = self.conv31(out)\n",
    "        out1 = F.relu(out1)\n",
    "        out2 = self.conv32(out)\n",
    "        out2 = F.relu(out2)\n",
    "        out3 = self.conv33(out)\n",
    "        out3 = F.relu(out3)\n",
    "        out = torch.cat((out1,out2,out3),1)\n",
    "        #out = F.relu(out)\n",
    "        out = self.dropout1(out)\n",
    "        out = self.maxpool1(out)\n",
    "\n",
    "        \n",
    "        out1 = self.conv41(out)\n",
    "        out1 = F.relu(out1)\n",
    "        out2 = self.conv42(out)\n",
    "        out2 = F.relu(out2)\n",
    "        out3 = self.conv43(out)\n",
    "        out3 = F.relu(out3)\n",
    "        out = torch.cat((out1,out2,out3),1)\n",
    "        #out = F.relu(out)\n",
    "        out = self.dropout1(out)\n",
    "        out = self.maxpool1(out)\n",
    "        \n",
    "        \n",
    "        \n",
    "        out = self.conv3(out)\n",
    "        out = self.dropout1(out)\n",
    "        out = self.maxpool1(out)\n",
    "        #out = out.permute(0,2,1)\n",
    "        \n",
    "        #out, (hn, cn)  = self.lstm(out)\n",
    "        \n",
    "        #l2allstates = torch.sqrt(torch.sum(out**2, dim = 1))\n",
    "        #last_state_cmbnd = torch.cat([hn, cn], dim = -1)\n",
    "        \n",
    "        #l2allstates = self.batchnrm(l2allstates)\n",
    "        #print(out.shape)\n",
    "        \n",
    "        out = self.linear1(out.view(-1, 32*15))\n",
    "        out = self.dropout2(out)\n",
    "        out = self.linear2(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "    def getname(self):\n",
    "        return \"CNPPNet\"\n",
    "    \n",
    "    \n",
    "    \n",
    "class CNPPNet_Hybrid(nn.Module):\n",
    "    \n",
    "    def __init__(self, num_features, hidden_sz):\n",
    "        super(CNPPNet_Hybrid, self).__init__()\n",
    "        #super().__init__()\n",
    "        \n",
    "        self.hidden_sz = hidden_sz\n",
    "        # inception layer 1\n",
    "        self.conv11 = nn.Conv1d(num_features,64, kernel_size= 3, padding=1)\n",
    "        self.conv12 = nn.Conv1d(num_features,64, kernel_size= 5, padding=2)\n",
    "        self.conv13 = nn.Conv1d(num_features,32, kernel_size= 7, padding=3)\n",
    "        \n",
    "        # inception layer 2\n",
    "        self.conv21 = nn.Conv1d(160,64, kernel_size= 3, padding=1)\n",
    "        self.conv22 = nn.Conv1d(160,64, kernel_size= 5, padding=2)\n",
    "        self.conv23 = nn.Conv1d(160,32, kernel_size= 7, padding=3)\n",
    "        \n",
    "        self.maxpool1d = torch.nn.MaxPool1d(kernel_size=4)\n",
    "        \n",
    "        self.conv3 = nn.Conv1d(160,32, kernel_size= 7)\n",
    "        #self.lstm = nn.LSTM(input_size = 64, hidden_size = hidden_sz, batch_first = True, bidirectional = True)\n",
    "        #self.linear1 = nn.Linear(in_features = 32*29 , out_features= 16)\n",
    "        \n",
    "        self.linear1 = nn.Linear(in_features = 32*29 + 64, out_features= 16)\n",
    "        self.linear2 = nn.Linear(in_features = 16, out_features= 2)\n",
    " \n",
    "        self.feature_linear1 = nn.Linear(in_features = 168, out_features= 128)\n",
    "        self.feature_linear2 = nn.Linear(in_features = 128, out_features= 64)\n",
    "\n",
    "        \n",
    "        self.dropout1 = nn.Dropout(0.5)\n",
    "        self.dropout2 = nn.Dropout(0.6)\n",
    "        self.dropout3 = nn.Dropout(0.7)\n",
    "        \n",
    "#         layers = [nn.BatchNorm1d(n_in)]\n",
    "#         layers.append(nn.Dropout(p))\n",
    "#         layers.append(nn.Linear(n_in, n_out))\n",
    "#         layers.append(actn)\n",
    "#         layers\n",
    "\n",
    "        \n",
    "    def forward(self,seq, features):\n",
    "        x = seq.permute(0,2,1)\n",
    "        features = features.permute(0,2,1)\n",
    "#         print(x.type())\n",
    "        \n",
    "        out1 = self.conv11(x)\n",
    "        out1 = F.relu(out1)\n",
    "        out2 = self.conv12(x)\n",
    "        out2 = F.relu(out2)\n",
    "        out3 = self.conv13(x)\n",
    "        out3 = F.relu(out3)\n",
    "        out = torch.cat((out1,out2,out3),1)\n",
    "        #out = F.relu(out)\n",
    "        out = F.dropout(out, p = 0.4)\n",
    "        out = self.maxpool1d(out)\n",
    "        \n",
    "        out1 = self.conv21(out)\n",
    "        out1 = F.relu(out1)\n",
    "        out2 = self.conv22(out)\n",
    "        out2 = F.relu(out2)\n",
    "        out3 = self.conv23(out)\n",
    "        out3 = F.relu(out3)\n",
    "        out = torch.cat((out1,out2,out3),1)\n",
    "        #out = F.relu(out)\n",
    "        out = F.dropout(out, p = 0.4)\n",
    "        out = self.maxpool1d(out)\n",
    "        out = self.conv3(out)\n",
    "        out = F.dropout(out, p = 0.4)\n",
    "        out = self.maxpool1d(out)\n",
    "        #out = out.permute(0,2,1)\n",
    "        \n",
    "        #out, (hn, cn)  = self.lstm(out)\n",
    "        \n",
    "        #l2allstates = torch.sqrt(torch.sum(out**2, dim = 1))\n",
    "        #last_state_cmbnd = torch.cat([hn, cn], dim = -1)\n",
    "        \n",
    "        #l2allstates = self.batchnrm(l2allstates)\n",
    "        \n",
    "        #print(features.shape)\n",
    "        feature_out = self.feature_linear1(features.view(-1,168))\n",
    "        feature_out = self.feature_linear2(feature_out)\n",
    "        \n",
    "        \n",
    "        combined_out = torch.cat((out.view(-1, 32*29),feature_out),1)\n",
    "        #combined_out = torch.cat(out.view(-1, 32*29),1)\n",
    "        out = self.linear1(combined_out)\n",
    "        \n",
    "        #out = self.linear1(out.view(-1, 32*29))\n",
    "        out = F.dropout(out, p = 0.5)\n",
    "        out = self.linear2(out)\n",
    "\n",
    "        return out    \n",
    "    \n",
    "    \n",
    "class VGGNet(nn.Module):\n",
    "    # https://peltarion.com/static/vgg_pa03.jpg\n",
    "    \n",
    "    def __init__(self, num_features, hidden_sz):\n",
    "        super(VGGNet, self).__init__()\n",
    "        #super().__init__()\n",
    "        \n",
    "        self.hidden_sz = hidden_sz\n",
    "        self.conv1 = nn.Conv1d(in_channels = num_features, out_channels = 64, kernel_size= 3)        \n",
    "        self.conv2 = nn.Conv1d(in_channels = self.conv1.out_channels, out_channels = 64, kernel_size= 3)\n",
    "        self.maxpool1d_2 = torch.nn.MaxPool1d(kernel_size=2)\n",
    "        \n",
    "        self.conv3 = nn.Conv1d(in_channels = self.conv2.out_channels,out_channels = 128, kernel_size= 3)        \n",
    "        self.conv4 = nn.Conv1d(in_channels = self.conv3.out_channels,out_channels = 128, kernel_size= 3)\n",
    "        self.maxpool1d_2 = torch.nn.MaxPool1d(kernel_size=2)\n",
    "\n",
    "        self.conv5 = nn.Conv1d(in_channels = self.conv4.out_channels,out_channels = 256, kernel_size= 3)        \n",
    "        self.conv6 = nn.Conv1d(in_channels = self.conv5.out_channels,out_channels = 256, kernel_size= 3)\n",
    "        self.conv7 = nn.Conv1d(in_channels = self.conv6.out_channels,out_channels = 256, kernel_size= 3)\n",
    "        self.maxpool1d_4 = torch.nn.MaxPool1d(kernel_size=4)\n",
    "\n",
    "        self.conv8 = nn.Conv1d(in_channels = self.conv7.out_channels,out_channels = 256, kernel_size= 3)        \n",
    "        self.conv9 = nn.Conv1d(in_channels = self.conv8.out_channels,out_channels = 256, kernel_size= 3)\n",
    "        self.conv10 = nn.Conv1d(in_channels = self.conv9.out_channels,out_channels = 256, kernel_size= 3)\n",
    "        self.maxpool1d_4 = torch.nn.MaxPool1d(kernel_size=4)\n",
    "        \n",
    "        \n",
    "        #self.lstm = nn.LSTM(input_size = 64, hidden_size = hidden_sz, batch_first = True, bidirectional = True)\n",
    "        self.linear1 = nn.Linear(in_features = 256*7, out_features= 64)\n",
    "        self.linear2 = nn.Linear(in_features = 64, out_features= 16)\n",
    "        self.linear3 = nn.Linear(in_features = 16, out_features= 2)        \n",
    " \n",
    "        self.dropout = nn.Dropout(p =0.5)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        x = x.permute(0,2,1)\n",
    "        \n",
    "        #print(x.shape)\n",
    "        out = self.conv1(x)\n",
    "        out = F.relu(out)\n",
    "        out = self.conv2(out)\n",
    "        out = F.relu(out)\n",
    "        out = self.maxpool1d_2(out)\n",
    "\n",
    "        out = self.conv3(out)\n",
    "        out = F.relu(out)\n",
    "        out = self.conv4(out)\n",
    "        out = F.relu(out)\n",
    "        out = self.maxpool1d_2(out)\n",
    "        \n",
    "        out = self.conv5(out)\n",
    "        out = F.relu(out)\n",
    "        out = self.conv6(out)\n",
    "        out = F.relu(out)\n",
    "        out = self.conv7(out)\n",
    "        out = F.relu(out)\n",
    "        out = self.maxpool1d_4(out)\n",
    "\n",
    "        out = self.conv8(out)\n",
    "        out = F.relu(out)\n",
    "        out = self.conv9(out)\n",
    "        out = F.relu(out)\n",
    "        out = self.conv10(out)\n",
    "        out = F.relu(out)\n",
    "        out = self.maxpool1d_4(out)\n",
    "        \n",
    "        \n",
    "        out = self.linear1(out.view(-1, 256*7))\n",
    "        out = F.relu(out)\n",
    "        out = self.dropout(out)\n",
    "        out = self.linear2(out)\n",
    "        out = F.relu(out)\n",
    "        out = self.dropout(out)\n",
    "        out = self.linear3(out)\n",
    "\n",
    "        #out = out.permute(0,2,1)\n",
    "        \n",
    "        #out, (hn, cn)  = self.lstm(out)\n",
    "        \n",
    "        #l2allstates = torch.sqrt(torch.sum(out**2, dim = 1))\n",
    "        #last_state_cmbnd = torch.cat([hn, cn], dim = -1)\n",
    "        \n",
    "        #l2allstates = self.batchnrm(l2allstates)\n",
    "        #print(out.shape)\n",
    "        #print(out.shape)\n",
    "\n",
    "        return out\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "class FantomNet_Bio_kmer_MLP(nn.Module):\n",
    "    # https://peltarion.com/static/vgg_pa03.jpg\n",
    "    \n",
    "    def __init__(self, num_features, hidden_sz):\n",
    "        super(FantomNet_Bio_kmer_MLP, self).__init__()\n",
    "        #super().__init__()\n",
    "        \n",
    "        num_features_seq, num_features_bio = num_features\n",
    "        \n",
    "        self.hidden_sz = hidden_sz\n",
    "        self.conv1 = nn.Conv1d(in_channels = num_features_seq, out_channels = 32, kernel_size= 7)        \n",
    "        self.conv2 = nn.Conv1d(in_channels = self.conv1.out_channels, out_channels = 64, kernel_size= 5)\n",
    "        self.maxpool1d_2 = torch.nn.MaxPool1d(kernel_size=4)\n",
    "        \n",
    "        self.conv3 = nn.Conv1d(in_channels = self.conv2.out_channels,out_channels = 128, kernel_size= 3)        \n",
    "        self.maxpool1d_4 = torch.nn.MaxPool1d(kernel_size=4)\n",
    "        \n",
    "        self.conv4 = nn.Conv1d(in_channels = self.conv3.out_channels,out_channels = 256, kernel_size= 3)        \n",
    "\n",
    "        \n",
    "        #self.lstm = nn.LSTM(input_size = 64, hidden_size = hidden_sz, batch_first = True, bidirectional = True)\n",
    "        self.linear1 = nn.Linear(in_features = 256*8, out_features= 64)\n",
    "        self.linear2 = nn.Linear(in_features = 64, out_features= 16)\n",
    "        self.linear3 = nn.Linear(in_features = 16, out_features= 2)        \n",
    " \n",
    "        self.dropout = nn.Dropout(p =0.5)\n",
    "    \n",
    "        # Bio_MLP\n",
    "        self.bio_linear1 = nn.Linear(in_features = num_features_bio, out_features= 64)\n",
    "        self.bio_linear2 = nn.Linear(in_features = 64, out_features= 16)\n",
    "        self.bio_linear3 = nn.Linear(in_features = 16, out_features= 2)        \n",
    " \n",
    "        self.bio_dropout1 = nn.Dropout(p =0.7)\n",
    "        self.bio_dropout2 = nn.Dropout(p =0.8)\n",
    "    \n",
    "        self.combined_linear1 = nn.Linear(self.bio_linear2.out_features + self.bio_linear2.out_features, 16)\n",
    "        self.combined_linear2 = nn.Linear(16, 2)\n",
    "        \n",
    "    \n",
    "    def forward(self, x, z):\n",
    "        x = x.permute(0,2,1)\n",
    "        z = z.permute(0,2,1)\n",
    "\n",
    "        out_seq = self.conv1(x)\n",
    "        out_seq = F.relu(out_seq)\n",
    "        out_seq = self.conv2(out_seq)\n",
    "        out_seq = F.relu(out_seq)\n",
    "        out_seq = self.maxpool1d_2(out_seq)\n",
    "\n",
    "        out_seq = self.conv3(out_seq)\n",
    "        out_seq = self.maxpool1d_4(out_seq)        \n",
    "        \n",
    "        out_seq = self.conv4(out_seq)\n",
    "        out_seq = self.maxpool1d_2(out_seq)        \n",
    "\n",
    "        out_seq = self.linear1(out_seq.view(-1, 256*8))\n",
    "        out_seq = F.relu(out_seq)\n",
    "        out_seq = self.dropout(out_seq)\n",
    "        out_seq = self.linear2(out_seq)\n",
    "        out_seq = F.relu(out_seq)\n",
    "        out_seq = self.dropout(out_seq)\n",
    "        #out = self.linear3(out)\n",
    "\n",
    "        out_bio = self.bio_linear1(z.contiguous().view(z.shape[0], -1))\n",
    "        out_bio = F.relu(out_bio)\n",
    "        out_bio = self.bio_dropout1(out_bio)\n",
    "        out_bio = self.bio_linear2(out_bio)\n",
    "        out_bio = F.relu(out_bio)\n",
    "        out_bio = self.bio_dropout2(out_bio)\n",
    "        #out_bio = self.linear3(out_bio)\n",
    "        #print(out_bio.shape)\n",
    "        #print(out_seq.shape)\n",
    "        \n",
    "        \n",
    "        out = torch.cat([out_seq, out_bio], dim = 1)\n",
    "        out = self.combined_linear1(out)\n",
    "        out = F.relu(out)\n",
    "        out = self.dropout(out)    \n",
    "        out = self.combined_linear2(out)\n",
    "        \n",
    "        #print(out.shape)\n",
    "        \n",
    "        \n",
    "        return out\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "class Bio_kmer_MLP(nn.Module):\n",
    "    # https://peltarion.com/static/vgg_pa03.jpg\n",
    "    \n",
    "    def __init__(self, num_features, hidden_sz):\n",
    "        super(Bio_kmer_MLP, self).__init__()\n",
    "        #super().__init__()\n",
    "        \n",
    "        self.linear1 = nn.Linear(in_features = 544, out_features= 64)\n",
    "        self.linear2 = nn.Linear(in_features = 64, out_features= 16)\n",
    "        self.linear3 = nn.Linear(in_features = 16, out_features= 2)        \n",
    " \n",
    "        self.dropout1 = nn.Dropout(p =0.7)\n",
    "        self.dropout2 = nn.Dropout(p =0.8)\n",
    "    \n",
    "    def forward(self,x, z):\n",
    "        \n",
    "        z = z.permute(0,2,1)\n",
    "        \n",
    "        out = self.linear1(z.contiguous().view(z.shape[0], -1))\n",
    "        out = F.relu(out)\n",
    "        out = self.dropout1(out)\n",
    "        out = self.linear2(out)\n",
    "        out = F.relu(out)\n",
    "        out = self.dropout2(out)\n",
    "        out = self.linear3(out)\n",
    "        \n",
    "        #out = out.permute(0,2,1)\n",
    "        \n",
    "        #out, (hn, cn)  = self.lstm(out)\n",
    "        \n",
    "        #l2allstates = torch.sqrt(torch.sum(out**2, dim = 1))\n",
    "        #last_state_cmbnd = torch.cat([hn, cn], dim = -1)\n",
    "        \n",
    "        #l2allstates = self.batchnrm(l2allstates)\n",
    "        #print(out.shape)\n",
    "        #print(out.shape)\n",
    "\n",
    "        return out\n",
    "    \n",
    "\n",
    "class FantomNet(nn.Module):\n",
    "    # https://peltarion.com/static/vgg_pa03.jpg\n",
    "    \n",
    "    def __init__(self, num_features, hidden_sz):\n",
    "        super(FantomNet, self).__init__()\n",
    "        #super().__init__()\n",
    "        \n",
    "        self.hidden_sz = hidden_sz\n",
    "        self.conv1 = nn.Conv1d(in_channels = num_features, out_channels = 32, kernel_size= 7)        \n",
    "        self.conv2 = nn.Conv1d(in_channels = self.conv1.out_channels, out_channels = 64, kernel_size= 5)\n",
    "        self.maxpool1d_2 = torch.nn.MaxPool1d(kernel_size=4)\n",
    "        \n",
    "        self.conv3 = nn.Conv1d(in_channels = self.conv2.out_channels,out_channels = 128, kernel_size= 3)        \n",
    "        self.maxpool1d_4 = torch.nn.MaxPool1d(kernel_size=4)\n",
    "        \n",
    "        self.conv4 = nn.Conv1d(in_channels = self.conv3.out_channels,out_channels = 256, kernel_size= 3)        \n",
    "\n",
    "        #self.lstm = nn.LSTM(input_size = 64, hidden_size = hidden_sz, batch_first = True, bidirectional = True)\n",
    "        self.linear1 = nn.Linear(in_features = 256*8, out_features= 64)\n",
    "        self.linear2 = nn.Linear(in_features = 64, out_features= 16)\n",
    "        self.linear3 = nn.Linear(in_features = 16, out_features= 2)        \n",
    " \n",
    "        self.dropout1 = nn.Dropout(p =0.3)\n",
    "        self.dropout2 = nn.Dropout(p =0.2)\n",
    "\n",
    "        \n",
    "    def forward(self, x, features):\n",
    "        x = x.permute(0,2,1)\n",
    "        \n",
    "        #print(x.shape)\n",
    "        out = self.conv1(x)\n",
    "        out = F.relu(out)\n",
    "        out = self.conv2(out)\n",
    "        out = F.relu(out)\n",
    "        out = self.maxpool1d_2(out)\n",
    "\n",
    "        out = self.conv3(out)\n",
    "        out = F.relu(out)\n",
    "        out = self.maxpool1d_4(out)        \n",
    "        \n",
    "        out = self.conv4(out)\n",
    "        out = F.relu(out)\n",
    "        out = self.maxpool1d_2(out) \n",
    "        \n",
    "        out = self.linear1(out.view(-1, 256*8))\n",
    "        out = F.relu(out)\n",
    "        out = self.dropout1(out)\n",
    "        out = self.linear2(out)\n",
    "        out = F.relu(out)\n",
    "        out = self.dropout2(out)\n",
    "        out = self.linear3(out)\n",
    "\n",
    "        #out = out.permute(0,2,1)\n",
    "        \n",
    "        #out, (hn, cn)  = self.lstm(out)\n",
    "        \n",
    "        #l2allstates = torch.sqrt(torch.sum(out**2, dim = 1))\n",
    "        #last_state_cmbnd = torch.cat([hn, cn], dim = -1)\n",
    "        \n",
    "        #l2allstates = self.batchnrm(l2allstates)\n",
    "        #print(out.shape)\n",
    "        #print(out.shape)\n",
    "\n",
    "        return out\n",
    "        \n",
    "    def getname(self):\n",
    "        return \"FantomNet\"    \n",
    "    \n",
    "class BioMLP(nn.Module):\n",
    "    # https://peltarion.com/static/vgg_pa03.jpg\n",
    "    \n",
    "    def __init__(self, num_features, hidden_sz):\n",
    "        super(BioMLP, self).__init__()\n",
    "        #super().__init__()\n",
    "        \n",
    "#         self.linear1 = nn.Linear(in_features = 292, out_features= 64)\n",
    "#         self.linear2 = nn.Linear(in_features = 64, out_features= 16)\n",
    "#         self.linear3 = nn.Linear(in_features = 16, out_features= 2)        \n",
    " \n",
    "        self.linear1 = nn.Linear(in_features = 292, out_features= 128)\n",
    "        self.linear2 = nn.Linear(in_features = 128, out_features= 64)\n",
    "        self.linear3 = nn.Linear(in_features = 64, out_features= 16)        \n",
    "        self.linear4 = nn.Linear(in_features = 16, out_features= 2)        \n",
    "     \n",
    "    \n",
    "#         self.dropout1 = nn.Dropout(p =0.7)\n",
    "#         self.dropout2 = nn.Dropout(p =0.8)\n",
    "\n",
    "        self.dropout1 = nn.Dropout(p =0.4)\n",
    "        self.dropout2 = nn.Dropout(p =0.5)\n",
    "        self.dropout3 = nn.Dropout(p =0.5)\n",
    "        \n",
    "    def forward(self,x, features):\n",
    "        \n",
    "        z = features[0]\n",
    "        z = z.permute(0,2,1)\n",
    "        \n",
    "#         out = self.linear1(z.contiguous().view(z.shape[0], -1))\n",
    "#         out = F.relu(out)\n",
    "#         out = self.dropout1(out)\n",
    "#         out = self.linear2(out)\n",
    "#         out = F.relu(out)\n",
    "#         out = self.dropout2(out)\n",
    "#         out = self.linear3(out)\n",
    "\n",
    "        out = self.linear1(z.contiguous().view(z.shape[0], -1))\n",
    "        out = F.relu(out)\n",
    "        out = self.dropout1(out)\n",
    "        out = self.linear2(out)\n",
    "        out = F.relu(out)\n",
    "        out = self.dropout2(out)\n",
    "        out = self.linear3(out)\n",
    "        out = F.relu(out)\n",
    "        out = self.dropout3(out)\n",
    "        out = self.linear4(out)\n",
    "\n",
    "\n",
    "        \n",
    "        #out = out.permute(0,2,1)\n",
    "        \n",
    "        #out, (hn, cn)  = self.lstm(out)\n",
    "        \n",
    "        #l2allstates = torch.sqrt(torch.sum(out**2, dim = 1))\n",
    "        #last_state_cmbnd = torch.cat([hn, cn], dim = -1)\n",
    "        \n",
    "        #l2allstates = self.batchnrm(l2allstates)\n",
    "        #print(out.shape)\n",
    "        #print(out.shape)\n",
    "\n",
    "        return out    \n",
    "    \n",
    "    def getname(self):\n",
    "        return \"BioMLP\"\n",
    "    \n",
    "class FantomNet_Bio_MLP(nn.Module):\n",
    "    # https://peltarion.com/static/vgg_pa03.jpg\n",
    "    \n",
    "    def __init__(self, num_features, hidden_sz):\n",
    "        super(FantomNet_Bio_MLP, self).__init__()\n",
    "        #super().__init__()\n",
    "        \n",
    "        num_features_seq, num_features_bio = num_features\n",
    "        \n",
    "        self.hidden_sz = hidden_sz\n",
    "        self.conv1 = nn.Conv1d(in_channels = num_features_seq, out_channels = 32, kernel_size= 7)        \n",
    "        self.conv2 = nn.Conv1d(in_channels = self.conv1.out_channels, out_channels = 64, kernel_size= 5)\n",
    "        self.maxpool1d_2 = torch.nn.MaxPool1d(kernel_size=4)\n",
    "        \n",
    "        self.conv3 = nn.Conv1d(in_channels = self.conv2.out_channels,out_channels = 128, kernel_size= 3)        \n",
    "        self.maxpool1d_4 = torch.nn.MaxPool1d(kernel_size=4)\n",
    "        \n",
    "        self.conv4 = nn.Conv1d(in_channels = self.conv3.out_channels,out_channels = 256, kernel_size= 3)        \n",
    "\n",
    "        #self.lstm = nn.LSTM(input_size = 64, hidden_size = hidden_sz, batch_first = True, bidirectional = True)\n",
    "        self.linear1 = nn.Linear(in_features = 256*8, out_features= 64)\n",
    "        self.linear2 = nn.Linear(in_features = 64, out_features= 16)\n",
    "        self.linear3 = nn.Linear(in_features = 16, out_features= 2)        \n",
    " \n",
    "        self.dropout1 = nn.Dropout(p =0.3)\n",
    "        self.dropout2 = nn.Dropout(p =0.2)\n",
    "        self.dropout3 = nn.Dropout(p =0.6)\n",
    "        \n",
    "        # Bio_MLP\n",
    "#         self.bio_linear1 = nn.Linear(in_features = 292, out_features= 128)\n",
    "#         self.bio_batchnorm_lin1 = nn.BatchNorm1d(self.bio_linear1.out_features)\n",
    "#         self.bio_linear2 = nn.Linear(in_features = 128, out_features= 64)\n",
    "#         self.bio_batchnorm_lin2 = nn.BatchNorm1d(self.bio_linear2.out_features)\n",
    "#         self.bio_linear3 = nn.Linear(in_features = 64, out_features= 16)        \n",
    "#         self.bio_batchnorm_lin3 = nn.BatchNorm1d(self.bio_linear3.out_features)\n",
    "#         self.bio_linear4 = nn.Linear(in_features = 16, out_features= 2)        \n",
    "     \n",
    "    \n",
    "#         self.dropout1 = nn.Dropout(p =0.7)\n",
    "#         self.dropout2 = nn.Dropout(p =0.8)\n",
    "\n",
    "        self.bio_linear1 = nn.Linear(in_features = num_features_bio, out_features= 12)\n",
    "\n",
    "        #self.bio_dropout1 = nn.Dropout(p =0.4)\n",
    "        self.bio_dropout2 = nn.Dropout(p =0.7)\n",
    "        #self.bio_dropout3 = nn.Dropout(p =0.5)        \n",
    "        #self.bio_dropout4 = nn.Dropout(p =0.75)\n",
    "        #self.bio_linear1 = nn.Linear(in_features = num_features_bio, out_features= 64)\n",
    "        #self.bio_batchnorm_lin1 = nn.BatchNorm1d(self.bio_linear1.out_features)\n",
    "        #self.bio_linear1 = nn.Linear(in_features = num_features_bio, out_features= 64)\n",
    "        #self.bio_batchnorm_lin1 = nn.BatchNorm1d(self.bio_linear1.out_features)\n",
    "        #self.bio_linear2 = nn.Linear(in_features = 64, out_features= 16)\n",
    "        #self.bio_batchnorm_lin2 = nn.BatchNorm1d(self.bio_linear1.out_features)\n",
    "        \n",
    "        #self.bio_dropout1 = nn.Dropout(p =0.6)\n",
    "        #self.bio_dropout2 = nn.Dropout(p =0.8)\n",
    "        #self.bio_dropout1 = nn.Dropout(p =0.8)\n",
    "        #self.bio_dropout2 = nn.Dropout(p =0.8)\n",
    "        \n",
    "        #self.combined_linear1 = nn.Linear(self.bio_linear2.out_features + self.bio_linear2.out_features, 16)\n",
    "        #self.combined_linear1 = nn.Linear(self.linear2.out_features + self.bio_linear1.out_features, 16)\n",
    "        \n",
    "        #self.combined_linear1 = nn.Linear(self.bio_linear2.out_features, 16)\n",
    "        #self.combined_linear2 = nn.Linear(self.combined_linear1.out_features, 2)\n",
    "\n",
    "        \n",
    "        #self.combined_linear_L = nn.Linear(self.linear2.out_features + num_features_bio, 2)\n",
    "        self.combined_linear_L = nn.Linear(self.linear2.out_features + self.bio_linear1.out_features, 2)\n",
    "        \n",
    "        #self.combined_dropout = nn.Dropout(p =0.8)\n",
    "    \n",
    "    def forward(self, x, features):\n",
    "        z = features[0]\n",
    "        x = x.permute(0,2,1)\n",
    "        z = z.permute(0,2,1)\n",
    "\n",
    "        out_seq = self.conv1(x)\n",
    "        out_seq = F.relu(out_seq)\n",
    "        out_seq = self.conv2(out_seq)\n",
    "        out_seq = F.relu(out_seq)\n",
    "        out_seq = self.maxpool1d_2(out_seq)\n",
    "\n",
    "        out_seq = self.conv3(out_seq)\n",
    "        out_seq = self.maxpool1d_4(out_seq)        \n",
    "        \n",
    "        out_seq = self.conv4(out_seq)\n",
    "        out_seq = self.maxpool1d_2(out_seq) \n",
    "        \n",
    "        out_seq = self.linear1(out_seq.view(-1, 256*8))\n",
    "        out_seq = F.relu(out_seq)\n",
    "        out_seq = self.dropout1(out_seq)\n",
    "        out_seq = self.linear2(out_seq)\n",
    "        #out_seq = F.relu(out_seq)\n",
    "        #out_seq = self.dropout2(out_seq)\n",
    "        #out_seq = self.linear3(out_seq)\n",
    "        \n",
    "        \n",
    "        #out_bio = self.bio_linear1(z.contiguous().view(z.shape[0], -1))\n",
    "        #out_bio = F.relu(out_bio)\n",
    "        #out_bio = self.bio_dropout1(out_bio)\n",
    "        #out_bio = self.bio_batchnorm_lin1(out_bio)\n",
    "        \n",
    "        #out_bio = self.bio_linear2(out_bio)\n",
    "        #out_bio = F.relu(out_bio)\n",
    "        #out_bio = self.bio_dropout2(out_bio)\n",
    "        #out_bio = self.bio_batchnorm_lin2(out_bio)\n",
    "        \n",
    "        #out_bio = self.bio_linear3(out_bio)\n",
    "        #print(out_bio.shape)\n",
    "        #print(out_seq.shape)\n",
    "        \n",
    "        #out = self.combined_linear1(out)\n",
    "        #print(out.shape)\n",
    "        \n",
    "        #out = self.combined_linear1(out)\n",
    "        #out = F.relu(out)\n",
    "        #out = self.combined_dropout(out)    \n",
    "        \n",
    "        \n",
    "#         out_bio = self.bio_linear1(z.contiguous().view(z.shape[0], -1))\n",
    "#         out_bio = self.bio_batchnorm_lin1(out_bio)\n",
    "#         out_bio = F.relu(out_bio)\n",
    "#         out_bio = self.bio_dropout1(out_bio)\n",
    "#         out_bio = self.bio_linear2(out_bio)\n",
    "#         out_bio = self.bio_batchnorm_lin2(out_bio)\n",
    "#         out_bio = F.relu(out_bio)\n",
    "#         out_bio = self.bio_dropout2(out_bio)\n",
    "#         out_bio = self.bio_linear3(out_bio)\n",
    "#         out_bio = self.bio_batchnorm_lin3(out_bio)\n",
    "#         out_bio = F.relu(out_bio)\n",
    "#         out_bio = self.bio_dropout3(out_bio)\n",
    "#         out_bio = self.bio_linear4(out_bio)\n",
    "        \n",
    "        \n",
    "#         out_seq = self.dropout3(out_seq)\n",
    "#         out_bio = self.bio_dropout4(out_bio)\n",
    "        \n",
    "        out_bio = self.bio_linear1(z.contiguous().view(z.shape[0], -1))\n",
    "        out_bio = self.bio_dropout2(out_bio)\n",
    "        \n",
    "        out = torch.cat([out_seq, out_bio], dim = 1)                \n",
    "        #out = torch.cat([out_seq, z.contiguous().view(z.shape[0], -1)], dim = 1)                \n",
    "        out = self.combined_linear_L(out)\n",
    "        \n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def getname(self):\n",
    "        return \"FantomNet_BioMLP\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# from matplotlib import pyplot as plt\n",
    "\n",
    "# print(\"torch version: \", torch.__version__)\n",
    "\n",
    "        \n",
    "# model = torch.nn.Linear(1, 1)\n",
    "\n",
    "# max_lr = 1e-3\n",
    "# init_lr = max_lr/25.0\n",
    "# last_lr = max_lr/25e4\n",
    "# print(\"init_lr\",init_lr)\n",
    "# print(\"max_lr\",max_lr)\n",
    "# print(\"last_lr\",last_lr)\n",
    "\n",
    "# num_epochs = 20\n",
    "\n",
    "# optim = torch.optim.SGD(model.parameters(), lr = init_lr)\n",
    "\n",
    "# for param_group in optim.param_groups:\n",
    "#         param_group['lr'] = init_lr\n",
    "\n",
    "        \n",
    "\n",
    "# l1 = lr_scheduler.CosineAnnealingLR(optim, T_max = num_epochs//3, eta_min = max_lr)\n",
    "# lr2 = lr_scheduler.CosineAnnealingLR(optim, T_max = 2*num_epochs//3, eta_min = last_lr)\n",
    "# #lr2 = lr_scheduler.StepLR(optim, gamma = 0.9, step_size = 2)\n",
    "# lrs = []\n",
    "\n",
    "\n",
    "# lrs.append(optim.param_groups[0]['lr'])    \n",
    "\n",
    "# for _ in range(num_epochs):\n",
    "#     #print(\"-\", l.last_epoch, optim.param_groups[0]['lr'])\n",
    "#     if(l1.last_epoch<num_epochs//3):\n",
    "#         l1.step()        \n",
    "#     else:\n",
    "#         lr2.step()\n",
    "#     lrs.append(optim.param_groups[0]['lr'])\n",
    "# plt.plot(lrs)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'legend.fontsize': 6,\n",
    "          'legend.handlelength': 2}\n",
    "plt.rcParams.update(params)\n",
    "\n",
    "\n",
    "# different experiment configurations\n",
    "\n",
    "configs = {\n",
    "    \n",
    "           'features': {\n",
    "                         'network':BioMLP,\n",
    "                         'use_comp_features': False,\n",
    "                         'use_bio_features': True,               \n",
    "                         'num_features': 292, \n",
    "                         'num_hidden': None, \n",
    "                         'max_lr': 2e-3, \n",
    "                         'wd': .06, \n",
    "                         'max_lr_decay': 0.8, \n",
    "                         'cycles_per_round': 1, \n",
    "                         'cycle_freq': 1,\n",
    "                         'bmarks': { \n",
    "                                     'benchmark_acc': 1.0, \n",
    "                                     'benchmark_f1': 1.0, \n",
    "                                     'benchmark_spec': 0.7601, \n",
    "                                     'benchmark_sen': 0.8974, \n",
    "                                     'benchmark_auc': 1.0, \n",
    "                                     'benchmark_mcc': 1.0\n",
    "                                   }               \n",
    "                       },    \n",
    "    \n",
    "            'seq': {\n",
    "                   'network':FantomNet, \n",
    "                   'use_comp_features': False,\n",
    "                   'use_bio_features': False,                 \n",
    "                   'num_features': len(unique_DNAs), \n",
    "                   'num_hidden': 16, \n",
    "                   'max_lr': 2e-3, \n",
    "                   'wd': .002,\n",
    "                   'max_lr_decay': 0.9, \n",
    "                   'cycles_per_round': 1, \n",
    "                   'cycle_freq': 1, \n",
    "                   'bmarks': { \n",
    "                               'benchmark_acc': 1.0, \n",
    "                               'benchmark_f1': 1.0, \n",
    "                               'benchmark_spec': 0.7601, \n",
    "                               'benchmark_sen': 0.8974, \n",
    "                               'benchmark_auc': 1.0, \n",
    "                               'benchmark_mcc': 1.0\n",
    "                             }\n",
    "                  \n",
    "                  },\n",
    "\n",
    " \n",
    "            'seq_LSTM_CNN_Net': {\n",
    "                   'network':LSTM_CNN_Net, \n",
    "                   'use_comp_features': False,\n",
    "                   'use_bio_features': False,                 \n",
    "                   'num_features': len(unique_DNAs), \n",
    "                   'num_hidden': (16, 600), \n",
    "                   'max_lr': 2e-3, \n",
    "                   'wd': .003,\n",
    "                   'max_lr_decay': 0.9, \n",
    "                   'cycles_per_round': 1, \n",
    "                   'cycle_freq': 1, \n",
    "                   'bmarks': { \n",
    "                               'benchmark_acc': 1.0, \n",
    "                               'benchmark_f1': 1.0, \n",
    "                               'benchmark_spec': 0.7601, \n",
    "                               'benchmark_sen': 0.8974, \n",
    "                               'benchmark_auc': 1.0, \n",
    "                               'benchmark_mcc': 1.0\n",
    "                             }\n",
    "                  \n",
    "                  },\n",
    "    \n",
    "        \n",
    "    \n",
    "            'seq-CNPP': {\n",
    "               'network':CNPPNet, \n",
    "               'use_comp_features': False,\n",
    "               'use_bio_features': False,                 \n",
    "               'num_features': len(unique_DNAs), \n",
    "               'num_hidden': 16, \n",
    "               'max_lr': 1e-3, \n",
    "               'wd': .002,\n",
    "               'max_lr_decay': 0.95, \n",
    "               'cycles_per_round': 2, \n",
    "               'cycle_freq': 1, \n",
    "               'bmarks': { \n",
    "                           'benchmark_acc': 1.0, \n",
    "                           'benchmark_f1': 1.0, \n",
    "                           'benchmark_spec': 0.7601, \n",
    "                           'benchmark_sen': 0.8974, \n",
    "                           'benchmark_auc': 1.0, \n",
    "                           'benchmark_mcc': 1.0\n",
    "                         }\n",
    "\n",
    "              },\n",
    "\n",
    "#            #'features': {'network':Bio_kmer_MLP, 'num_features': 544, 'num_hidden': None, 'max_lr': 5e-3, 'wd': .065, 'max_lr_decay': 0.8, 'cycles_per_round': 1, 'cycle_freq': 7},\n",
    "\n",
    "           'seq+features': {'network':FantomNet_Bio_MLP, \n",
    "                            'use_comp_features': False,\n",
    "                            'use_bio_features': True,     \n",
    "                            'num_features': (len(unique_DNAs), 292), \n",
    "                            'num_hidden': 16, \n",
    "                            'max_lr': 1e-3, \n",
    "                            'wd': .0045, \n",
    "                            'max_lr_decay': 0.8,\n",
    "                            'cycles_per_round': 1,\n",
    "                            'cycle_freq': 1,\n",
    "                            'bmarks': {'benchmark_acc': 1.0, \n",
    "                                       'benchmark_f1': 1.0, \n",
    "                                       'benchmark_spec': 0.8819, \n",
    "                                       'benchmark_sen': 0.8895, \n",
    "                                       'benchmark_auc': 1.0, \n",
    "                                       'benchmark_mcc':.7447\n",
    "                                      }\n",
    "                           },\n",
    "           #'seq+features': {'network':FantomNet_Bio_kmer_MLP, 'num_features': (len(unique_DNAs), 544), 'num_hidden': 16, 'max_lr': 5e-3, 'wd': .065, 'max_lr_decay': 0.8},\n",
    "          }\n",
    "\n",
    "\n",
    "\n",
    "xval_fold_count =  10 \n",
    "num_epochs =  400\n",
    "batch_sz = 2048\n",
    "\n",
    "init_learning_rate = 0.01    \n",
    "\n",
    "###### Setup K-fold X-validation\n",
    "skf = StratifiedKFold( n_splits= xval_fold_count , random_state = 23, shuffle=True)\n",
    "\n",
    "\n",
    "## packing variable-length data in pytorch using packing and padding:\n",
    "## https://github.com/HarshTrivedi/packing-unpacking-pytorch-minimal-tutorial\n",
    "\n",
    "## https://gist.github.com/MikulasZelinka/9fce4ed47ae74fca454e88a39f8d911a\n",
    "\n",
    "num_buckets = len(X_buckets)\n",
    "train_val_idxs_each_bucket = []\n",
    "\n",
    "# generating train and validation indices for all k folds, for each bucket, separately\n",
    "for bucket_idx in range(num_buckets):\n",
    "    bucket_input_features = X_buckets[bucket_idx] # tuple containing the sequence (and optionally computed features and bio features)\n",
    "    \n",
    "    if(use_features):\n",
    "        bucket_input_features = bucket_input_features[0]\n",
    "        \n",
    "    bucket_labels = Y_buckets[bucket_idx]\n",
    "    \n",
    "    train_val_idxs = list(skf.split(np.zeros(len(bucket_labels)), bucket_labels))\n",
    "    train_val_idxs_each_bucket.append(train_val_idxs)\n",
    "    \n",
    "config_idx = 4\n",
    "config_name = list(configs.keys())[config_idx]\n",
    "\n",
    "use_comp_features = configs[config_name]['use_comp_features']\n",
    "use_bio_features = configs[config_name]['use_bio_features']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if do_cross_validation:\n",
    "\n",
    "\n",
    "    #metrics_ht = Metrics(benchmark_acc = 0.8169, benchmark_f1 = 1.0, benchmark_spec = 0.8060, benchmark_sen = 0.8277, benchmark_auc = 1.0, benchmark_mcc = .6476)\n",
    "    metrics_ht = Metrics(**configs[config_name]['bmarks'])\n",
    "\n",
    "\n",
    "    all_folds_last_results = []\n",
    "    all_folds_best_results_for_fold = []\n",
    "\n",
    "    print(\"############################################################################################################################\")\n",
    "    print(\"############################################################################################################################\")\n",
    "    print(f\"##################################             {config_name}                ###############################################\")\n",
    "    print(\"############################################################################################################################\")\n",
    "    print(\"############################################################################################################################\")\n",
    "\n",
    "\n",
    "    num_features_means = []\n",
    "    num_features_sds = []\n",
    "\n",
    "    bio_features_means = []\n",
    "    bio_features_sds = []    \n",
    "\n",
    "    for fold_idx in range(xval_fold_count):  # k-fold x-validation\n",
    "\n",
    "        if fold_idx == 10:\n",
    "            break\n",
    "\n",
    "        best_model_mcc = -1\n",
    "\n",
    "        print(\"Fold\", fold_idx)\n",
    "\n",
    "        metrics_ht.reset_history()\n",
    "\n",
    "        train_datasets = []\n",
    "        val_datasets = []\n",
    "\n",
    "        bucket_sampling_idxs = np.random.permutation(list(range(len(X_buckets))))  # random ordering of buckets\n",
    "\n",
    "        training_seq_label_buckets = []\n",
    "        val_seq_label_buckets = []\n",
    "\n",
    "        training_num_features_buckets = []\n",
    "        val_num_features_buckets = []\n",
    "\n",
    "        training_bio_features_buckets = []\n",
    "        val_bio_features_buckets = []\n",
    "\n",
    "\n",
    "        # iterate over the train-val buckets to create separate train lists for each bucket and validation lists for each bucket\n",
    "        for bucket_idx in bucket_sampling_idxs:\n",
    "\n",
    "            bucket_input_features = X_buckets[bucket_idx]\n",
    "            bucket_labels = Y_buckets[bucket_idx]\n",
    "\n",
    "            train_idx = train_val_idxs_each_bucket[bucket_idx][fold_idx][0]\n",
    "            val_idx = train_val_idxs_each_bucket[bucket_idx][fold_idx][1]\n",
    "\n",
    "            # original indices of data points in each bucket (assuming no subsampling took place while bucketing)\n",
    "            #idxs_in_bucket = np.array(idxs_in_buckets[bucket_idx])\n",
    "\n",
    "            train_labels = bucket_labels[train_idx]\n",
    "            val_labels = bucket_labels[val_idx]\n",
    "\n",
    "            #train_data_idxs = idxs_in_bucket[train_idx]        \n",
    "            #val_data_idxs = idxs_in_bucket[val_idx]\n",
    "\n",
    "            seq_features = bucket_input_features[0]\n",
    "            train_seq_features = seq_features[train_idx,:]\n",
    "            val_seq_features = seq_features[val_idx,:]\n",
    "\n",
    "            training_seq_label_buckets.append( (train_seq_features, train_labels) )\n",
    "            val_seq_label_buckets.append( (val_seq_features, val_labels) )\n",
    "\n",
    "            if(use_features):\n",
    "\n",
    "                #train_features = (bucket_input_features[0][train_idx,:], bucket_input_features[1][train_idx,:], bucket_input_features[2][train_idx,:])\n",
    "                #val_features = (bucket_input_features[0][val_idx,:], bucket_input_features[1][val_idx,:], bucket_input_features[2][val_idx,:])\n",
    "\n",
    "                #training_seq_label_buckets.append( (train_features[0], train_labels) )\n",
    "                #val_seq_label_buckets.append( (val_features[0], val_labels) )\n",
    "\n",
    "                comp_features = bucket_input_features[1]\n",
    "                bio_features = bucket_input_features[2]\n",
    "\n",
    "                if use_features and use_comp_features:\n",
    "                    train_comp_features = comp_features[train_idx,:]\n",
    "                    val_comp_features = comp_features[val_idx,:]\n",
    "\n",
    "                    # kmers\n",
    "                    #training_num_features_buckets.append(np.concatenate([train_features[1], train_features[2]], axis = 1) )\n",
    "                    #val_num_features_buckets.append(np.concatenate([val_features[1], val_features[2]], axis = 1))\n",
    "                    training_num_features_buckets.append(train_comp_features)\n",
    "                    val_num_features_buckets.append(val_comp_features)\n",
    "\n",
    "                if use_features and use_bio_features:\n",
    "                    train_bio_features = bio_features[train_idx,:]\n",
    "                    val_bio_features = bio_features[val_idx,:]\n",
    "\n",
    "                    # bio only\n",
    "                    #training_bio_features_buckets.append(train_features[2])\n",
    "                    #val_bio_features_buckets.append(val_features[2])\n",
    "\n",
    "                    training_bio_features_buckets.append(train_bio_features)\n",
    "                    val_bio_features_buckets.append(val_bio_features)\n",
    "\n",
    "                #train_datasets.append(data_utils.TensorDataset(torch.from_numpy(train_features[0]), torch.from_numpy(train_features[1]).float(), torch.from_numpy(train_labels)))\n",
    "                #val_datasets.append(data_utils.TensorDataset(torch.from_numpy(val_features[0]), torch.from_numpy(val_features[1]).float(), torch.from_numpy(val_labels)))\n",
    "\n",
    "            #else:\n",
    "            #    train_features = bucket_input_features[train_idx,:]\n",
    "            #    val_features = bucket_input_features[val_idx]\n",
    "\n",
    "            #    train_datasets.append(data_utils.TensorDataset(torch.from_numpy(train_features), torch.from_numpy(train_labels)))\n",
    "            #    val_datasets.append(data_utils.TensorDataset(torch.from_numpy(val_features), torch.from_numpy(val_labels)))\n",
    "\n",
    "        # normalize numerical features combining data across buckets        \n",
    "        if(use_features):\n",
    "            if use_comp_features:\n",
    "                training_num_features = np.concatenate(training_num_features_buckets, axis = 0)\n",
    "                mu = np.mean(training_num_features, axis = 0)\n",
    "                sd = np.std(training_num_features, axis = 0)\n",
    "                training_num_features_buckets = [ (training_num_features_bucket - mu)/sd for training_num_features_bucket in training_num_features_buckets]\n",
    "\n",
    "                #training_num_features = (training_num_features - mu)/sd\n",
    "                #val_num_features = np.concatenate(val_num_features_buckets, axis = 0)\n",
    "                #val_num_features = (val_num_features - mu)/sd\n",
    "                #print(np.mean(training_num_features_buckets[0].squeeze(),0) )\n",
    "                #print(np.std(training_num_features_buckets[0].squeeze(),0) )\n",
    "                val_num_features_buckets = [ (val_num_features_bucket - mu)/sd for val_num_features_bucket in val_num_features_buckets]\n",
    "\n",
    "                num_features_means.append(mu)\n",
    "                num_features_sds.append(sd)\n",
    "            else: # dummy data\n",
    "                training_num_features_buckets = [ np.zeros( (training_seq_label_bucket[0].shape[0],252,1) ) for training_seq_label_bucket in training_seq_label_buckets ]\n",
    "                val_num_features_buckets = [ np.zeros( (val_seq_label_bucket[0].shape[0],252,1) ) for val_seq_label_bucket in val_seq_label_buckets ]\n",
    "\n",
    "            if use_bio_features:\n",
    "                training_bio_features = np.concatenate(training_bio_features_buckets, axis = 0)\n",
    "                bio_mu = np.mean(training_bio_features, axis = 0)\n",
    "                bio_sd = np.std(training_bio_features, axis = 0)\n",
    "                training_bio_features_buckets = [ (training_bio_features_bucket - bio_mu)/bio_sd for training_bio_features_bucket in training_bio_features_buckets]\n",
    "\n",
    "                #training_num_features = (training_num_features - mu)/sd\n",
    "                #val_num_features = np.concatenate(val_num_features_buckets, axis = 0)\n",
    "                #val_num_features = (val_num_features - mu)/sd\n",
    "                #print(np.mean(training_num_features_buckets[0].squeeze(),0) )\n",
    "                #print(np.std(training_num_features_buckets[0].squeeze(),0) )\n",
    "                val_bio_features_buckets = [ (val_bio_features_bucket - bio_mu)/bio_sd for val_bio_features_bucket in val_bio_features_buckets]\n",
    "\n",
    "                bio_features_means.append(bio_mu)\n",
    "                bio_features_sds.append(bio_sd)\n",
    "            else:  # dummy data\n",
    "                training_bio_features_buckets = [ np.zeros( (training_seq_label_bucket[0].shape[0],292,1) ) for training_seq_label_bucket in training_seq_label_buckets ]\n",
    "                val_bio_features_buckets = [ np.zeros( (val_seq_label_bucket[0].shape[0],292,1) ) for val_seq_label_bucket in val_seq_label_buckets ]\n",
    "\n",
    "        for bucket_idx in bucket_sampling_idxs:\n",
    "            #print(training_seq_label_buckets[bucket_idx][0].shape)\n",
    "            #print(training_num_features[bucket_idx].shape)\n",
    "            #print(training_seq_label_buckets[bucket_idx][1].shape)\n",
    "\n",
    "            train_seq_T = torch.from_numpy(training_seq_label_buckets[bucket_idx][0])\n",
    "            train_label_T = torch.from_numpy(training_seq_label_buckets[bucket_idx][1])\n",
    "            #train_dataset_components = [train_seq_T]\n",
    "            #train_dataset_components.append(train_label_T)\n",
    "\n",
    "            val_seq_T = torch.from_numpy(val_seq_label_buckets[bucket_idx][0])\n",
    "            val_label_T = torch.from_numpy(val_seq_label_buckets[bucket_idx][1])\n",
    "            #val_dataset_components = [val_seq_T]\n",
    "            #val_dataset_components.append(val_label_T)\n",
    "\n",
    "            #if use_comp_features: \n",
    "            train_num_T = torch.from_numpy(training_num_features_buckets[bucket_idx]).float()\n",
    "            #train_dataset_components.append(train_num_T)\n",
    "\n",
    "            val_num_T = torch.from_numpy(val_num_features_buckets[bucket_idx]).float()\n",
    "            #val_dataset_components.append(val_num_T)\n",
    "\n",
    "            #if use_bio_features: \n",
    "            train_bio_T = torch.from_numpy(training_bio_features_buckets[bucket_idx]).float()\n",
    "            #train_dataset_components.append(train_bio_T)\n",
    "\n",
    "            val_bio_T = torch.from_numpy(val_bio_features_buckets[bucket_idx]).float()\n",
    "            #val_dataset_components.append(val_bio_T)\n",
    "\n",
    "            train_datasets.append(data_utils.TensorDataset(train_seq_T, train_label_T, train_num_T, train_bio_T ) )\n",
    "            val_datasets.append(data_utils.TensorDataset(val_seq_T, val_label_T, val_num_T, val_bio_T))\n",
    "\n",
    "\n",
    "        train_dataloaders = [data_utils.DataLoader(train_dataset, batch_size = batch_sz, shuffle = True) for train_dataset in train_datasets]\n",
    "        val_dataloaders = [data_utils.DataLoader(val_dataset, batch_size = batch_sz, shuffle = False) for val_dataset in val_datasets]\n",
    "\n",
    "        num_iter_per_epoch = np.sum([len(loader) for loader in train_dataloaders])\n",
    "\n",
    "        training_losses = []\n",
    "        val_losses = []\n",
    "\n",
    "        # initialize the model\n",
    "        #model = RNA_Net(num_DNAs, 16).cuda()\n",
    "        #model = CNPPNet(num_DNAs, 16).cuda()\n",
    "\n",
    "    #         if(use_features):   \n",
    "    #             model = CNPPNet_Hybrid(num_DNAs, 16).cuda()\n",
    "    #         else:\n",
    "    #             model = FantomNet(num_DNAs, 16).cuda()\n",
    "\n",
    "        model = configs[config_name]['network'](configs[config_name]['num_features'], configs[config_name]['num_hidden']).cuda()\n",
    "\n",
    "        print(model)\n",
    "\n",
    "        # loss function, optimization algorithm, and final layer activation function\n",
    "        loss_function = torch.nn.CrossEntropyLoss(weight=torch.Tensor(list(class_weights.values())).cuda() )\n",
    "        activationFunc = torch.nn.LogSoftmax(dim = -1)\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr = 1e-3,  weight_decay = configs[config_name]['wd'])\n",
    "        scheduler = OneCycleCosineAnnealing(optimizer, \n",
    "                                            num_iter_per_epoch*num_epochs/configs[config_name]['cycle_freq'], \n",
    "                                            cycles_per_round = configs[config_name]['cycles_per_round'], \n",
    "                                            max_lr = configs[config_name]['max_lr'], \n",
    "                                            decay_rate = configs[config_name]['max_lr_decay'])\n",
    "\n",
    "        print(num_iter_per_epoch)\n",
    "\n",
    "\n",
    "\n",
    "        # train the network \n",
    "        for epoch in range(num_epochs):\n",
    "            print(\"Epoch\",epoch,\"started.\")\n",
    "\n",
    "            train_dataset_sizes = [len(train_dataloader.dataset) for train_dataloader in train_dataloaders]\n",
    "            val_dataset_sizes = [len(val_dataloader.dataset) for val_dataloader in val_dataloaders]\n",
    "\n",
    "            print(\"Training dataset size(s):\",*train_dataset_sizes)\n",
    "            print(\"Validation dataset size(s):\",*val_dataset_sizes)\n",
    "\n",
    "            training_loss = []\n",
    "            train_ys = []\n",
    "            train_preds = []\n",
    "\n",
    "            model = model.train()\n",
    "\n",
    "            # train the network                \n",
    "            for train_dataloader in tqdm_notebook(train_dataloaders):\n",
    "\n",
    "                #for x, y in train_dataloader:\n",
    "                for x, y, p, q in train_dataloader:\n",
    "                    model.zero_grad()\n",
    "                    optimizer.zero_grad()\n",
    "\n",
    "                    x = x.cuda()\n",
    "                    y = y.cuda()\n",
    "\n",
    "                    feature_data = []\n",
    "\n",
    "                    if use_comp_features:\n",
    "                        p = p.cuda()\n",
    "                        feature_data.append(p)\n",
    "\n",
    "                    if use_bio_features:\n",
    "                        q = q.cuda()\n",
    "                        feature_data.append(q)\n",
    "\n",
    "                    #x_lens = torch.sum(torch.any(x !=0, dim = -1), dim = -1)\n",
    "\n",
    "                    #torch.all(x !=0)\n",
    "\n",
    "                    # compute the length of each sequence in the batch (to be used for packing)\n",
    "                    # pad the sequences in the batch\n",
    "\n",
    "                    #logits = model(x, seq_lengths = x_lens)\n",
    "\n",
    "                    logits = model(x, feature_data)\n",
    "\n",
    "                    #logits = model(x, z)\n",
    "                    preds = torch.argmax(activationFunc(logits), dim = -1)\n",
    "\n",
    "                    #print(logits.shape)\n",
    "                    #print(y.shape)\n",
    "                    loss = loss_function(logits, y)\n",
    "                    training_loss.append(loss.item())\n",
    "\n",
    "                    train_ys.append(y.cpu().numpy())\n",
    "                    train_preds.append(preds.cpu().numpy())\n",
    "\n",
    "                    loss.backward()\n",
    "                    scheduler.step()\n",
    "                    optimizer.step()\n",
    "\n",
    "            training_losses.append(np.mean(training_loss))        \n",
    "            train_ys = np.concatenate(train_ys) \n",
    "            train_preds = np.concatenate(train_preds)\n",
    "\n",
    "            print(\"Epoch\",epoch,\"completed.\")\n",
    "\n",
    "            #print('________________________________________')\n",
    "            #print('Training metrics')\n",
    "            #results = metrics_ht.compute_metrics(train_ys, train_preds, epoch, do_print = False)\n",
    "            #print('training_accuracy:',results['accuracy'])\n",
    "            #print('________________________________________')\n",
    "\n",
    "            model = model.eval()\n",
    "            # compute validation loss, and accuracy metrics        \n",
    "            with torch.no_grad():\n",
    "                val_ys = []\n",
    "                val_preds = []\n",
    "                val_pred_probs = []\n",
    "                val_loss = []\n",
    "\n",
    "                for val_dataloader in val_dataloaders:\n",
    "                    #for x, y in val_dataloader:\n",
    "                    for x, y, p, q in val_dataloader:\n",
    "                        x = x.cuda()\n",
    "                        y = y.cuda()\n",
    "\n",
    "                        feature_data = []\n",
    "\n",
    "                        if use_comp_features:\n",
    "                            p = p.cuda()\n",
    "                            feature_data.append(p)\n",
    "\n",
    "                        if use_bio_features:\n",
    "                            q = q.cuda()\n",
    "                            feature_data.append(q)\n",
    "\n",
    "                        #x_lens = torch.sum(torch.any(x !=0, dim = -1), dim = -1)\n",
    "\n",
    "                        #logits = model(x, seq_lengths = x_lens.cpu())                \n",
    "\n",
    "                        logits = model(x, feature_data)                \n",
    "                        #logits = model(x, z)                \n",
    "                        log_pred_probs = activationFunc(logits)\n",
    "                        preds = torch.argmax(activationFunc(logits), dim = -1)\n",
    "\n",
    "                        loss = loss_function(logits, y)\n",
    "                        val_loss.append(loss.item())\n",
    "\n",
    "                        val_ys.append(y.cpu().numpy())\n",
    "                        val_preds.append(preds.cpu().numpy())\n",
    "                        val_pred_probs.append(np.exp(log_pred_probs.cpu().numpy()))\n",
    "                val_losses.append(np.mean(val_loss))\n",
    "\n",
    "\n",
    "            val_ys = np.concatenate(val_ys) \n",
    "            val_preds = np.concatenate(val_preds)\n",
    "            val_pred_probs = np.concatenate(val_pred_probs)\n",
    "            #val_accuracy = (val_ys==val_preds).sum()/len(val_preds) \n",
    "            #print(\"--- validation accuracy:\", val_accuracy)   \n",
    "            print('============================================================================================================')\n",
    "            print('Validation metrics')\n",
    "            val_results = metrics_ht.compute_metrics(val_ys, val_preds, epoch, do_print = True, store_vals = True)\n",
    "            print('============================================================================================================')\n",
    "            #print('Training metrics')\n",
    "            #results = metrics_ht.compute_metrics(train_ys, train_preds, epoch, do_print = True, store_vals = False)\n",
    "            #print('============================================================================================================')\n",
    "\n",
    "            line_colors = plt.cm.tab20(np.linspace(0,1,20))\n",
    "\n",
    "            tr_loss_color = line_colors[0]\n",
    "            val_loss_color = line_colors[2]\n",
    "\n",
    "\n",
    "\n",
    "            epoch_mcc = Metrics.compute_mcc(val_ys, val_preds)\n",
    "            epoch_sensitivity = Metrics.compute_sensitivity(val_ys, val_preds)\n",
    "            epoch_specificity = Metrics.compute_specificity(val_ys, val_preds)\n",
    "\n",
    "            if epoch_mcc > best_model_mcc:\n",
    "                network_name = model.getname()\n",
    "                best_model_mcc = epoch_mcc \n",
    "                now = datetime.now()\n",
    "                savefile_name = now.strftime(\"%H_%M_%S\")+\"_\"+now.strftime(\"%m\") + now.strftime(\"%d\") # + now.strftime(\"%Y\")\n",
    "                savedir = f'{network_name}/fold_{fold_idx:02d}'\n",
    "                if not os.path.exists(savedir):\n",
    "                    os.makedirs(savedir)\n",
    "\n",
    "                pth  = Path(savedir)\n",
    "                for f in list(pth.glob('*.pkl')):\n",
    "                    os.remove(f)\n",
    "\n",
    "                for f in list(pth.glob('*.npy')):\n",
    "                    os.remove(f)\n",
    "                    \n",
    "                prefix = savedir.replace('/','_')\n",
    "                torch.save(model.state_dict(), f'{savedir}/mdl_{prefix}_ep_{epoch:03d}_mcc_{epoch_mcc:.05f}_sens_{epoch_sensitivity:.05f}_spec_{epoch_specificity:.05f}__{savefile_name}.pkl')    \n",
    "\n",
    "                if use_comp_features:\n",
    "                    np.save(f'{savedir}/num_features_means_fold_{fold_idx}', num_features_means[fold_idx])\n",
    "                    np.save(f'{savedir}/num_features_sds_fold_{fold_idx}', num_features_sds[fold_idx])\n",
    "\n",
    "                if use_bio_features:\n",
    "                    np.save(f'{savedir}/bio_features_means_fold_{fold_idx}', bio_features_means[fold_idx])\n",
    "                    np.save(f'{savedir}/bio_features_sds_fold_{fold_idx}', bio_features_sds[fold_idx])\n",
    "                \n",
    "                best_results = metrics_ht.compute_metrics(val_ys, val_preds, epoch, do_print = False, store_vals = False)\n",
    "                # use to load and start inference:\n",
    "                # model.load_state_dict(torch.load(filepath))\n",
    "                # model.eval()\n",
    "\n",
    "    #             print(\"Test results\")\n",
    "    #             for result in all_folds_last_results:\n",
    "    #                 print(\"accuracy:\",result['accuracy'])\n",
    "    #                 print(\"sensitivity:\",result['sensitivity'])\n",
    "    #                 print(\"specificity:\",result['specificity'])\n",
    "\n",
    "\n",
    "            if epoch%5==0:   # only plot in even-numbered epoch to reduce stress on the webpage\n",
    "                x_data = range(epoch+1)\n",
    "\n",
    "                f, (ax1, ax2, ax3, ax4) = plt.subplots(1, 4,figsize=(12,2))\n",
    "                ax1.plot(x_data, training_losses, label=\"train loss\", color = tr_loss_color, alpha=1.0)\n",
    "                ax1.plot(x_data, val_losses, label=\"val_loss\", color = val_loss_color, alpha=1.0)\n",
    "                ax1.legend()\n",
    "                ax1.set_title(\"Fold: \"+str(fold_idx)+\" epoch: \"+str(epoch))\n",
    "\n",
    "                #ax = fig.subplot(122), alpha=.7\n",
    "                tmp = np.array(metrics_ht.tp_tn_fp_fn)\n",
    "                ax2.plot(x_data, tmp[:,2], label=\"False Positive\", color = tr_loss_color, alpha=1.0)\n",
    "                ax2.plot(x_data, tmp[:,3], label=\"False Negatives\", color = val_loss_color, alpha=1.0)\n",
    "                ax2.legend()\n",
    "                ax2.set_title(\"FP & FN Fold: \"+str(fold_idx)+\" epoch: \"+str(epoch))\n",
    "\n",
    "\n",
    "                #ax3.plot(x_data, [metrics_ht.b_accuracy]*len(x_data), label=\"b_accuracy\", color = 'r', alpha=.5)\n",
    "                #ax3.plot(x_data, [metrics_ht.b_sensitivity]*len(x_data), label=\"b_sensitivity\", color = 'g', alpha=.5)\n",
    "                #ax3.plot(x_data, [metrics_ht.b_specificity]*len(x_data), label=\"b_specificity\", color = 'b', alpha=.5)\n",
    "                #ax3.plot(x_data, [metrics_ht.b_f1]*len(x_data), label=\"b_f1_score\", color = 'c', alpha=.5)\n",
    "                #ax3.plot(x_data, [metrics_ht.b_auc_roc]*len(x_data), label=\"b_auc_roc\", color = 'k', alpha=.5)\n",
    "\n",
    "                #ax3.plot(x_data, metrics_ht.accuracies, '--', label=\"accuracy\", color = 'r')\n",
    "                #ax3.plot(x_data, metrics_ht.sensitivity, '--',  label=\"sensitivity\", color = 'g')\n",
    "                #ax3.plot(x_data, metrics_ht.specificity, '--',  label=\"specificity\", color = 'b')\n",
    "                #ax3.plot(x_data, metrics_ht.f1s,  '--', label=\"b_f1_score\", color = 'c')\n",
    "                #ax3.plot(x_data, metrics_ht.auc_roc,  '--', label=\"b_auc_roc\", color = 'k')\n",
    "\n",
    "                #ax3.legend(loc='lower left')\n",
    "                #ax3.set_title(\"Acc-Sensitivity-Specificity-F1-AUC for Fold: \"+str(k)+\" epoch: \"+str(epoch))\n",
    "\n",
    "                ax4.plot(range(len(scheduler.lrs)), scheduler.lrs, label=\"learning rates\")\n",
    "                ax4.legend()\n",
    "\n",
    "                plt.show()\n",
    "                plt.close()\n",
    "\n",
    "        print('--------------------------------------------------------------------------------')\n",
    "        print('--------------------------------------------------------------------------------')\n",
    "        print('--------------------------------------------------------------------------------')\n",
    "        print('--------------------------------------------------------------------------------')\n",
    "        print('--------------------------------------------------------------------------------')\n",
    "        print('--------------------------------------------------------------------------------')\n",
    "        print('--------------------------------------------------------------------------------')\n",
    "        print('--------------------------------------------------------------------------------')\n",
    "        print('--------------------------------------------------------------------------------')\n",
    "        print('--------------------------------------------------------------------------------')\n",
    "        all_folds_last_results.append(val_results)\n",
    "        all_folds_best_results_for_fold.append(best_results)\n",
    "\n",
    "    #     for result in all_folds_last_results:\n",
    "    #         print(result)\n",
    "\n",
    "    # for result in all_folds_best_results_for_fold:\n",
    "    #     print(result)\n",
    "\n",
    "    print(\"Best results from each fold\")\n",
    "    for fold_idx, result in enumerate(all_folds_best_results_for_fold):\n",
    "        print(f\"Fold {fold_idx:02d}: mcc:{result['mcc']:.05f} sensitivity:{result['sensitivity']:.05f} specificity:{result['specificity']:.05f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Cross validation results (for last epochs results from each fold ):\")\n",
    "# accuracy =  [result['accuracy'] for result in all_folds_last_results]\n",
    "# #print('mean accuracy:', np.mean(accuracy), np.std(accuracy))    \n",
    "# sensitivity =  [result['sensitivity'] for result in all_folds_last_results]\n",
    "# print('mean sensitivity:', np.mean(sensitivity), np.std(sensitivity))    \n",
    "# specificity =  [result['specificity'] for result in all_folds_last_results]\n",
    "# print('mean specificity:', np.mean(specificity), np.std(specificity))   \n",
    "# mcc =  [result['mcc'] for result in all_folds_last_results]\n",
    "# print('mean mcc:', np.mean(mcc), np.std(mcc))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if do_cross_validation:\n",
    "    print(\"Best results from each fold\")\n",
    "    for fold_idx, result in enumerate(all_folds_best_results_for_fold):\n",
    "        print(f\"Fold {fold_idx:02d}: mcc:{result['mcc']:.05f} sensitivity:{result['sensitivity']:.05f} specificity:{result['specificity']:.05f}\")\n",
    "\n",
    "    print(\"===========================================================\")\n",
    "    print(\"Cross validation results (for best results from each fold):\")\n",
    "    #accuracy =  [result['accuracy'] for result in all_folds_best_results_for_fold]\n",
    "    #print('mean accuracy:', np.mean(accuracy), np.std(accuracy))    \n",
    "    sensitivity =  [result['sensitivity'] for result in all_folds_best_results_for_fold]\n",
    "    print('mean sensitivity:', np.mean(sensitivity), np.std(sensitivity))    \n",
    "    specificity =  [result['specificity'] for result in all_folds_best_results_for_fold]\n",
    "    print('mean specificity:', np.mean(specificity), np.std(specificity))   \n",
    "    mcc =  [result['mcc'] for result in all_folds_best_results_for_fold]\n",
    "    print('mean mcc:', np.mean(mcc), np.std(mcc))   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing with best (manually selected) model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if do_testing_using_best_model:\n",
    "\n",
    "    model = configs[config_name]['network'](configs[config_name]['num_features'], configs[config_name]['num_hidden']).cuda()\n",
    "\n",
    "    best_fold_idx = 0\n",
    "\n",
    "    best_model_dir = model.getname() + f'/fold_{best_fold_idx:02d}'\n",
    "    #best_model_filename = 'mdl_FantomNet_BioMLP_fold_01_ep_159_mcc_0.64987_sens_0.85923_spec_0.81466__16_21_55_0814.pkl'\n",
    "\n",
    "    best_model_path = str(list(Path(f'{best_model_dir}/').glob('*.pkl'))[0])\n",
    "    #best_model_path = best_model_dir + '/' + str(best_model_filename)     \n",
    "    \n",
    "    test_model_dir = best_model_dir\n",
    "    test_model_path = best_model_path\n",
    "\n",
    "    \n",
    "    #metrics_ht = Metrics(benchmark_acc = 0.8169, benchmark_f1 = 1.0, benchmark_spec = 0.8060, benchmark_sen = 0.8277, benchmark_auc = 1.0, benchmark_mcc = .6476)\n",
    "    test_metrics_ht = Metrics(**configs[config_name]['bmarks'])\n",
    "    test_metrics_ht.reset_history()\n",
    "    test_bucket_sampling_idxs = np.random.permutation(list(range(len(test_X_buckets))))  # random ordering of buckets\n",
    "\n",
    "    test_seq_label_buckets = []\n",
    "    test_num_features_buckets = []\n",
    "    test_bio_features_buckets  = []\n",
    "    test_datasets = []\n",
    "\n",
    "    for test_bucket_idx in test_bucket_sampling_idxs:\n",
    "\n",
    "        test_bucket_input_features = test_X_buckets[test_bucket_idx]\n",
    "        test_bucket_labels = test_Y_buckets[test_bucket_idx]\n",
    "\n",
    "        test_seq_features = test_bucket_input_features[0]\n",
    "\n",
    "        test_seq_label_buckets.append( (test_seq_features, test_bucket_labels) )\n",
    "\n",
    "        if(use_features):\n",
    "\n",
    "            test_comp_features = test_bucket_input_features[1]\n",
    "            test_bio_features = test_bucket_input_features[2]\n",
    "\n",
    "            if use_features and use_comp_features:\n",
    "                test_num_features_buckets.append(test_comp_features)\n",
    "\n",
    "            if use_features and use_bio_features:\n",
    "                test_bio_features_buckets.append(test_bio_features)\n",
    "\n",
    "    if(use_features):\n",
    "        if use_comp_features:\n",
    "            num_features_means = np.load(f'{best_model_dir}/num_features_means_fold_{best_fold_idx}.npy')\n",
    "            num_features_sds = np.load(f'{best_model_dir}/num_features_sds_fold_{best_fold_idx}.npy')\n",
    "            test_num_features = np.concatenate(test_num_features_buckets, axis = 0)\n",
    "            mu = num_features_means[best_fold_idx]\n",
    "            sd = num_features_sds[best_fold_idx]\n",
    "            test_num_features_buckets = [ (test_num_features_bucket - mu)/sd for test_num_features_bucket in test_num_features_buckets]\n",
    "        else: # dummy data\n",
    "            test_num_features_buckets = [ np.zeros( (test_seq_label_bucket[0].shape[0],1,1) ) for test_seq_label_bucket in test_seq_label_buckets ]\n",
    "\n",
    "        if use_bio_features:\n",
    "            bio_features_means = np.load(f'{best_model_dir}/bio_features_means_fold_{best_fold_idx}.npy')\n",
    "            bio_features_sds = np.load(f'{best_model_dir}/bio_features_sds_fold_{best_fold_idx}.npy')\n",
    "            test_bio_features = np.concatenate(test_bio_features_buckets, axis = 0)\n",
    "            bio_mu = bio_features_means[best_fold_idx]\n",
    "            bio_sd = bio_features_sds[best_fold_idx]\n",
    "            test_bio_features_buckets = [ (test_bio_features_bucket - bio_mu)/bio_sd for test_bio_features_bucket in test_bio_features_buckets]        \n",
    "\n",
    "        else:  # dummy data\n",
    "            test_bio_features_buckets = [ np.zeros( (test_seq_label_bucket[0].shape[0],2,1) ) for test_seq_label_bucket in test_seq_label_buckets ]\n",
    "\n",
    "    for test_bucket_idx in test_bucket_sampling_idxs:\n",
    "        test_seq_T = torch.from_numpy(test_seq_label_buckets[test_bucket_idx][0])\n",
    "        test_label_T = torch.from_numpy(test_seq_label_buckets[test_bucket_idx][1])\n",
    "\n",
    "        #if use_comp_features: \n",
    "        test_num_T = torch.from_numpy(test_num_features_buckets[test_bucket_idx]).float()\n",
    "\n",
    "        #if use_bio_features: \n",
    "        test_bio_T = torch.from_numpy(test_bio_features_buckets[test_bucket_idx]).float()\n",
    "\n",
    "        test_datasets.append(data_utils.TensorDataset(test_seq_T, test_label_T, test_num_T, test_bio_T ) )\n",
    "\n",
    "\n",
    "    test_dataloaders = [data_utils.DataLoader(test_dataset, batch_size = batch_sz, shuffle = False) for test_dataset in test_datasets]\n",
    "\n",
    "    num_iter_per_test_epoch = np.sum([len(loader) for loader in test_dataloaders])\n",
    "\n",
    "    test_losses = []\n",
    "\n",
    "    # loss function and final layer activation function\n",
    "    loss_function = torch.nn.CrossEntropyLoss(weight=torch.Tensor(list(class_weights.values())).cuda() )\n",
    "    activationFunc = torch.nn.LogSoftmax(dim = -1)\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (neg_gt_test_preds_probs[:len(neg_gt_test_preds_probs)//2,:] + neg_gt_test_preds_probs[len(neg_gt_test_preds_probs)//2:,:])/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing by retraining with train+val set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best_fold_idx = 0\n",
    "# best_model_filename = 'mdl_BioMLP_fold_00_ep_034_mcc_0.67554_sens_0.89331_spec_0.79017__03_03_48_0809.pkl'\n",
    "\n",
    "# best_model_dir = model.getname() + f'/fold_{best_fold_idx:02d}'\n",
    "# best_model_path = best_model_dir + '/' + best_model_filename \n",
    "\n",
    "\n",
    "#metrics_ht = Metrics(benchmark_acc = 0.8169, benchmark_f1 = 1.0, benchmark_spec = 0.8060, benchmark_sen = 0.8277, benchmark_auc = 1.0, benchmark_mcc = .6476)\n",
    "\n",
    "if do_retraining or do_testing_only:\n",
    "\n",
    "    retrain_metrics_ht = Metrics(**configs[config_name]['bmarks'])\n",
    "    retrain_metrics_ht.reset_history()\n",
    "\n",
    "    test_metrics_ht = Metrics(**configs[config_name]['bmarks'])\n",
    "    test_metrics_ht.reset_history()\n",
    "\n",
    "    retrain_bucket_sampling_idxs = np.random.permutation(list(range(len(X_buckets))))  # random ordering of buckets\n",
    "    test_bucket_sampling_idxs = np.random.permutation(list(range(len(test_X_buckets))))  # random ordering of buckets\n",
    "\n",
    "    test_seq_label_buckets = []\n",
    "    test_num_features_buckets = []\n",
    "    test_bio_features_buckets  = []\n",
    "    test_datasets = []\n",
    "\n",
    "    retrain_seq_label_buckets = []\n",
    "    retrain_num_features_buckets = []\n",
    "    retrain_bio_features_buckets  = []\n",
    "    retrain_datasets = []\n",
    "\n",
    "\n",
    "    for retrain_bucket_idx in retrain_bucket_sampling_idxs:\n",
    "\n",
    "        retrain_bucket_input_features = X_buckets[retrain_bucket_idx]\n",
    "        retrain_bucket_labels = Y_buckets[retrain_bucket_idx]\n",
    "\n",
    "        retrain_seq_features = retrain_bucket_input_features[0]\n",
    "\n",
    "        retrain_seq_label_buckets.append( (retrain_seq_features, retrain_bucket_labels) )\n",
    "\n",
    "        if(use_features):\n",
    "\n",
    "            retrain_comp_features = retrain_bucket_input_features[1]\n",
    "            retrain_bio_features = retrain_bucket_input_features[2]\n",
    "\n",
    "            if use_features and use_comp_features:\n",
    "                retrain_num_features_buckets.append(retrain_comp_features)\n",
    "\n",
    "            if use_features and use_bio_features:\n",
    "                retrain_bio_features_buckets.append(retrain_bio_features)\n",
    "\n",
    "    for test_bucket_idx in test_bucket_sampling_idxs:\n",
    "\n",
    "        test_bucket_input_features = test_X_buckets[test_bucket_idx]\n",
    "        test_bucket_labels = test_Y_buckets[test_bucket_idx]\n",
    "\n",
    "        test_seq_features = test_bucket_input_features[0]\n",
    "\n",
    "        test_seq_label_buckets.append( (test_seq_features, test_bucket_labels) )\n",
    "\n",
    "        if(use_features):\n",
    "\n",
    "            test_comp_features = test_bucket_input_features[1]\n",
    "            test_bio_features = test_bucket_input_features[2]\n",
    "\n",
    "            if use_features and use_comp_features:\n",
    "                test_num_features_buckets.append(test_comp_features)\n",
    "\n",
    "            if use_features and use_bio_features:\n",
    "                test_bio_features_buckets.append(test_bio_features)\n",
    "\n",
    "    if(use_features):\n",
    "        if use_comp_features:\n",
    "\n",
    "            retrain_num_features = np.concatenate(retrain_num_features_buckets, axis = 0)\n",
    "            mu = np.mean(retrain_num_features, axis = 0)\n",
    "            sd = np.std(retrain_num_features, axis = 0)\n",
    "            retrain_num_features_buckets = [ (retrain_num_features_bucket - mu)/sd for retrain_num_features_bucket in retrain_num_features_buckets]\n",
    "\n",
    "            test_num_features_buckets = [ (test_num_features_bucket - mu)/sd for test_num_features_bucket in test_num_features_buckets]\n",
    "        else: # dummy data\n",
    "            retrain_num_features_buckets = [ np.zeros( (retrain_seq_label_bucket[0].shape[0],252,1) ) for retrain_seq_label_bucket in retrain_seq_label_buckets ]        \n",
    "            test_num_features_buckets = [ np.zeros( (test_seq_label_bucket[0].shape[0],252,1) ) for test_seq_label_bucket in test_seq_label_buckets ]\n",
    "\n",
    "        if use_bio_features:\n",
    "            retrain_bio_features = np.concatenate(retrain_bio_features_buckets, axis = 0)\n",
    "            bio_mu = np.mean(retrain_bio_features, axis = 0)\n",
    "            bio_sd = np.std(retrain_bio_features, axis = 0)\n",
    "            retrain_bio_features_buckets = [ (retrain_bio_features_bucket - bio_mu)/bio_sd for retrain_bio_features_bucket in retrain_bio_features_buckets]        \n",
    "\n",
    "            test_bio_features_buckets = [ (test_bio_features_bucket - bio_mu)/bio_sd for test_bio_features_bucket in test_bio_features_buckets]        \n",
    "\n",
    "        else:  # dummy data\n",
    "            retrain_bio_features_buckets = [ np.zeros( (retrain_seq_label_bucket[0].shape[0],292,1) ) for retrain_seq_label_bucket in retrain_seq_label_buckets ]\n",
    "            test_bio_features_buckets = [ np.zeros( (test_seq_label_bucket[0].shape[0],292,1) ) for test_seq_label_bucket in test_seq_label_buckets ]\n",
    "\n",
    "    for retrain_bucket_idx in retrain_bucket_sampling_idxs:\n",
    "        retrain_seq_T = torch.from_numpy(retrain_seq_label_buckets[retrain_bucket_idx][0])\n",
    "        retrain_label_T = torch.from_numpy(retrain_seq_label_buckets[retrain_bucket_idx][1])\n",
    "\n",
    "        #if use_comp_features: \n",
    "        retrain_num_T = torch.from_numpy(retrain_num_features_buckets[retrain_bucket_idx]).float()\n",
    "\n",
    "        #if use_bio_features: \n",
    "        retrain_bio_T = torch.from_numpy(retrain_bio_features_buckets[retrain_bucket_idx]).float()\n",
    "\n",
    "        retrain_datasets.append(data_utils.TensorDataset(retrain_seq_T, retrain_label_T, retrain_num_T, retrain_bio_T ) )\n",
    "\n",
    "\n",
    "    for test_bucket_idx in test_bucket_sampling_idxs:\n",
    "        test_seq_T = torch.from_numpy(test_seq_label_buckets[test_bucket_idx][0])\n",
    "        test_label_T = torch.from_numpy(test_seq_label_buckets[test_bucket_idx][1])\n",
    "\n",
    "        #if use_comp_features: \n",
    "        test_num_T = torch.from_numpy(test_num_features_buckets[test_bucket_idx]).float()\n",
    "\n",
    "        #if use_bio_features: \n",
    "        test_bio_T = torch.from_numpy(test_bio_features_buckets[test_bucket_idx]).float()\n",
    "\n",
    "        test_datasets.append(data_utils.TensorDataset(test_seq_T, test_label_T, test_num_T, test_bio_T ) )\n",
    "\n",
    "    retrain_dataloaders = [data_utils.DataLoader(retrain_dataset, batch_size = batch_sz, shuffle = True) for retrain_dataset in retrain_datasets]\n",
    "    test_dataloaders = [data_utils.DataLoader(test_dataset, batch_size = batch_sz, shuffle = True) for test_dataset in test_datasets]\n",
    "\n",
    "    num_iter_per_retrain_epoch = np.sum([len(loader) for loader in retrain_dataloaders])\n",
    "    num_iter_per_test_epoch = np.sum([len(loader) for loader in test_dataloaders])\n",
    "\n",
    "    model = configs[config_name]['network'](configs[config_name]['num_features'], configs[config_name]['num_hidden']).cuda()\n",
    "\n",
    "    \n",
    "    # loss function and final layer activation function\n",
    "    loss_function = torch.nn.CrossEntropyLoss(weight=torch.Tensor(list(class_weights.values())).cuda() )\n",
    "    activationFunc = torch.nn.LogSoftmax(dim = -1)\n",
    "    \n",
    "    if do_retraining:\n",
    "        \n",
    "        print(model)\n",
    "\n",
    "        \n",
    "        retrain_losses = []\n",
    "        \n",
    "\n",
    "        # use to load and start inference:\n",
    "        #model.load_state_dict(torch.load(best_model_path))\n",
    "        #model.eval()\n",
    "\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr = 1e-3,  weight_decay = configs[config_name]['wd'])\n",
    "        scheduler = OneCycleCosineAnnealing(optimizer, \n",
    "                                            num_iter_per_retrain_epoch*num_epochs/configs[config_name]['cycle_freq'], \n",
    "                                            cycles_per_round = configs[config_name]['cycles_per_round'], \n",
    "                                            max_lr = configs[config_name]['max_lr'], \n",
    "                                            decay_rate = configs[config_name]['max_lr_decay'])\n",
    "\n",
    "        print(num_iter_per_retrain_epoch)\n",
    "\n",
    "        model = model.train()\n",
    "\n",
    "        # train the network \n",
    "        for epoch in range(num_epochs):\n",
    "            print(\"Epoch\",epoch,\"started.\")\n",
    "\n",
    "            retrain_dataset_sizes = [len(retrain_dataloader.dataset) for retrain_dataloader in retrain_dataloaders]\n",
    "            print(\"Retrain dataset size(s):\",*retrain_dataset_sizes)\n",
    "\n",
    "            retraining_loss = []\n",
    "            retrain_ys = []\n",
    "            retrain_preds = []\n",
    "\n",
    "            # train the network                \n",
    "            for retrain_dataloader in tqdm_notebook(retrain_dataloaders):\n",
    "\n",
    "                #for x, y in train_dataloader:\n",
    "                for x, y, p, q in retrain_dataloader:\n",
    "                    model.zero_grad()\n",
    "                    optimizer.zero_grad()\n",
    "\n",
    "                    x = x.cuda()\n",
    "                    y = y.cuda()\n",
    "\n",
    "                    retrain_feature_data = []\n",
    "\n",
    "                    if use_comp_features:\n",
    "                        p = p.cuda()\n",
    "                        retrain_feature_data.append(p)\n",
    "\n",
    "                    if use_bio_features:\n",
    "                        q = q.cuda()\n",
    "                        retrain_feature_data.append(q)\n",
    "\n",
    "                    logits = model(x, retrain_feature_data)\n",
    "\n",
    "                    preds = torch.argmax(activationFunc(logits), dim = -1)\n",
    "\n",
    "                    loss = loss_function(logits, y)\n",
    "                    retraining_loss.append(loss.item())\n",
    "\n",
    "                    retrain_ys.append(y.cpu().numpy())\n",
    "                    retrain_preds.append(preds.cpu().numpy())\n",
    "\n",
    "                    loss.backward()\n",
    "                    scheduler.step()\n",
    "                    optimizer.step()\n",
    "\n",
    "            retrain_losses.append(np.mean(retraining_loss))        \n",
    "            retrain_ys = np.concatenate(retrain_ys) \n",
    "            retrain_preds = np.concatenate(retrain_preds)\n",
    "\n",
    "            print(\"Epoch\",epoch,\"completed.\")\n",
    "\n",
    "            print('============================================================================================================')\n",
    "            print('Retrain metrics')\n",
    "            retrain_results = retrain_metrics_ht.compute_metrics(retrain_ys, retrain_preds, epoch, do_print = True, store_vals = True)\n",
    "            print('============================================================================================================')    \n",
    "\n",
    "            if epoch%5==0: \n",
    "                line_colors = plt.cm.tab20(np.linspace(0,1,20))\n",
    "\n",
    "                retr_loss_color = line_colors[0]\n",
    "                retr_loss_color2 = line_colors[1]\n",
    "\n",
    "                x_data = range(epoch+1)\n",
    "\n",
    "                f, (ax1, ax2, ax3, ax4) = plt.subplots(1, 4,figsize=(12,2))\n",
    "                ax1.plot(x_data, retrain_losses, label=\"retrain loss\", color = retr_loss_color, alpha=1.0)\n",
    "                ax1.legend()\n",
    "                ax1.set_title(\"Epoch: \"+str(epoch))\n",
    "\n",
    "                #ax = fig.subplot(122), alpha=.7\n",
    "                tmp = np.array(retrain_metrics_ht.tp_tn_fp_fn)\n",
    "                ax2.plot(x_data, tmp[:,2], label=\"False Positive\", color = retr_loss_color, alpha=1.0)\n",
    "                ax2.plot(x_data, tmp[:,3], label=\"False Negatives\", color = retr_loss_color2, alpha=1.0)\n",
    "                ax2.legend()\n",
    "                ax2.set_title(\"FP & FN Epoch: \"+str(epoch))\n",
    "\n",
    "                ax4.plot(range(len(scheduler.lrs)), scheduler.lrs, label=\"learning rates\")\n",
    "                ax4.legend()\n",
    "\n",
    "                plt.show()\n",
    "                plt.close()\n",
    "\n",
    "\n",
    "        network_name = model.getname()\n",
    "        now = datetime.now()\n",
    "        savefile_name = now.strftime(\"%H_%M_%S\")+\"_\"+now.strftime(\"%m\") + now.strftime(\"%d\") # + now.strftime(\"%Y\")\n",
    "        savedir = f'{network_name}/retrained'\n",
    "        if not os.path.exists(savedir):\n",
    "            os.makedirs(savedir)\n",
    "\n",
    "        pth  = Path(savedir)\n",
    "        for f in list(pth.glob('*.pkl')):\n",
    "            os.remove(f)\n",
    "\n",
    "        prefix = savedir.replace('/','_')\n",
    "        torch.save(model.state_dict(), f'{savedir}/mdl_{prefix}__{savefile_name}.pkl')    \n",
    "\n",
    "        test_model_dir = savedir\n",
    "        test_model_path = f'{test_model_dir}/mdl_{prefix}__{savefile_name}.pkl'\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if do_retraining or do_testing_using_best_model or do_testing_only:\n",
    "\n",
    "    if(do_testing_only and not do_retraining):\n",
    "        \n",
    "        network_name = model.getname()\n",
    "        \n",
    "        savedir = f'{network_name}/retrained'\n",
    "        \n",
    "        test_model_path = list(Path(savedir).glob('*.pkl'))[0]\n",
    "\n",
    "        #test_model_dir = savedir\n",
    "        #test_model_path = test_model_dir + '/' + test_model_filename \n",
    "\n",
    "# do_testing_using_best_model = True\n",
    "# do_retraining = False\n",
    "# do_testing_only = False    \n",
    "    \n",
    "    # use to load and start inference:\n",
    "    model.load_state_dict(torch.load(test_model_path))\n",
    "    model.eval()\n",
    "\n",
    "    print(model)\n",
    "    \n",
    "    print(num_iter_per_test_epoch)\n",
    "\n",
    "    test_dataset_sizes = [len(test_dataloader.dataset) for test_dataloader in test_dataloaders]\n",
    "\n",
    "    print(\"Test dataset size(s):\",*test_dataset_sizes)\n",
    "\n",
    "    test_losses = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        test_ys = []\n",
    "        test_preds = []\n",
    "        test_preds_probs = []\n",
    "        test_loss = []\n",
    "\n",
    "        for test_dataloader in test_dataloaders:\n",
    "            #for x, y in val_dataloader:\n",
    "            for test_x, test_y, test_p, test_q in test_dataloader:\n",
    "                test_x = test_x.cuda()\n",
    "                test_y = test_y.cuda()\n",
    "\n",
    "                test_feature_data = []\n",
    "\n",
    "                if use_comp_features:\n",
    "                    test_p = test_p.cuda()\n",
    "                    test_feature_data.append(test_p)\n",
    "\n",
    "                if use_bio_features:\n",
    "                    test_q = test_q.cuda()\n",
    "                    test_feature_data.append(test_q)\n",
    "                #x_lens = torch.sum(torch.any(x !=0, dim = -1), dim = -1)\n",
    "\n",
    "                #logits = model(x, seq_lengths = x_lens.cpu())                \n",
    "\n",
    "                test_logits = model(test_x, test_feature_data)\n",
    "                #logits = model(x, z)                \n",
    "\n",
    "                preds = torch.argmax(activationFunc(test_logits), dim = -1)\n",
    "                log_pred_probs = activationFunc(test_logits)\n",
    "\n",
    "                loss = loss_function(test_logits, test_y)\n",
    "                test_loss.append(loss.item())\n",
    "\n",
    "                test_ys.append(test_y.cpu().numpy())\n",
    "                test_preds.append(preds.cpu().numpy())\n",
    "                test_preds_probs.append(np.exp(log_pred_probs.cpu().numpy()))\n",
    "\n",
    "        test_losses.append(np.mean(test_loss))\n",
    "\n",
    "\n",
    "    test_ys = np.concatenate(test_ys) \n",
    "    test_preds = np.concatenate(test_preds)\n",
    "    test_preds_probs = np.concatenate(test_preds_probs)\n",
    "\n",
    "    test_ys_processed = test_ys\n",
    "    test_preds_processed = test_preds\n",
    "\n",
    "    if do_horz_flip:    \n",
    "        neg_test_ys = test_ys[:test_num_neg]\n",
    "        pos_test_ys = test_ys[test_num_neg:]\n",
    "\n",
    "        neg_gt_test_preds = test_preds[:test_num_neg]\n",
    "        pos_gt_test_preds = test_preds[test_num_neg:]\n",
    "\n",
    "        neg_gt_test_preds_probs = test_preds_probs[:test_num_neg,:]\n",
    "        pos_gt_test_preds_probs = test_preds_probs[test_num_neg:,:]\n",
    "\n",
    "        # fold in half\n",
    "        neg_test_ys = neg_test_ys[::2]\n",
    "        pos_test_ys = pos_test_ys[::2]\n",
    "\n",
    "        neg_gt_test_preds_probs = (neg_gt_test_preds_probs[:len(neg_gt_test_preds_probs)//2,:] + neg_gt_test_preds_probs[len(neg_gt_test_preds_probs)//2:,:])/2\n",
    "        pos_gt_test_preds_probs = (pos_gt_test_preds_probs[:len(pos_gt_test_preds_probs)//2,:] + pos_gt_test_preds_probs[len(pos_gt_test_preds_probs)//2:,:])/2\n",
    "\n",
    "        neg_gt_preds = np.argmax(neg_gt_test_preds_probs, axis = -1)\n",
    "        pos_gt_preds = np.argmax(pos_gt_test_preds_probs, axis = -1)\n",
    "\n",
    "        test_ys_processed = np.concatenate([neg_test_ys, pos_test_ys], axis = 0)\n",
    "        test_preds_processed = np.concatenate([neg_gt_preds, pos_gt_preds], axis = 0)\n",
    "\n",
    "    print('============================================================================================================')\n",
    "    print('Test metrics')\n",
    "    test_results = test_metrics_ht.compute_metrics(test_ys_processed, test_preds_processed, 0, do_print = True, store_vals = False)\n",
    "    print('============================================================================================================')\n",
    "\n",
    "    test_mcc = Metrics.compute_mcc(test_ys, test_preds)\n",
    "    test_sensitivity = Metrics.compute_sensitivity(test_ys, test_preds)\n",
    "    test_specificity = Metrics.compute_specificity(test_ys, test_preds)\n",
    "\n",
    "\n",
    "    print(\"Test results\")\n",
    "    print(\"sensitivity:\",test_results['sensitivity'])\n",
    "    print(\"specificity:\",test_results['specificity'])\n",
    "    print(\"mcc:\",test_results['mcc'])\n",
    "    print(\"accuracy:\",test_results['accuracy'])\n",
    "\n",
    "\n",
    "    print('--------------------------------------------------------------------------------')\n",
    "    print('--------------------------------------------------------------------------------')\n",
    "    print('--------------------------------------------------------------------------------')\n",
    "    print('--------------------------------------------------------------------------------')\n",
    "    print('--------------------------------------------------------------------------------')\n",
    "    print('--------------------------------------------------------------------------------')\n",
    "    print('--------------------------------------------------------------------------------')\n",
    "    print('--------------------------------------------------------------------------------')\n",
    "    print('--------------------------------------------------------------------------------')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if do_testing_only or do_testing_using_best_model or do_retraining:\n",
    "    print(\"Test results\")\n",
    "    print(\"sensitivity:\",test_results['sensitivity'])\n",
    "    print(\"specificity:\",test_results['specificity'])\n",
    "    print(\"mcc:\",test_results['mcc'])\n",
    "    print(\"accuracy:\",test_results['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper functions for Saliency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_kmers(alphabet, k):\n",
    "    kmers_list = []\n",
    "    kmers_list.append(alphabet)\n",
    "    \n",
    "    for k_idx in range(k-1):\n",
    "        kmers_list.append([a+b for a in kmers_list[k_idx] for b in alphabet ])\n",
    "\n",
    "    seq_lst = list(seq)\n",
    "    subs = [\"\".join(seq_lst[idx:idx+k]) for idx in range(len(seq)-k+1)]\n",
    "    kmers = kmers_list[k-1]\n",
    "\n",
    "    return kmers\n",
    "\n",
    "def sequence2encodings(alphabet, sequence_lst):\n",
    "\n",
    "    label_encodings = np.array([alphabet.index(c)+1  for seq in sequence_lst for c in seq ]).reshape(len(sequence_lst),-1)\n",
    "\n",
    "    onehot_encodings=[(np.arange(len(alphabet)+1) == label_encoding[:,None]).astype(dtype='float32') for label_encoding in label_encodings]#one_hot\n",
    "\n",
    "    onehot_encodings = np.array([np.delete(onehot,0, axis=-1) for onehot in onehot_encodings]).reshape(len(onehot_encodings),-1, len(alphabet))\n",
    "        \n",
    "    return label_encodings, onehot_encodings\n",
    "\n",
    "def onehots2sequences(alphabet, onehot_encodings):\n",
    "\n",
    "    for onehot_encoding in onehot_encodings: \n",
    "        assert onehot_encoding.shape[-1] ==len(alphabet), \"One hot sequence must be batch_sz x seq_length x alphabet_sz\"\n",
    "    \n",
    "    DNA_sequences = [\"\".join(seq_lst) for seq_lst in [[alphabet[i] for i in np.argmax(one_hot, axis = 1)] for one_hot in onehot_encodings ]]\n",
    "\n",
    "    label_encodings = np.array([alphabet.index(c)+1  for seq in DNA_sequences for c in seq ]).reshape(len(DNA_sequences),-1)\n",
    "\n",
    "    return DNA_sequences, label_encodings\n",
    "\n",
    "\n",
    "def mutate_seq_for_saliencymap(sequence_to_mutate, window_sz, mutation_probs, mutation_rate_pct, stride = None, start_idx = 0, alphabet = None, include_identicals = False ):\n",
    "    \n",
    "    if stride is None or stride <= 0:\n",
    "        stride = window_sz\n",
    "        \n",
    "    if alphabet is None:\n",
    "        alphabet = list(sorted(list(set(list(sequence_to_mutate)))))\n",
    "    \n",
    "    seq_len = len(sequence_to_mutate)\n",
    "    region_start_idxs = np.array(list(range(start_idx, seq_len, stride)))\n",
    "    region_end_idxs = region_start_idxs + window_sz \n",
    "    #print(region_start_idxs, region_end_idxs, region_end_idxs>(seq_len))\n",
    "    \n",
    "    region_end_idxs[region_end_idxs>(seq_len)]=seq_len\n",
    "    #print(region_start_idxs, region_end_idxs, )\n",
    "    \n",
    "    mutation_segments = [sequence_to_mutate[start:start+window_sz] for start in region_start_idxs]\n",
    "\n",
    "    mutation_counts = [int(len(mutation_segment)*mutation_rate_pct) for mutation_segment in mutation_segments]\n",
    "    #mutation_locations = [np.random.randint(0,len(mutation_segment), size=(1, mutation_count)) for mutation_segment,mutation_count in zip(mutation_segments, mutation_counts)]\n",
    "    mutation_locations_segments = [[np.random.randint(0,len(mutation_segment), size=(1, mutation_count))] for mutation_segment, mutation_count in zip(mutation_segments, mutation_counts)]\n",
    "    \n",
    "    #print(alphabet, list(mutation_probs.values()))\n",
    "    replacements_segments = [np.random.choice(alphabet, size = (mutation_count, ), p = list(mutation_probs.values())) for mutation_count in mutation_counts ]\n",
    "    #print(\"mutation_locations_segments:\", mutation_locations_segments)\n",
    "    #print(\"replacements_segments:\", replacements_segments)\n",
    "    #print()\n",
    "    mutated_segments = []\n",
    "    for mutation_segment, mutation_locations_segment, replacements_segment in zip(mutation_segments, mutation_locations_segments, replacements_segments):\n",
    "        #print(\"mutation_locations_segment\",mutation_locations_segment)\n",
    "        #print(\"replacements_segment\",replacements_segment)\n",
    "        mutation_locations_segment = np.array(mutation_locations_segment).reshape(1,)\n",
    "        \n",
    "        mutated_segment = list(mutation_segment[:]) \n",
    "        #print(mutated_segment)\n",
    "        #\n",
    "        for i_mloc, mloc in enumerate(np.array(mutation_locations_segment)):\n",
    "            mutated_segment[mloc]=replacements_segment[i_mloc] \n",
    "        #    print(\"    \", mloc, replacements_segment[i_mloc] )\n",
    "        #print(mutated_segment)\n",
    "        mutated_segments.append(\"\".join(mutated_segment))\n",
    "        #print()\n",
    "    \n",
    "    mutated_sequence = sequence_to_mutate[:] # copy the sequence since strings are immutable\n",
    "    \n",
    "    for mutated_segment, start, end in zip(mutated_segments, region_start_idxs, region_end_idxs):\n",
    "        #print(start,sequence_to_mutate, mutated_sequence[:start+1], mutated_segment, mutated_sequence[end:])\n",
    "        mutated_sequence = mutated_sequence[:start] + mutated_segment + mutated_sequence[end:]\n",
    "        #print(sequence_to_mutate)\n",
    "        #print(mutated_sequence)\n",
    "        #print()\n",
    "    \n",
    "    if not include_identicals:\n",
    "        mutated_idxs = [i for i in range(len(sequence_to_mutate)) if sequence_to_mutate[i]!=mutated_sequence[i] ]\n",
    "    else:\n",
    "        mutated_idxs = list(range(len(sequence_to_mutate)))\n",
    "        \n",
    "    all_one_pos_change_sequences = []\n",
    "    for i in mutated_idxs:\n",
    "        all_one_pos_change_sequences.append(\"\".join(list(sequence_to_mutate[:i])+list(mutated_sequence[i])+list(sequence_to_mutate[i+1:])))\n",
    "    \n",
    "    #return mutation_segments, mutated_segments, sequence_to_mutate, mutated_sequence, all_one_pos_change_sequences\n",
    "    return sequence_to_mutate, mutated_sequence, all_one_pos_change_sequences\n",
    "    \n",
    "    \n",
    "    \n",
    "alphabet = unique_DNAs\n",
    "kmers = get_kmers(alphabet, 2)\n",
    "\n",
    "mutation_probs = {'eq':{'A': 0.25, 'C': 0.25, 'G': 0.25, 'T': 0.25 }, \n",
    "                  'A': {'A': 1.,   'C': 0.,   'G': 0.,   'T': 0.,  },\n",
    "                  'C': {'A': 0.,   'C': 1.,   'G': 0.,   'T': 0.,  },\n",
    "                  'G': {'A': 0.,   'C': 0.,   'G': 1.,   'T': 0.,  },\n",
    "                  'T': {'A': 0.,   'C': 0.,   'G': 0.,   'T': 1.,  },\n",
    "                 }\n",
    "\n",
    "\n",
    "# # # labelencodings, one_hots = sequence2encodings(alphabet, kmers)\n",
    "# # # DNA_seqs, labelencodings_2 = onehots2sequences(alphabet, one_hots)\n",
    "\n",
    "# # # DNA_seqs, kmers, labelencodings, labelencodings_2, one_hots\n",
    "# # # \"\".join(list(map(chr,list(range(65,65+26)))))\n",
    "# # #\"\".join(DNA_seqs)\n",
    "# # #print(mut_segs),\"\".join(DNA_seqs), mutation_segments, mutated_segments\n",
    "# # # print(mutation_segments)\n",
    "# # # print(mutated_segments)\n",
    "# # # print()\n",
    "\n",
    "# test_seq = \"AGCTATG\"\n",
    "\n",
    "# all_one_place_changes_for_all_DNA = []\n",
    "# for DNA in unique_DNAs:\n",
    "#     orig_seq, mut_seq, all_changed = mutate_seq_for_saliencymap(test_seq, \n",
    "#                                                                 window_sz = 1, \n",
    "#                                                                 mutation_probs = mutation_probs[DNA], \n",
    "#                                                                 mutation_rate_pct = 1., \n",
    "#                                                                 stride = 1, \n",
    "#                                                                 alphabet = unique_DNAs, \n",
    "#                                                                 include_identicals = True)\n",
    "#     all_one_place_changes_for_all_DNA.extend(all_changed)\n",
    "# len(all_one_place_changes_for_all_DNA), print(all_one_place_changes_for_all_DNA)\n",
    "\n",
    "\n",
    "# # #[print(c_seq) for c_seq in all_one_place_changes_for_all_DNA]    \n",
    "# # #print(len(all_one_place_changes_for_all_DNA))\n",
    "# # all_one_place_changes_for_all_DNA = list(sorted(list(set(all_one_place_changes_for_all_DNA))))    \n",
    "# # #print(len(all_one_place_changes_for_all_DNA))\n",
    "# # [print(c_seq) for c_seq in all_one_place_changes_for_all_DNA]\n",
    "# # print(orig_seq)\n",
    "# # print(mut_seq)\n",
    "# # print()\n",
    "# # _, ohe = sequence2encodings(alphabet, mut_seq)\n",
    "# # [print(c_seq) for c_seq in all_one_place_changes_for_all_DNA]\n",
    "\n",
    "\n",
    "# # #print(ohe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute Saliency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take the first bucket for now\n",
    "\n",
    "bucket_idx = 0\n",
    "\n",
    "test_X_onehots = test_datasets[bucket_idx][:][0]\n",
    "test_Ys = test_datasets[bucket_idx][:][1]\n",
    "test_num_features = test_datasets[bucket_idx][:][2]\n",
    "test_bio_features = test_datasets[bucket_idx][:][3]\n",
    "\n",
    "test_X_seqs, test_X_label_encodings = onehots2sequences(unique_DNAs, test_X_onehots)\n",
    "\n",
    "\n",
    "test_X_onehots.shape, len(test_X_seqs), len(test_Ys), test_Ys[285:295], test_num_features.shape, test_bio_features.shape, type(test_X_onehots), type(test_bio_features) \n",
    "\n",
    "\n",
    "# test_num_features_buckets, test_bio_features_buckets\n",
    "\n",
    "# for test_bucket_idx in test_bucket_sampling_idxs:\n",
    "#     test_seq_T = torch.from_numpy(test_seq_label_buckets[test_bucket_idx][0])\n",
    "#     test_label_T = torch.from_numpy(test_seq_label_buckets[test_bucket_idx][1])\n",
    "\n",
    "#     #if use_comp_features: \n",
    "#     test_num_T = torch.from_numpy(test_num_features_buckets[test_bucket_idx]).float()\n",
    "    \n",
    "#     #if use_bio_features: \n",
    "#     test_bio_T = torch.from_numpy(test_bio_features_buckets[test_bucket_idx]).float()\n",
    "    \n",
    "#     test_datasets.append(data_utils.TensorDataset(test_seq_T, test_label_T, test_num_T, test_bio_T ) )\n",
    "\n",
    "\n",
    "# print(len(X_seqs))\n",
    "\n",
    "# verifying generated sequences and onehot conversion algorithm's correctness\n",
    "# X_label_encodings, X_onehot_from_seq = sequence2encodings(unique_DNAs, X_seqs)\n",
    "# torch.all(np.equal(X_onehot_from_seq[0], X_onehots[0])).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphabet = unique_DNAs\n",
    "\n",
    "# kmers = get_kmers(alphabet, 2)\n",
    "\n",
    "mutation_probs = {'eq':{'A': 0.25, 'C': 0.25, 'G': 0.25, 'T': 0.25 }, \n",
    "                  'A': {'A': 1.,   'C': 0.,   'G': 0.,   'T': 0.,  },\n",
    "                  'C': {'A': 0.,   'C': 1.,   'G': 0.,   'T': 0.,  },\n",
    "                  'G': {'A': 0.,   'C': 0.,   'G': 1.,   'T': 0.,  },\n",
    "                  'T': {'A': 0.,   'C': 0.,   'G': 0.,   'T': 1.,  },\n",
    "                 }\n",
    "\n",
    "saliency_maps = dict()\n",
    "\n",
    "for sample_class_label_int in [0,1]:  # for both negative and positive classes\n",
    "    \n",
    "    correct_classification_idxs_for_class = np.where(np.array(test_preds == test_ys) & np.array(test_ys == sample_class_label_int))[0].squeeze()\n",
    "\n",
    "    sort_idxs = np.argsort(test_preds_probs[correct_classification_idxs_for_class, sample_class_label_int])\n",
    "    best_performing_examples = correct_classification_idxs_for_class[sort_idxs]  # one with highest probability performing is at -1\n",
    "    \n",
    "    print(correct_classification_idxs_for_class.shape)\n",
    "    print(test_preds_probs[correct_classification_idxs_for_class, sample_class_label_int].shape)\n",
    "    print(best_performing_examples.shape)\n",
    "    \n",
    "    \n",
    "    saliency_maps_for_class = []\n",
    "    \n",
    "    then = time.time() \n",
    "    for i_sample_idx, sample_idx in enumerate(best_performing_examples):\n",
    "        if i_sample_idx%(len(best_performing_examples)//20) == 0:\n",
    "            print(f\"{i_sample_idx}/{len(best_performing_examples)}\")\n",
    "        sequence_to_mutate_idx = sample_idx #best_performing_examples[sample_idx] # one with highest probability performing is at -1\n",
    "        sequence_to_mutate = test_X_seqs[sequence_to_mutate_idx]\n",
    "        label_of_sequence_to_mutate = test_Ys[sequence_to_mutate_idx]\n",
    "\n",
    "        #orig_seq, mut_seq, all_one_step_changed = mutate_seq_for_saliencymap( sequence_to_mutate, window_sz = 3, mutation_probs = mutation_probs[1], mutation_rate_pct = 0.5, stride = 3)\n",
    "\n",
    "        all_one_step_changed = []\n",
    "        for DNA in unique_DNAs:\n",
    "            orig_seq, mut_seq, all_changed = mutate_seq_for_saliencymap( sequence_to_mutate, window_sz = 1, mutation_probs = mutation_probs[DNA], mutation_rate_pct = 1., \n",
    "                                                                        stride = 1, alphabet = unique_DNAs, include_identicals=True)\n",
    "            all_one_step_changed.extend(all_changed)\n",
    "\n",
    "\n",
    "        _, X_onehot_mutated = sequence2encodings(unique_DNAs, [sequence_to_mutate]+all_one_step_changed)\n",
    "        X_onehot_mutated.shape\n",
    "\n",
    "        saliencyy_Xs = torch.from_numpy(X_onehot_mutated)\n",
    "        saliency_Ys = torch.from_numpy(np.zeros(shape=(X_onehot_mutated.shape[0],), dtype=np.long)+label_of_sequence_to_mutate.item())\n",
    "        saliency_data_idx = torch.from_numpy(np.arange(X_onehot_mutated.shape[0]))\n",
    "        \n",
    "        saliency_dummy_num_features = torch.from_numpy(np.zeros((saliencyy_Xs.shape[0],252,1) )).float()\n",
    "        saliency_dummy_bio_features = torch.from_numpy(np.zeros((saliencyy_Xs.shape[0],292,1) )).float()\n",
    "        \n",
    "        \n",
    "        saliency_dataset = data_utils.TensorDataset(saliencyy_Xs, saliency_Ys, saliency_dummy_num_features, saliency_dummy_bio_features, saliency_data_idx)\n",
    "\n",
    "        len(saliency_dataset), type(saliencyy_Xs), type(saliency_Ys), saliency_Ys.shape, saliencyy_Xs.dtype, saliency_Ys.dtype\n",
    "\n",
    "        saliency_loader_batch_sz = 512\n",
    "\n",
    "        saliency_dataloader = data_utils.DataLoader(saliency_dataset, batch_size = saliency_loader_batch_sz, shuffle = False)\n",
    "\n",
    "\n",
    "        model = model.eval()\n",
    "        # compute validation loss, and accuracy metrics        \n",
    "        with torch.no_grad():\n",
    "\n",
    "            saliency_ys = []\n",
    "            saliency_preds = []\n",
    "            saliency_pred_probs = []\n",
    "            saliency_loss = []\n",
    "            saliency_losses = []\n",
    "\n",
    "            saliency_x_idxs = []\n",
    "            for saliency_x, saliency_y, dummy_num_features, dummy_bio_features, saliency_x_batch_idxs in saliency_dataloader:\n",
    "            #for x, z, y in val_dataloader:\n",
    "                saliency_x = saliency_x.cuda()\n",
    "                #z = z.cuda()\n",
    "                saliency_y = saliency_y.cuda()\n",
    "                #x_lens = torch.sum(torch.any(x !=0, dim = -1), dim = -1)\n",
    "\n",
    "                #logits = model(x, seq_lengths = x_lens.cpu())                \n",
    "\n",
    "                saliency_feature_data = []\n",
    "\n",
    "                if use_comp_features:\n",
    "                    dummy_num_features = dummy_num_features.cuda()\n",
    "                    saliency_feature_data.append(dummy_num_features)\n",
    "\n",
    "                if use_bio_features:\n",
    "                    dummy_bio_features = dummy_bio_features.cuda()\n",
    "                    saliency_feature_data.append(dummy_bio_features)\n",
    "                \n",
    "                saliency_logits = model(saliency_x, saliency_feature_data)                \n",
    "                #logits = model(x, z)                \n",
    "                saliency_log_pred_probs_batch = activationFunc(saliency_logits)\n",
    "                saliency_preds_batch = torch.argmax(saliency_log_pred_probs_batch, dim = -1)\n",
    "\n",
    "                #print(saleincy_logits.shape, saleincy_y.shape )\n",
    "                saliency_loss_batch = loss_function(saliency_logits, saliency_y)\n",
    "\n",
    "                saliency_loss.append(saliency_loss_batch.item())\n",
    "                saliency_ys.append(saliency_y.cpu().numpy())\n",
    "                saliency_pred_probs.append(np.exp(saliency_log_pred_probs_batch.cpu().numpy()))\n",
    "                saliency_preds.append(saliency_preds_batch.cpu().numpy())\n",
    "                saliency_x_idxs.append(saliency_x_batch_idxs)\n",
    "\n",
    "            saliency_losses.append(np.mean(saliency_loss))\n",
    "\n",
    "\n",
    "        saliency_ys = np.concatenate(saliency_ys) \n",
    "        saliency_preds = np.concatenate(saliency_preds)\n",
    "        saliency_pred_probs = np.concatenate(saliency_pred_probs)\n",
    "        saliency_x_idxs = np.concatenate(saliency_x_idxs)\n",
    "\n",
    "        #sample_class_label_int\n",
    "        #saliency_pred_probs[:5,sample_class_label_int], saliency_preds[:5], np.unique(saliency_ys), label_of_sequence_to_mutate, val_pred_probs[best_performing_examples[-1],sample_class_label_int]\n",
    "        score_diff = saliency_pred_probs[:, sample_class_label_int] - saliency_pred_probs[0, sample_class_label_int]\n",
    "\n",
    "        saliency_map = score_diff[1:].reshape(1,-1).reshape(len(unique_DNAs), len(orig_seq))\n",
    "        score_diff\n",
    "        saliency_maps_for_class.append(saliency_map)\n",
    "\n",
    "    now = time.time()\n",
    "    print(f\"Time taken for class{sample_class_label_int}:{now-then} seconds\")\n",
    "    # plot saliency matrix\n",
    "    saliency_maps[sample_class_label_int] = saliency_maps_for_class\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot Saliency maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sample_class_label_int in [0,1]:    \n",
    "    score_multipliers = [-1.,1.]\n",
    "    saliency_map = np.mean(np.array(saliency_maps[sample_class_label_int]), axis = 0) \n",
    "    saliency_map *= score_multipliers[sample_class_label_int]\n",
    "    #print(\"saliency_map shape:\", saliency_map.shape)\n",
    "    #print(f'For class {class_names[sample_class_label_int]}')\n",
    "    sns.set()\n",
    "    #sns.set_context(\"poster\", font_scale = .5, rc={\"grid.linewidth\": 5})\n",
    "\n",
    "    span_min = 350\n",
    "    span_max = 450 + 1\n",
    "    label_min = span_min - 400\n",
    "    label_max = label_min + (span_max-span_min) + 1\n",
    "    plt.figure(figsize=(30,3))\n",
    "    # sns_cmap = sns.palplot(sns.diverging_palette(240, 10, n=9, as_cmap = True))\n",
    "    ax= sns.heatmap(saliency_map[:,span_min:span_max], \n",
    "                cmap = sns.diverging_palette(220,20, n=250, as_cmap = True), \n",
    "                vmax = np.max(saliency_map[:,span_min:span_max]), \n",
    "                vmin = np.min(saliency_map[:,span_min:span_max]), \n",
    "                center = 0.0, \n",
    "                cbar=False)\n",
    "\n",
    "    #ax.set_aspect(\"equal\")\n",
    "\n",
    "    plt.grid(True, color='r', linestyle='--', linewidth=2, which = 'both')\n",
    "    loc = ticker.MultipleLocator(base=2)\n",
    "    ax.xaxis.set_minor_locator(loc)\n",
    "    ax.yaxis.set_minor_locator(loc)\n",
    "\n",
    "    plt.xticks(np.arange(0,span_max-span_min,10)+0.5, np.arange(label_min,label_max,10), fontsize=20)\n",
    "    plt.yticks(np.arange(0,saliency_map.shape[0])+0.5, unique_DNAs, fontsize=20)\n",
    "    #ax.set_yticklabels(ax.get_yticklabels(), rotation=0)x sssssssssdem  n\n",
    "    plt.yticks(rotation=0)\n",
    "    plt.title(\"(\" + chr(65+sample_class_label_int) + f\") For {class_names[sample_class_label_int]} promoter\", fontsize=20)\n",
    "    # ax.get_xaxis().set_minor_locator(ticker.AutoMinorLocator())\n",
    "    # ax.get_yaxis().set_minor_locator(ticker.AutoMinorLocator())\n",
    "    # ax.grid(b=True, which='major', color='w', linewidth=10.0)\n",
    "    # ax.grid(b=True, which='minor', color='w', linewidth=15)\n",
    "    gridlinewidth = 1.6\n",
    "    ax.hlines(np.arange(len(unique_DNAs)+1), *ax.get_xlim(), color='w', linewidth=gridlinewidth)\n",
    "    ax.vlines(np.arange(0,span_max-span_min+1), *ax.get_ylim(), color='w', linewidth=gridlinewidth)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot saliency map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PWM generation step zero for PWM Exp 1 and PWM Exp 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphabet = unique_DNAs\n",
    "\n",
    "# kmers = get_kmers(alphabet, 2)\n",
    "\n",
    "mutation_probs = {'eq':{'A': 0.25, 'C': 0.25, 'G': 0.25, 'T': 0.25 }, \n",
    "                  'A': {'A': 1.,   'C': 0.,   'G': 0.,   'T': 0.,  },\n",
    "                  'C': {'A': 0.,   'C': 1.,   'G': 0.,   'T': 0.,  },\n",
    "                  'G': {'A': 0.,   'C': 0.,   'G': 1.,   'T': 0.,  },\n",
    "                  'T': {'A': 0.,   'C': 0.,   'G': 0.,   'T': 1.,  },\n",
    "                 }\n",
    "\n",
    "\n",
    "subsequences = []\n",
    "\n",
    "for sample_class_label_int in [0,1]:  # for both negative and positive classes\n",
    "    \n",
    "    correct_classification_idxs_for_class = np.where(np.array(test_preds == test_ys) & np.array(test_ys == sample_class_label_int))[0].squeeze()\n",
    "\n",
    "    sort_idxs = np.argsort(test_preds_probs[correct_classification_idxs_for_class, sample_class_label_int])\n",
    "    best_performing_examples = correct_classification_idxs_for_class[sort_idxs]  # one with highest probability performing is at -1\n",
    "\n",
    "#     print(correct_classification_idxs_for_class.shape)\n",
    "#     print(val_pred_probs[correct_classification_idxs_for_class, sample_class_label_int].shape)\n",
    "#     print(best_performing_examples.shape)\n",
    "    \n",
    "    \n",
    "    subsequences_for_class = []\n",
    "    \n",
    "    then = time.time() \n",
    "    for i_sample_idx, sample_idx in enumerate(best_performing_examples):\n",
    "        sequence_idx = sample_idx #best_performing_examples[sample_idx] # one with highest probability performing is at -1\n",
    "        sequence = test_X_seqs[sequence_idx]\n",
    "        label_of_sequence_to_mutate = test_Ys[sequence_idx]\n",
    "\n",
    "        seq_length = 7\n",
    "        \n",
    "        sub_seqs = [sequence[i:i+seq_length] for i in range(len(sequence)-(seq_length-1))] \n",
    "        #orig_seq, mut_seq, all_one_step_changed = mutate_seq_for_saliencymap( sequence_to_mutate, window_sz = 3, mutation_probs = mutation_probs[1], mutation_rate_pct = 0.5, stride = 3)\n",
    "        #rint(len(sequence), len(sub_seqs), sub_seqs[0], sub_seqs[-1],sequence[:10],sequence[-10:])\n",
    "        subsequences_for_class.append(sub_seqs)\n",
    "\n",
    "    subsequences.append(subsequences_for_class)\n",
    "\n",
    "print('Subsequence generation completed')\n",
    "\n",
    "\n",
    "all_subsequences_class_0  = []\n",
    "[all_subsequences_class_0.extend(subs) for subs in subsequences[0] ];\n",
    "#all_unique_subsequences_class_0  = list(set(all_subsequences_class_0) )\n",
    "\n",
    "all_subsequences_class_1  = []\n",
    "[all_subsequences_class_1.extend(subs) for subs in subsequences[1] ];\n",
    "#all_unique_subsequences_class_1  = list(set(all_subsequences_class_1) )\n",
    "\n",
    "print('Subsequence merging completed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PWM Exp 1 : PWM for only unique subsequences appearing in a class, computing score, and taking the median as cut-off"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(all_unique_subsequences_class_0), len(all_subsequences_class_0), len(all_unique_subsequences_class_1), len(all_subsequences_class_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import count\n",
    "from collections import defaultdict\n",
    "\n",
    "c_0 = itertools.count(0)\n",
    "indexer_0 = lambda: next(c_0)\n",
    "class_0_unique_subseq_indxs = []\n",
    "class_0_unique_subseqs_dict = defaultdict(indexer_0)\n",
    "class_0_unique_subseq_indxs = [class_0_unique_subseqs_dict[subseq] for subseq in all_subsequences_class_0]   \n",
    "\n",
    "\n",
    "c_1 = itertools.count(0)\n",
    "indexer_1 = lambda: next(c_1)\n",
    "class_1_unique_subseq_indxs = []\n",
    "class_1_unique_subseqs_dict = defaultdict(indexer_1)\n",
    "class_1_unique_subseq_indxs = [class_1_unique_subseqs_dict[subseq] for subseq in all_subsequences_class_1]   \n",
    "\n",
    "unique_subseqs_class_0 = list(class_0_unique_subseqs_dict.keys())\n",
    "unique_subseqs_class_1 = list(class_1_unique_subseqs_dict.keys())\n",
    "\n",
    "d_class_0 = dict(zip(unique_subseqs_class_0,range(len(unique_subseqs_class_0))))\n",
    "d_class_1 = dict(zip(unique_subseqs_class_1,range(len(unique_subseqs_class_1))))\n",
    "\n",
    "t0 = time.time()\n",
    "#all_subsequences_only_in_class_0 = [subseq for subseq in all_subsequences_class_0 if subseq not in unique_subseqs_class_1]\n",
    "all_subsequences_only_in_class_0 = [subseq for subseq in all_subsequences_class_0 if d_class_1.get(subseq) is None]\n",
    "t1 = time.time()\n",
    "all_subsequences_only_in_class_1 = [subseq for subseq in all_subsequences_class_1 if d_class_0.get(subseq) is None]\n",
    "# all_subsequences_only_in_class_1 = [subseq for subseq in all_subsequences_class_1 if subseq not in unique_subseqs_class_0]\n",
    "t2 = time.time()\n",
    "common_subsequences = [subseq for subseq in all_subsequences_class_1 if not d_class_0.get(subseq) is None]\n",
    "# common_subsequences = [subseq for subseq in all_subsequences_class_1 if subseq in unique_subseqs_class_0]\n",
    "t3 = time.time()\n",
    "\n",
    "print(f\"{t1-t0:0.2f}\")\n",
    "print(f\"{t2-t1:0.2f}\")\n",
    "print(f\"{t3-t2:0.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(all_subsequences_class_0), len(all_subsequences_class_1), len(unique_subseqs_class_0), len(unique_subseqs_class_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(all_subsequences_only_in_class_0), len(all_subsequences_only_in_class_1), len(common_subsequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(list(set(all_subsequences_only_in_class_0))), len(list(set(all_subsequences_only_in_class_1))), len(list(set(common_subsequences)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(list(set(common_subsequences) | set(all_subsequences_only_in_class_0))),  len(list(set(common_subsequences))), len(list(set(all_subsequences_only_in_class_0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_subsequences_per_class__list = []\n",
    "unique_subsequences_per_class__list.append(all_subsequences_only_in_class_0)\n",
    "unique_subsequences_per_class__list.append(all_subsequences_only_in_class_1)\n",
    "\n",
    "\n",
    "params = list(model.parameters())\n",
    "print([param.shape for param in params])\n",
    "\n",
    "# params[0][0]\n",
    "print(unique_DNAs)\n",
    "\n",
    "\n",
    "mean_activations_for_classes = []\n",
    "PWMs_for_classes = []\n",
    "layers_idxs_for_classes = []\n",
    "filters_idxs_for_classes = []\n",
    "\n",
    "for unique_subsequences in unique_subsequences_per_class__list:\n",
    "    \n",
    "    mean_activations = []\n",
    "    PWMs = []\n",
    "    layers_idxs = []\n",
    "    filters_idxs = []\n",
    "\n",
    "    for l in range(0,6,2): # first layer conv filters only\n",
    "        param_layer = params[l]\n",
    "\n",
    "        for i, filter_weights in enumerate(param_layer):\n",
    "\n",
    "            if(filter_weights.shape[1] < 6):\n",
    "                continue\n",
    "            #print(\"Filter shape:\",filter_weights.shape)\n",
    "            filter_length = filter_weights.shape[1]\n",
    "            filter_weights = filter_weights.data.permute(1,0)\n",
    "\n",
    "            motif_sequences = unique_subsequences#get_kmers(unique_DNAs,filter_length)\n",
    "\n",
    "            label_encodings = np.array([unique_DNAs.index(c)+1  for motif_seq in motif_sequences for c in motif_seq ]).reshape(len(motif_sequences),-1)\n",
    "\n",
    "            ohs=[(np.arange(len(unique_DNAs)+1) == label_encoding[:,None]).astype(dtype='float32') for label_encoding in label_encodings]#one_hot\n",
    "\n",
    "            ohs = torch.Tensor(np.array([np.delete(oh,0, axis=-1) for oh in ohs]).reshape(len(ohs),-1, len(unique_DNAs)))\n",
    "            #print(ohs.shape)\n",
    "            #print(filter_weights.shape)\n",
    "            logits = (ohs*filter_weights.cpu())\n",
    "\n",
    "            unique_subseq_activisions = torch.nn.functional.relu(torch.sum(torch.sum(logits,1),1)).detach().squeeze()\n",
    "            #print(unique_subseq_activisions.shape)\n",
    "            passed_sequences_idx = np.where(unique_subseq_activisions > max(unique_subseq_activisions)/2)[0]\n",
    "            if(len(passed_sequences_idx) <=1):\n",
    "                continue\n",
    "            \n",
    "            #print(\"activisions:\",activisions[activisions > max(activisions)/2].shape)\n",
    "            mean_activation = torch.mean(unique_subseq_activisions[unique_subseq_activisions > max(unique_subseq_activisions)/2].squeeze())\n",
    "            mean_activations.append(mean_activation.item())\n",
    "\n",
    "            \n",
    "            passed_sequences_motif = [unique_subsequences[i] for i in passed_sequences_idx.squeeze() ] \n",
    "            passed_sequences_le = np.array([unique_DNAs.index(c)+1  for motif_seq in passed_sequences_motif for c in motif_seq ]).reshape(len(passed_sequences_motif),-1)\n",
    "            \n",
    "            #passed_sequences_motif = [motif_sequences[i] for i in passed_sequences_idx.squeeze()]\n",
    "            #passed_sequences, print(passed_sequences_motif)\n",
    "\n",
    "            arr = np.array(passed_sequences_le).squeeze().transpose(1,0)\n",
    "            probs = [np.sum(arr==i, axis = 1, keepdims=True)/len(passed_sequences_le) for i in np.unique(label_encodings) ]\n",
    "\n",
    "            PWMs.append(np.array(probs).squeeze())\n",
    "\n",
    "            layers_idxs.append(l//2)\n",
    "            filters_idxs.append(i)\n",
    "\n",
    "    mean_activations_for_classes.append(mean_activations)\n",
    "    PWMs_for_classes.append(PWMs)\n",
    "    layers_idxs_for_classes.append(layers_idxs)\n",
    "    filters_idxs_for_classes.append(filters_idxs)\n",
    "\n",
    "\n",
    "\n",
    "#unique_subseqs_class_1_dict = dict(zip(unique_subseqs_class_1, list(range(len(unique_subseqs_class_1))) ) )\n",
    "\n",
    "# len(unique_subseqs_class_0), len(unique_subseqs_class_1)\n",
    "# recons = [unique_subseqs_class_1[i] for i in class_1_unique_subseq_indxs]\n",
    "# len(recons), len(all_subsequences_class_1)\n",
    "# comps = [recons[i]==all_subsequences_class_1[i] for i in range(len(all_subsequences_class_1))]\n",
    "# np.all(comps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PWM Exp 2: PWM for all subsequences appearing in a class (including duplicates), computing score for all, and taking the median as cut-off"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_subsequences_for_classes = []\n",
    "all_subsequences_for_classes.append(all_subsequences_class_0)\n",
    "all_subsequences_for_classes.append(all_subsequences_class_1)\n",
    "\n",
    "\n",
    "unique_subsequences_for_classes = []\n",
    "unique_subsequences_for_classes.append(unique_subseqs_class_0)\n",
    "unique_subsequences_for_classes.append(unique_subseqs_class_1)\n",
    "\n",
    "unique_subsequences_idxs_for_classes = []\n",
    "unique_subsequences_idxs_for_classes.append(class_0_unique_subseq_indxs)\n",
    "unique_subsequences_idxs_for_classes.append(class_1_unique_subseq_indxs)\n",
    "\n",
    "\n",
    "params = list(model.parameters())\n",
    "print([param.shape for param in params])\n",
    "\n",
    "# params[0][0]\n",
    "print(unique_DNAs)\n",
    "\n",
    "\n",
    "mean_activations_for_classes = []\n",
    "PWMs_for_classes = []\n",
    "layers_idxs_for_classes = []\n",
    "filters_idxs_for_classes = []\n",
    "\n",
    "for unique_subsequences, unique_subsequences_idx, all_subsequences in zip(unique_subsequences_for_classes,unique_subsequences_idxs_for_classes, all_subsequences_for_classes):\n",
    "    \n",
    "    mean_activations = []\n",
    "    PWMs = []\n",
    "    layers_idxs = []\n",
    "    filters_idxs = []\n",
    "\n",
    "    unique_subsequences_idx = np.array(unique_subsequences_idx)\n",
    "    for l in range(0,6,2): # first layer conv filters only\n",
    "        param_layer = params[l]\n",
    "\n",
    "        for i, filter_weights in enumerate(param_layer):\n",
    "\n",
    "            if(filter_weights.shape[1] < 6):\n",
    "                continue\n",
    "            #print(\"Filter shape:\",filter_weights.shape)\n",
    "            filter_length = filter_weights.shape[1]\n",
    "            filter_weights = filter_weights.data.permute(1,0)\n",
    "\n",
    "            motif_sequences = unique_subsequences#get_kmers(unique_DNAs,filter_length)\n",
    "\n",
    "            label_encodings = np.array([unique_DNAs.index(c)+1  for motif_seq in motif_sequences for c in motif_seq ]).reshape(len(motif_sequences),-1)\n",
    "\n",
    "            ohs=[(np.arange(len(unique_DNAs)+1) == label_encoding[:,None]).astype(dtype='float32') for label_encoding in label_encodings]#one_hot\n",
    "\n",
    "            ohs = torch.Tensor(np.array([np.delete(oh,0, axis=-1) for oh in ohs]).reshape(len(ohs),-1, len(unique_DNAs)))\n",
    "            #print(ohs.shape)\n",
    "            #print(filter_weights.shape)\n",
    "            logits = (ohs*filter_weights.cpu())\n",
    "\n",
    "            unique_subseq_activisions = torch.nn.functional.relu(torch.sum(torch.sum(logits,1),1))\n",
    "            \n",
    "            all_subseq_activations = np.array([unique_subseq_activisions[i].detach() for i in unique_subsequences_idx])\n",
    "            \n",
    "            passed_sequences_idx = np.where(all_subseq_activations > max(all_subseq_activations)/2)[0].squeeze()\n",
    "            if(len(passed_sequences_idx) <=1):\n",
    "                continue\n",
    "            \n",
    "            #print(\"activisions:\",activisions[activisions > max(activisions)/2].shape)\n",
    "            mean_activation = np.mean(all_subseq_activations[all_subseq_activations > max(all_subseq_activations)/2].squeeze())\n",
    "            mean_activations.append(mean_activation.item())\n",
    "\n",
    "            \n",
    "            passed_sequences_motif = [all_subsequences[i] for i in passed_sequences_idx.squeeze() ] \n",
    "            passed_sequences_le = np.array([unique_DNAs.index(c)+1  for motif_seq in passed_sequences_motif for c in motif_seq ]).reshape(len(passed_sequences_motif),-1)\n",
    "            \n",
    "            \n",
    "            [label_encodings[i,:] for i in unique_subsequences_idx[passed_sequences_idx.squeeze() ] ]\n",
    "            #passed_sequences_motif = [motif_sequences[i] for i in passed_sequences_idx.squeeze()]\n",
    "            #passed_sequences, print(passed_sequences_motif)\n",
    "\n",
    "            arr = np.array(passed_sequences_le).squeeze().transpose(1,0)\n",
    "            probs = [np.sum(arr==i, axis = 1, keepdims=True)/len(passed_sequences_le) for i in np.unique(label_encodings) ]\n",
    "\n",
    "            PWMs.append(np.array(probs).squeeze())\n",
    "\n",
    "            layers_idxs.append(l//2)\n",
    "            filters_idxs.append(i)\n",
    "\n",
    "    mean_activations_for_classes.append(mean_activations)\n",
    "    PWMs_for_classes.append(PWMs)\n",
    "    layers_idxs_for_classes.append(layers_idxs)\n",
    "    filters_idxs_for_classes.append(filters_idxs)\n",
    "\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PWM visualization for PWM Exp 1 and Exp 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(precision=3)\n",
    "for j in range(len(mean_activations_for_classes)):\n",
    "    \n",
    "    mean_activations = mean_activations_for_classes[j]\n",
    "    PWMs = PWMs_for_classes[j]\n",
    "    layers_idxs = layers_idxs_for_classes[j]\n",
    "    filters_idxs = filters_idxs_for_classes[j]\n",
    "\n",
    "    \n",
    "    sorted_idxs = np.argsort(mean_activations)\n",
    "\n",
    "    for i in sorted_idxs:\n",
    "        mean_activation = mean_activations[i]\n",
    "        PWM = PWMs[i]\n",
    "        layer_idx = layers_idxs[i]\n",
    "        filter_idx = filters_idxs[i]\n",
    "\n",
    "        print(f\"Mean activation:{mean_activation:.3}\")\n",
    "        print(f\"Layer {layer_idx}, filter#{filter_idx}\\n\")\n",
    "        print(f\"{repr(PWM)}\")\n",
    "        print()\n",
    "        plt.figure(figsize=(12,2))\n",
    "        ax = plt.imshow(PWM, cmap = plt.get_cmap('Blues') )\n",
    "\n",
    "        plt.grid(False)\n",
    "\n",
    "        gridlinewidth = 1.6\n",
    "\n",
    "        plt.title(str(j))\n",
    "        plt.show()\n",
    "        \n",
    "        print(\"----------------------------------------------------------------------\")\n",
    "        \n",
    "    print(\"+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\")\n",
    "    print(sorted_idxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PWM Exp 3: PWM for all subsequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Motif (PWM) generation from Conv1 layers n_dictxL filter weights*all L-length subsequences, keeping the ones that leads to activations greater than half of the maximum activation, and computing the probability # of each nucleotide at each of L position of the subsequence. Only consider motifs of length 6 or greater"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = list(model.parameters())\n",
    "print([param.shape for param in params])\n",
    "\n",
    "# params[0][0]\n",
    "print(unique_DNAs)\n",
    "\n",
    "mean_activations = []\n",
    "PWMs = []\n",
    "layers_idxs = []\n",
    "filters_idxs = []\n",
    "\n",
    "for l in range(0,6,2): # first layer conv filters only\n",
    "    param_layer = params[l]\n",
    "\n",
    "    for i, filter_weights in enumerate(param_layer):\n",
    "\n",
    "        if(filter_weights.shape[1] < 6):\n",
    "            continue\n",
    "        #print(\"Filter shape:\",filter_weights.shape)\n",
    "        filter_length = filter_weights.shape[1]\n",
    "        filter_weights = filter_weights.data.permute(1,0)\n",
    "\n",
    "        motif_sequences = get_kmers(unique_DNAs,filter_length)\n",
    "\n",
    "        label_encodings = np.array([unique_DNAs.index(c)+1  for motif_seq in motif_sequences for c in motif_seq ]).reshape(len(motif_sequences),-1)\n",
    "\n",
    "        ohs=[(np.arange(len(unique_DNAs)+1) == label_encoding[:,None]).astype(dtype='float32') for label_encoding in label_encodings]#one_hot\n",
    "\n",
    "        ohs = torch.Tensor(np.array([np.delete(oh,0, axis=-1) for oh in ohs]).reshape(len(ohs),-1, len(unique_DNAs)))\n",
    "        #print(ohs.shape)\n",
    "        #print(filter_weights.shape)\n",
    "        vals = (ohs*filter_weights.cpu())\n",
    "\n",
    "        activisions = torch.nn.functional.relu(torch.sum(torch.sum(vals,1),1))\n",
    "        #activisions[np.where(activisions > max(activisions)/2)], max(activisions)/2\n",
    "        \n",
    "        #print(\"activisions:\",activisions[activisions > max(activisions)/2].shape)\n",
    "        mean_activation = torch.mean(activisions[activisions > max(activisions)/2].squeeze())\n",
    "        \n",
    "        passed_sequences_idx = np.where(activisions > max(activisions)/2)[0]\n",
    "        if(len(passed_sequences_idx) <=1):\n",
    "            continue\n",
    "        mean_activations.append(mean_activation.item())\n",
    "        \n",
    "        passed_sequences = [label_encodings[i] for i in passed_sequences_idx.squeeze()]\n",
    "        passed_sequences_motif = [motif_sequences[i] for i in passed_sequences_idx.squeeze()]\n",
    "\n",
    "        #passed_sequences, print(passed_sequences_motif)\n",
    "\n",
    "        arr = np.array(passed_sequences).squeeze().transpose(1,0)\n",
    "        probs = [np.sum(arr==i, axis = 1, keepdims=True)/len(passed_sequences) for i in np.unique(label_encodings) ]\n",
    "        \n",
    "        PWMs.append(np.array(probs).squeeze())\n",
    "        \n",
    "        layers_idxs.append(l//2)\n",
    "        filters_idxs.append(i)\n",
    "        \n",
    "#         print(\"Mean activation:\",mean_activation.item())\n",
    "#         print(f\"Layer {l//2}, filter#{i}\\n\",np.array(probs).squeeze())\n",
    "#         print()\n",
    "#         ax = plt.imshow(np.array(probs).squeeze(), cmap = plt.get_cmap('Blues') )\n",
    "        \n",
    "#         plt.grid(False)\n",
    "        \n",
    "#         #plt.grid(True, color='w', linestyle='-', linewidth=2, which = 'both')\n",
    "#         #loc = ticker.MultipleLocator(base=2)\n",
    "#         #ax.xaxis.set_minor_locator(loc)\n",
    "#         #ax.yaxis.set_minor_locator(loc)\n",
    "\n",
    "#         #plt.xticks(np.arange(0,filter_weights.shape[0])+0.5, np.arange(1,filter_weights.shape[0] + 1), fontsize=20)\n",
    "#         #plt.yticks(np.arange(0,filter_weights.shape[1])+0.5, unique_DNAs, fontsize=20)\n",
    "#         #ax.set_yticklabels(ax.get_yticklabels(), rotation=0)x sssssssssdem  n\n",
    "#         #plt.yticks(rotation=0)\n",
    "#         gridlinewidth = 1.6\n",
    "#         #ax.hlines(np.arange(len(unique_DNAs)+1), *ax.get_xlim(), color='r', linewidth=gridlinewidth)\n",
    "#         #ax.vlines(np.arange(0,filter_weights.shape[0]+1), *ax.get_ylim(), color='w', linewidth=gridlinewidth)\n",
    "#         plt.show()\n",
    "#         print(\"----------------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_idxs = np.argsort(mean_activations)\n",
    "\n",
    "for i in sorted_idxs:\n",
    "    mean_activation = mean_activations[i]\n",
    "    PWM = PWMs[i]\n",
    "    layer_idx = layers_idxs[i]\n",
    "    filter_idx = filters_idxs[i]\n",
    "    \n",
    "    print(\"Mean activation:\",mean_activation)\n",
    "    print(f\"Layer {layer_idx}, filter#{filter_idx}\\n\",PWM)\n",
    "    print()\n",
    "    ax = plt.imshow(PWM, cmap = plt.get_cmap('Blues') )\n",
    "\n",
    "    plt.grid(False)\n",
    "\n",
    "    #plt.grid(True, color='w', linestyle='-', linewidth=2, which = 'both')\n",
    "    #loc = ticker.MultipleLocator(base=2)\n",
    "    #ax.xaxis.set_minor_locator(loc)\n",
    "    #ax.yaxis.set_minor_locator(loc)\n",
    "\n",
    "    #plt.xticks(np.arange(0,filter_weights.shape[0])+0.5, np.arange(1,filter_weights.shape[0] + 1), fontsize=20)\n",
    "    #plt.yticks(np.arange(0,filter_weights.shape[1])+0.5, unique_DNAs, fontsize=20)\n",
    "    #ax.set_yticklabels(ax.get_yticklabels(), rotation=0)x sssssssssdem  n\n",
    "    #plt.yticks(rotation=0)\n",
    "    gridlinewidth = 1.6\n",
    "    #ax.hlines(np.arange(len(unique_DNAs)+1), *ax.get_xlim(), color='r', linewidth=gridlinewidth)\n",
    "    #ax.vlines(np.arange(0,filter_weights.shape[0]+1), *ax.get_ylim(), color='w', linewidth=gridlinewidth)\n",
    "    plt.show()\n",
    "    print(\"----------------------------------------------------------------------\")\n",
    "\n",
    "\n",
    "\n",
    "# params = list(model.parameters())\n",
    "# print([param.shape for param in params])\n",
    "\n",
    "# # params[0][0]\n",
    "# print(unique_DNAs)\n",
    "\n",
    "\n",
    "\n",
    "# for l in range(0,6,2): # first layer conv filters only\n",
    "#     param_layer = params[l]\n",
    "\n",
    "#     for i, filter_weights in enumerate(param_layer):\n",
    "\n",
    "#         if(filter_weights.shape[1] < 6):\n",
    "#             continue\n",
    "#         print(\"Filter shape:\",filter_weights.shape)\n",
    "#         filter_length = filter_weights.shape[1]\n",
    "#         filter_weights = filter_weights.data.permute(1,0)\n",
    "\n",
    "#         motif_sequences = get_kmers(unique_DNAs,filter_length)\n",
    "\n",
    "#         label_encodings = np.array([unique_DNAs.index(c)+1  for motif_seq in motif_sequences for c in motif_seq ]).reshape(len(motif_sequences),-1)\n",
    "\n",
    "#         ohs=[(np.arange(len(unique_DNAs)+1) == label_encoding[:,None]).astype(dtype='float32') for label_encoding in label_encodings]#one_hot\n",
    "\n",
    "#         ohs = torch.Tensor(np.array([np.delete(oh,0, axis=-1) for oh in ohs]).reshape(len(ohs),-1, len(unique_DNAs)))\n",
    "#         #print(ohs.shape)\n",
    "#         #print(filter_weights.shape)\n",
    "#         vals = (ohs*filter_weights.cpu())\n",
    "\n",
    "\n",
    "#         activisions = torch.nn.functional.relu(torch.sum(torch.sum(vals,1),1))\n",
    "\n",
    "#         #activisions[np.where(activisions > max(activisions)/2)], max(activisions)/2\n",
    "        \n",
    "#         passed_sequences_idx = np.where(activisions > max(activisions)/2)[0]\n",
    "#         if(len(passed_sequences_idx) ==0):\n",
    "#             continue\n",
    "#         passed_sequences = [label_encodings[i] for i in passed_sequences_idx.squeeze()]\n",
    "#         passed_sequences_motif = [motif_sequences[i] for i in passed_sequences_idx.squeeze()]\n",
    "\n",
    "#         #passed_sequences, print(passed_sequences_motif)\n",
    "\n",
    "#         arr = np.array(passed_sequences).squeeze().transpose(1,0)\n",
    "#         probs = [np.sum(arr==i, axis = 1, keepdims=True)/len(passed_sequences) for i in np.unique(label_encodings) ]\n",
    "#         print(f\"Layer {l//2}, filter#{i}\\n\",np.array(probs).squeeze())\n",
    "#         print()\n",
    "#         plt.imshow(np.array(probs).squeeze(), cmap = plt.get_cmap('jet') )\n",
    "#         plt.grid('off')\n",
    "        \n",
    "#         plt.show()\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
