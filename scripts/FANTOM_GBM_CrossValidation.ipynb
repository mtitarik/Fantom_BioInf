{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import requests\n",
    "from astropy.units import one\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "import itertools\n",
    "import random\n",
    "import math\n",
    "from sklearn.metrics import classification_report, confusion_matrix, precision_score, recall_score, f1_score, cohen_kappa_score, \\\n",
    "                            precision_recall_curve, average_precision_score, roc_auc_score, roc_curve, auc, accuracy_score\n",
    "\n",
    "from IPython.display import clear_output\n",
    "\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "import pandas as pd\n",
    "# the distribution of sequence length\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "# from torch.nn.utils.rnn import pad_sequence\n",
    "# from torch import Tensor\n",
    "\n",
    "# import torch\n",
    "# import torch.nn.functional as F\n",
    "# import torch.nn as nn\n",
    "# import torch.utils.data as data_utils\n",
    "# import torch.optim.lr_scheduler as lr_scheduler \n",
    "# import matplotlib.pyplot as plt\n",
    "# import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import lightgbm as lgb\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# sklearn tools for model training and assesment\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import PredefinedSplit\n",
    "from sklearn.model_selection import GridSearchCV, ParameterGrid\n",
    "from sklearn.metrics import (roc_curve, auc, accuracy_score, balanced_accuracy_score, matthews_corrcoef)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_dir = '../../../'\n",
    "\n",
    "#ds_dir = f'{project_dir}data/seq/'\n",
    "ds_dir = f'{project_dir}data/processed/'\n",
    "\n",
    "snapshot_dir = f'{project_dir}snapshots/'\n",
    "\n",
    "classes = ['lnc_e','lnc_p']\n",
    "class_suffix = '_noN'\n",
    "\n",
    "neg_filename = f'{ds_dir}train_val_test__80_20/{classes[0]}{class_suffix}_tr_val.csv'\n",
    "pos_filename = f'{ds_dir}train_val_test__80_20/{classes[1]}{class_suffix}_tr_val.csv'\n",
    "\n",
    "test_neg_filename = f'{ds_dir}train_val_test__80_20/{classes[0]}{class_suffix}_test.csv'\n",
    "test_pos_filename = f'{ds_dir}train_val_test__80_20/{classes[1]}{class_suffix}_test.csv'\n",
    "\n",
    "\n",
    "# feature_filenames_class_0 = [f'{ds_dir}train_val_test/{classes[0]}{class_suffix}_1mer_features_tr_val.csv',\n",
    "#                              f'{ds_dir}train_val_test/{classes[0]}{class_suffix}_2mer_features_tr_val.csv',\n",
    "#                              f'{ds_dir}train_val_test/{classes[0]}{class_suffix}_3mer_features_tr_val.csv']\n",
    "# feature_filenames_class_1 = [f'{ds_dir}train_val_test/{classes[1]}{class_suffix}_1mer_features_tr_val.csv',\n",
    "#                              f'{ds_dir}train_val_test/{classes[1]}{class_suffix}_2mer_features_tr_val.csv',\n",
    "#                              f'{ds_dir}train_val_test/{classes[1]}{class_suffix}_3mer_features_tr_val.csv']\n",
    "\n",
    "\n",
    "# test_feature_filenames_class_0 = [f'{ds_dir}train_val_test/{classes[0]}{class_suffix}_1mer_features_test.csv',\n",
    "#                                   f'{ds_dir}train_val_test/{classes[0]}{class_suffix}_2mer_features_test.csv',\n",
    "#                                   f'{ds_dir}train_val_test/{classes[0]}{class_suffix}_3mer_features_test.csv']\n",
    "# test_feature_filenames_class_1 = [f'{ds_dir}train_val_test/{classes[1]}{class_suffix}_1mer_features_test.csv',\n",
    "#                                   f'{ds_dir}train_val_test/{classes[1]}{class_suffix}_2mer_features_test.csv',\n",
    "#                                   f'{ds_dir}train_val_test/{classes[1]}{class_suffix}_3mer_features_test.csv']\n",
    "\n",
    "\n",
    "bio_feature_filenames_class_0 = [f'{ds_dir}train_val_test__80_20/{classes[0]}_nosim.fa.matrix_tr_val.csv']\n",
    "bio_feature_filenames_class_1 = [f'{ds_dir}train_val_test__80_20/{classes[1]}_nosim.fa.matrix_tr_val.csv']\n",
    "\n",
    "\n",
    "test_bio_feature_filenames_class_0 = [f'{ds_dir}train_val_test__80_20/{classes[0]}_nosim.fa.matrix_test.csv']\n",
    "test_bio_feature_filenames_class_1 = [f'{ds_dir}train_val_test__80_20/{classes[1]}_nosim.fa.matrix_test.csv']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2312\n",
      "5462\n"
     ]
    }
   ],
   "source": [
    "# load positive and negative sequence data to construct ground label list\n",
    "\n",
    "with open(neg_filename,'r') as nf:\n",
    "    neg_examples = [seq.strip() for seq in nf.readlines()]\n",
    "    num_neg = len(neg_examples) # number of negative examples\n",
    "    print(num_neg)\n",
    "\n",
    "with open(pos_filename,'r') as pf:\n",
    "    pos_examples = [seq.strip() for seq in pf.readlines()]\n",
    "    num_pos = len(pos_examples) # number of positive examples\n",
    "    print(num_pos)    \n",
    "\n",
    "sequences_DS = neg_examples + pos_examples \n",
    "labels_DS = np.array([0]*num_neg + [1]*num_pos)   \n",
    "len(sequences_DS), len(labels_DS)\n",
    "\n",
    "scale_pos_weight = num_neg/num_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2312, 292) (5462, 292) (7774, 292)\n"
     ]
    }
   ],
   "source": [
    "# load training + validation data\n",
    "\n",
    "bio_features_class0 = np.concatenate([pd.read_csv(bio_feature_filename, header=None, index_col=False).to_numpy() for bio_feature_filename in bio_feature_filenames_class_0], axis = 1)\n",
    "bio_features_class1 = np.concatenate([pd.read_csv(bio_feature_filename, header=None, index_col=False).to_numpy() for bio_feature_filename in bio_feature_filenames_class_1], axis = 1)\n",
    "\n",
    "neg_bio_features = bio_features_class0\n",
    "pos_bio_features = bio_features_class1\n",
    "\n",
    "bio_features_DS = np.concatenate([bio_features_class0, bio_features_class1], axis = 0)    \n",
    "print(neg_bio_features.shape, pos_bio_features.shape,bio_features_DS.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(579, 292) (1366, 292) (1945, 292) 1945\n"
     ]
    }
   ],
   "source": [
    "# load test data\n",
    "\n",
    "test_bio_features_class0 = np.concatenate([pd.read_csv(bio_feature_filename, header=None, index_col=False).to_numpy() for bio_feature_filename in test_bio_feature_filenames_class_0], axis = 1)\n",
    "test_bio_features_class1 = np.concatenate([pd.read_csv(bio_feature_filename, header=None, index_col=False).to_numpy() for bio_feature_filename in test_bio_feature_filenames_class_1], axis = 1)\n",
    "\n",
    "test_neg_bio_features = test_bio_features_class0\n",
    "test_pos_bio_features = test_bio_features_class1\n",
    "\n",
    "test_bio_features_DS = np.concatenate([test_bio_features_class0, test_bio_features_class1], axis = 0)    \n",
    "\n",
    "test_labels_DS = np.array([0]*test_bio_features_class0.shape[0] + [1]*test_bio_features_class1.shape[0])  \n",
    "print(test_neg_bio_features.shape, test_pos_bio_features.shape,test_bio_features_DS.shape, len(test_labels_DS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lgb_mcc_score(y_hat, data):\n",
    "    y_true = data.get_label()\n",
    "    y_pred = np.round(y_hat) # scikits f1 doesn't like probabilities\n",
    "    return 'mcc', matthews_corrcoef(y_true, y_pred), True\n",
    "\n",
    "def lgb_f1_score(y_hat, data):\n",
    "    y_true = data.get_label()\n",
    "    y_hat = np.round(y_hat) # scikits f1 doesn't like probabilities\n",
    "    return 'f1', f1_score(y_true, y_hat), True\n",
    "\n",
    "\n",
    "def compute_sens_spec_mcc(y_true, y_pred):\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    #cm = cm.astype('float') / cm.sum(axis = 1)[:, np.newaxis]\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "\n",
    "    specificity = tn/(fp+tn)\n",
    "    sensitivity = tp/(tp+fn)\n",
    "    \n",
    "    return {'specificity':specificity, 'sensitivity': sensitivity, 'mcc':matthews_corrcoef(y_true, y_pred)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: (6219, 292) 6219\n",
      "val: (1555, 292) 1555\n",
      "test: (1945, 292) 1945\n",
      "train+val: (7774, 292) 7774\n"
     ]
    }
   ],
   "source": [
    "# randomly shuffle the dataset (required as all negative class examples come first in the original)\n",
    "\n",
    "indx = list(range(len(labels_DS)))\n",
    "np.random.shuffle(indx)\n",
    "labels_DS_np = np.array(labels_DS)\n",
    "labels_shuf = labels_DS_np[indx]\n",
    "bio_features_shuf = bio_features_DS[indx,:]\n",
    "\n",
    "\n",
    "train_frac = 0.8\n",
    "val_frac = 1 - train_frac\n",
    "\n",
    "train_count = int(len(indx)*train_frac)\n",
    "val_count = len(indx) - train_count\n",
    "\n",
    "train_features = bio_features_shuf[:train_count,:]\n",
    "train_labels = labels_shuf[:train_count]\n",
    "\n",
    "val_features = bio_features_shuf[train_count:,:]\n",
    "val_labels = labels_shuf[train_count:]\n",
    "\n",
    "test_features = test_bio_features_DS\n",
    "test_labels = test_labels_DS\n",
    "\n",
    "\n",
    "train_dataset = lgb.Dataset(train_features, label=train_labels)\n",
    "valid_dataset = lgb.Dataset(val_features, label=val_labels)\n",
    "test_dataset = lgb.Dataset(test_features, label=test_labels)\n",
    "\n",
    "train_val_dataset = lgb.Dataset(bio_features_DS, label=labels_DS_np)\n",
    "\n",
    "print(\"train:\", train_features.shape, len(train_labels))\n",
    "print(\"val:\", val_features.shape, len(val_labels))\n",
    "print(\"test:\", test_features.shape, len(test_labels))\n",
    "\n",
    "print(\"train+val:\", bio_features_DS.shape, len(labels_DS_np))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grid search to find the best parameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 48 candidates, totalling 480 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  17 tasks      | elapsed:    5.8s\n",
      "/home/tarik/anaconda3/envs/project_selfie/lib/python3.7/site-packages/joblib/externals/loky/process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  \"timeout or by a memory leak.\", UserWarning\n",
      "[Parallel(n_jobs=-1)]: Done 138 tasks      | elapsed:   42.7s\n",
      "[Parallel(n_jobs=-1)]: Done 341 tasks      | elapsed:  1.6min\n",
      "[Parallel(n_jobs=-1)]: Done 480 out of 480 | elapsed:  2.2min finished\n",
      "/home/tarik/anaconda3/envs/project_selfie/lib/python3.7/site-packages/lightgbm/engine.py:118: UserWarning: Found `num_boost_round` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters found by grid search are: {'boosting_type': 'gbdt', 'learning_rate': 0.1, 'num_boost_round': 200, 'num_leaves': 80, 'objective': 'binary'}\n",
      "Best score found by grid search is: 0.855544121430409\n"
     ]
    }
   ],
   "source": [
    "# specify your configurations as a dict\n",
    "params = {\n",
    "    'task': 'train',\n",
    "    'boosting_type': 'gbdt',\n",
    "    'objective': 'binary',\n",
    "    'metric': {'binary_logloss', 'auc'},\n",
    "    'metric_freq': 1,\n",
    "    'is_training_metric': True,\n",
    "    'max_bin': 255,\n",
    "    'learning_rate': 0.1,\n",
    "    'num_leaves': 63,\n",
    "    'tree_learner': 'serial',\n",
    "    'feature_fraction': 0.8,\n",
    "    'bagging_fraction': 0.8,\n",
    "    'bagging_freq': 5,\n",
    "    'min_data_in_leaf': 50,\n",
    "    'min_sum_hessian_in_leaf': 5,\n",
    "    'is_enable_sparse': True,\n",
    "    'use_two_round_loading': False,\n",
    "    'is_save_binary_file': False,\n",
    "    'output_model': 'LightGBM_model.txt',\n",
    "    'num_machines': 1,\n",
    "    'local_listen_port': 12400,\n",
    "    'machine_list_file': 'mlist.txt',\n",
    "    'verbose': 0,\n",
    "    'subsample_for_bin': 200000,\n",
    "    'min_child_samples': 20,\n",
    "    'min_child_weight': 0.001,\n",
    "    'min_split_gain': 0.0,\n",
    "    'colsample_bytree': 1.0,\n",
    "    'reg_alpha': 0.0,\n",
    "    'reg_lambda': 0.0\n",
    "}\n",
    "\n",
    "gridParams = {\n",
    "    'learning_rate': [ 0.05, 0.1, 0.2, 0.3],\n",
    "    'num_leaves': [80, 100, 150, 200],\n",
    "    'num_boost_round': [100, 150, 200],\n",
    "    'boosting_type' : ['gbdt'],\n",
    "    'objective' : ['binary']\n",
    "}\n",
    "\n",
    "mdl = lgb.LGBMClassifier(\n",
    "    task = params['task'],\n",
    "    metric = 'matthews_corrcoef',\n",
    "    metric_freq = params['metric_freq'],\n",
    "    scale_pos_weight = scale_pos_weight,\n",
    "    is_training_metric = params['is_training_metric'],\n",
    "    max_bin = params['max_bin'],\n",
    "    tree_learner = params['tree_learner'],\n",
    "    feature_fraction = params['feature_fraction'],\n",
    "    bagging_fraction = params['bagging_fraction'],\n",
    "    bagging_freq = params['bagging_freq'],\n",
    "    min_data_in_leaf = params['min_data_in_leaf'],\n",
    "    min_sum_hessian_in_leaf = params['min_sum_hessian_in_leaf'],\n",
    "    is_enable_sparse = params['is_enable_sparse'],\n",
    "    use_two_round_loading = params['use_two_round_loading'],\n",
    "    is_save_binary_file = params['is_save_binary_file'],\n",
    "    n_jobs = -1\n",
    ")\n",
    "\n",
    "scoring = {'mcc': 'matthews_corrcoef'}\n",
    "\n",
    "# Create the grid\n",
    "grid = GridSearchCV(mdl, gridParams, verbose=2, cv=10, n_jobs=-1, refit='mcc' )\n",
    "# Run the grid\n",
    "grid.fit(bio_features_DS, labels_DS )\n",
    "\n",
    "print('Best parameters found by grid search are:', grid.best_params_)\n",
    "print('Best score found by grid search is:', grid.best_score_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-fold cross validation using selected parameters  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tarik/anaconda3/envs/project_selfie/lib/python3.7/site-packages/lightgbm/engine.py:430: UserWarning: Found `num_boost_round` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/home/tarik/anaconda3/envs/project_selfie/lib/python3.7/site-packages/sklearn/metrics/classification.py:872: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  mcc = cov_ytyp / np.sqrt(cov_ytyt * cov_ypyp)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6667882628766504 0.022779498712166772\n"
     ]
    }
   ],
   "source": [
    "# k-fold cross validation (built-in) on training + validation set (using selected params)\n",
    "#num_round = 31\n",
    "#param_nogrid = {'num_leaves': 100, 'num_boost_round': 100, 'objective': 'binary' }\n",
    "# using best parameters from grid search result\n",
    "param_nogrid = {'boosting_type': 'gbdt', 'learning_rate': 0.2, 'num_boost_round': 200, 'num_leaves': 80, 'objective': 'binary', 'scale_pos_weight':scale_pos_weight } \n",
    "\n",
    "\n",
    "param_nogrid['metric'] = 'mcc'\n",
    "\n",
    "evals_result = {}\n",
    "\n",
    "results = lgb.cv(param_nogrid, train_dataset, nfold= 10, feval = lgb_mcc_score, verbose_eval=0)\n",
    "print(results['mcc-mean'][-1], results['mcc-stdv'][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tarik/anaconda3/envs/project_selfie/lib/python3.7/site-packages/lightgbm/engine.py:430: UserWarning: Found `num_boost_round` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/home/tarik/anaconda3/envs/project_selfie/lib/python3.7/site-packages/sklearn/metrics/classification.py:872: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  mcc = cov_ytyp / np.sqrt(cov_ytyt * cov_ypyp)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6802998642286617 0.03640292187559192\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tarik/anaconda3/envs/project_selfie/lib/python3.7/site-packages/lightgbm/engine.py:118: UserWarning: Found `num_boost_round` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/home/tarik/anaconda3/envs/project_selfie/lib/python3.7/site-packages/sklearn/metrics/classification.py:872: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  mcc = cov_ytyp / np.sqrt(cov_ytyt * cov_ypyp)\n"
     ]
    }
   ],
   "source": [
    "# k-fold cross validation on training + validation set (using selected params)\n",
    "\n",
    "skf = StratifiedKFold( n_splits= 10, random_state = 23, shuffle=True)\n",
    "\n",
    "results = []\n",
    "\n",
    "models = []\n",
    "for train_index, val_index in skf.split(bio_features_DS, labels_DS_np):\n",
    "    train_dataset_kfold = lgb.Dataset(bio_features_DS[train_index,:], label=labels_DS_np[train_index])\n",
    "    valid_dataset_kfold = lgb.Dataset(bio_features_DS[val_index,:], label=labels_DS_np[val_index])\n",
    "    # train\n",
    "    gbm = lgb.train(param_nogrid, train_dataset_kfold, feval = lgb_mcc_score, early_stopping_rounds = 5, valid_sets = valid_dataset_kfold, verbose_eval=0)\n",
    "    \n",
    "    models.append(gbm)\n",
    "    results.append(compute_sens_spec_mcc(labels_DS_np[val_index], np.round(gbm.predict(bio_features_DS[val_index,:]))))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross validation results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "specificity: mean: 0.8382669055082849 standard deviation: 0.05960943143708073\n",
      "sensitivity: mean: 0.8315018315018315 standard deviation: 0.0\n",
      "mcc: mean: 0.6539740102929884 standard deviation: 0.021922523956461035\n"
     ]
    }
   ],
   "source": [
    "specs = []\n",
    "senss = []\n",
    "mccs = []\n",
    "\n",
    "for result in results:\n",
    "    spec =  result['specificity']\n",
    "    sens = result['sensitivity']\n",
    "    mcc = result['mcc']\n",
    "\n",
    "    specs.append(spec)\n",
    "    senss.append(sens)\n",
    "    mccs.append(mcc)\n",
    "#print(matthews_corrcoef(labels_DS[indx[train_count:]], np.round(gbm.predict(bio_features_DS[indx[train_count:],:]))))  # validation data \n",
    "#print(matthews_corrcoef(labels_DS, gbm.predict(train_data)))\n",
    "\n",
    "\n",
    "print(\"specificity:\",\"mean:\",np.mean(specs), \"standard deviation:\", np.std(specs) )\n",
    "print(\"sensitivity:\",\"mean:\",np.mean(sens), \"standard deviation:\", np.std(sens) )\n",
    "print(\"mcc:\",\"mean:\",np.mean(mccs), \"standard deviation:\", np.std(mccs) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "specificity: mean: 0.854675803124079 standard deviation: 0.02806946278720652\n",
      "sensitivity: mean: 0.8452768729641694 standard deviation: 0.0\n",
      "mcc: mean: 0.6565167252726619 standard deviation: 0.018990340559675023\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model_idx = np.argmax(mccs)\n",
    "gbm = models[best_model_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test using the best model (with highest metric value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'specificity': 0.8482758620689655,\n",
       " 'sensitivity': 0.8243045387994143,\n",
       " 'mcc': 0.6350657193454664}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_sens_spec_mcc(test_labels, np.round(gbm.predict(test_features)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test by retraining the model with best parameters, using the entire Train + Val set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'specificity': 0.803448275862069,\n",
       " 'sensitivity': 0.8799414348462665,\n",
       " 'mcc': 0.6680509143620029}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gbm = lgb.train(param_nogrid, train_val_dataset, feval = lgb_mcc_score, verbose_eval=0)\n",
    "compute_sens_spec_mcc(test_labels, np.round(gbm.predict(test_features)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # specify your configurations as a dict\n",
    "# params = {\n",
    "#     'task': 'train',\n",
    "#     'boosting_type': 'gbdt',\n",
    "#     'objective': 'binary',\n",
    "#     'metric': {'binary_logloss', 'auc'},\n",
    "#     'metric_freq': 1,\n",
    "#     'is_training_metric': True,\n",
    "#     'max_bin': 255,\n",
    "#     'learning_rate': 0.1,\n",
    "#     'num_leaves': 63,\n",
    "#     'tree_learner': 'serial',\n",
    "#     'feature_fraction': 0.8,\n",
    "#     'bagging_fraction': 0.8,\n",
    "#     'bagging_freq': 5,\n",
    "#     'min_data_in_leaf': 50,\n",
    "#     'min_sum_hessian_in_leaf': 5,\n",
    "#     'is_enable_sparse': True,\n",
    "#     'use_two_round_loading': False,\n",
    "#     'is_save_binary_file': False,\n",
    "#     'output_model': 'LightGBM_model.txt',\n",
    "#     'num_machines': 1,\n",
    "#     'local_listen_port': 12400,\n",
    "#     'machine_list_file': 'mlist.txt',\n",
    "#     'verbose': 0,\n",
    "#     'subsample_for_bin': 200000,\n",
    "#     'min_child_samples': 20,\n",
    "#     'min_child_weight': 0.001,\n",
    "#     'min_split_gain': 0.0,\n",
    "#     'colsample_bytree': 1.0,\n",
    "#     'reg_alpha': 0.0,\n",
    "#     'reg_lambda': 0.0\n",
    "# }\n",
    "\n",
    "# gridParams = {\n",
    "#     'learning_rate': [ 0.1, 0.2, 0.3],\n",
    "#     'num_leaves': [63, 80, 100],\n",
    "#     'boosting_type' : ['gbdt'],\n",
    "#     'objective' : ['binary']\n",
    "# }\n",
    "\n",
    "# mdl = lgb.LGBMClassifier(\n",
    "#     task = params['task'],\n",
    "#     metric = 'matthews_corrcoef',\n",
    "#     metric_freq = params['metric_freq'],\n",
    "#     is_training_metric = params['is_training_metric'],\n",
    "#     max_bin = params['max_bin'],\n",
    "#     tree_learner = params['tree_learner'],\n",
    "#     feature_fraction = params['feature_fraction'],\n",
    "#     bagging_fraction = params['bagging_fraction'],\n",
    "#     bagging_freq = params['bagging_freq'],\n",
    "#     min_data_in_leaf = params['min_data_in_leaf'],\n",
    "#     min_sum_hessian_in_leaf = params['min_sum_hessian_in_leaf'],\n",
    "#     is_enable_sparse = params['is_enable_sparse'],\n",
    "#     use_two_round_loading = params['use_two_round_loading'],\n",
    "#     is_save_binary_file = params['is_save_binary_file'],\n",
    "#     n_jobs = -1\n",
    "# )\n",
    "\n",
    "# scoring = {'mcc': 'matthews_corrcoef'}\n",
    "\n",
    "# # Create the grid\n",
    "# grid = GridSearchCV(mdl, gridParams, verbose=2, cv=10, n_jobs=-1, refit='mcc' )\n",
    "# # Run the grid\n",
    "# grid.fit(bio_features_DS, labels_DS )\n",
    "\n",
    "# print('Best parameters found by grid search are:', grid.best_params_)\n",
    "# print('Best score found by grid search is:', grid.best_score_)\n",
    "\n",
    "# # df_train = pd.read_csv(\"../../../data/sample/binary.train\", header=None, sep='\\t')\n",
    "# # df_test = pd.read_csv(\"../../../data/sample/binary.test\", header=None, sep='\\t')\n",
    "\n",
    "# # y_train = df_train[0].values\n",
    "# # y_test = df_test[0].values\n",
    "# # X_train = df_train.drop(0, axis=1).values\n",
    "# # X_test = df_test.drop(0, axis=1).values\n",
    "\n",
    "# # lgb_train = lgb.Dataset(X_train, y_train)\n",
    "# # lgb_eval = lgb.Dataset(X_test, y_test, reference=lgb_train)\n",
    "\n",
    "# # clf = lgb.train(param, train_data, valid_sets=[val_data, train_data], valid_names=['val', 'train'], feval=lgb_mcc_score, evals_result=evals_result)\n",
    "\n",
    "# # lgb.plot_metric(evals_result, metric='f1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['accuracy',\n",
       " 'adjusted_mutual_info_score',\n",
       " 'adjusted_rand_score',\n",
       " 'average_precision',\n",
       " 'balanced_accuracy',\n",
       " 'brier_score_loss',\n",
       " 'completeness_score',\n",
       " 'explained_variance',\n",
       " 'f1',\n",
       " 'f1_macro',\n",
       " 'f1_micro',\n",
       " 'f1_samples',\n",
       " 'f1_weighted',\n",
       " 'fowlkes_mallows_score',\n",
       " 'homogeneity_score',\n",
       " 'jaccard',\n",
       " 'jaccard_macro',\n",
       " 'jaccard_micro',\n",
       " 'jaccard_samples',\n",
       " 'jaccard_weighted',\n",
       " 'max_error',\n",
       " 'mutual_info_score',\n",
       " 'neg_log_loss',\n",
       " 'neg_mean_absolute_error',\n",
       " 'neg_mean_squared_error',\n",
       " 'neg_mean_squared_log_error',\n",
       " 'neg_median_absolute_error',\n",
       " 'normalized_mutual_info_score',\n",
       " 'precision',\n",
       " 'precision_macro',\n",
       " 'precision_micro',\n",
       " 'precision_samples',\n",
       " 'precision_weighted',\n",
       " 'r2',\n",
       " 'recall',\n",
       " 'recall_macro',\n",
       " 'recall_micro',\n",
       " 'recall_samples',\n",
       " 'recall_weighted',\n",
       " 'roc_auc',\n",
       " 'v_measure_score']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sklearn\n",
    "sorted(sklearn.metrics.SCORERS.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
